# Chapter 16: Resource-Aware Optimization | <mark>ç¬¬ 16 ç« ï¼šèµ„æºæ„ŸçŸ¥ä¼˜åŒ–</mark>

Resource-Aware Optimization enables intelligent agents to dynamically monitor and manage computational, temporal, and financial resources during operation. This differs from simple planning, which primarily focuses on action sequencing. Resource-Aware Optimization requires agents to make decisions regarding action execution to achieve goals within specified resource budgets or to optimize efficiency. This involves choosing between more accurate but expensive models and faster, lower-cost ones, or deciding whether to allocate additional compute for a more refined response versus returning a quicker, less detailed answer.

<mark>èµ„æºæ„ŸçŸ¥ä¼˜åŒ–ä½¿å…·æ™ºèƒ½ä½“ç‰¹æ€§çš„ç³»ç»Ÿèƒ½å¤Ÿåœ¨è¿è¡Œè¿‡ç¨‹ä¸­åŠ¨æ€ç›‘æ§å’Œç®¡ç†è®¡ç®—ã€æ—¶é—´å’Œè´¢åŠ¡èµ„æºã€‚è¿™ä¸ç®€å•çš„è§„åˆ’ä¸åŒï¼Œåè€…ä¸»è¦å…³æ³¨åŠ¨ä½œåºåˆ—ã€‚èµ„æºæ„ŸçŸ¥ä¼˜åŒ–è¦æ±‚æ™ºèƒ½ä½“åœ¨æ‰§è¡ŒåŠ¨ä½œæ—¶åšå‡ºå†³ç­–ï¼Œä»¥åœ¨æŒ‡å®šçš„èµ„æºé¢„ç®—å†…å®ç°ç›®æ ‡æˆ–ä¼˜åŒ–æ•ˆç‡ã€‚è¿™æ¶‰åŠåœ¨æ›´å‡†ç¡®ä½†æ˜‚è´µçš„æ¨¡å‹ä¸æ›´å¿«é€Ÿä½†æˆæœ¬è¾ƒä½çš„æ¨¡å‹ä¹‹é—´è¿›è¡Œé€‰æ‹©ï¼Œæˆ–è€…å†³å®šæ˜¯å¦ä¸ºæ›´ç²¾ç»†çš„å“åº”åˆ†é…é¢å¤–çš„è®¡ç®—èµ„æºï¼Œè¿˜æ˜¯è¿”å›æ›´å¿«é€Ÿä½†ä¸å¤ªè¯¦ç»†çš„ç­”æ¡ˆã€‚</mark>

For example, consider an agent tasked with analyzing a large dataset for a financial analyst. If the analyst needs a preliminary report immediately, the agent might use a faster, more affordable model to quickly summarize key trends. However, if the analyst requires a highly accurate forecast for a critical investment decision and has a larger budget and more time, the agent would allocate more resources to utilize a powerful, slower, but more precise predictive model. A key strategy in this category is the fallback mechanism, which acts as a safeguard when a preferred model is unavailable due to being overloaded or throttled. To ensure graceful degradation, the system automatically switches to a default or more affordable model, maintaining service continuity instead of failing completely.

<mark>ä¾‹å¦‚ï¼Œè€ƒè™‘ä¸€ä¸ªè´Ÿè´£ä¸ºé‡‘èåˆ†æå¸ˆåˆ†æå¤§å‹æ•°æ®é›†çš„æ™ºèƒ½ä½“ã€‚å¦‚æœåˆ†æå¸ˆéœ€è¦ç«‹å³è·å¾—åˆæ­¥æŠ¥å‘Šï¼Œæ™ºèƒ½ä½“å¯èƒ½ä¼šä½¿ç”¨æ›´å¿«é€Ÿã€æ›´å®æƒ çš„æ¨¡å‹æ¥å¿«é€Ÿæ€»ç»“å…³é”®è¶‹åŠ¿ã€‚ç„¶è€Œï¼Œå¦‚æœåˆ†æå¸ˆéœ€è¦ä¸ºå…³é”®æŠ•èµ„å†³ç­–æä¾›é«˜åº¦å‡†ç¡®çš„é¢„æµ‹ï¼Œå¹¶ä¸”æœ‰æ›´å¤§çš„é¢„ç®—å’Œæ›´å¤šæ—¶é—´ï¼Œæ™ºèƒ½ä½“å°†åˆ†é…æ›´å¤šèµ„æºæ¥ä½¿ç”¨å¼ºå¤§ã€è¾ƒæ…¢ä½†æ›´ç²¾ç¡®çš„é¢„æµ‹æ¨¡å‹ã€‚æ­¤ç±»åˆ«ä¸­çš„ä¸€ä¸ªå…³é”®ç­–ç•¥æ˜¯åå¤‡æœºåˆ¶ï¼Œå½“é¦–é€‰æ¨¡å‹ç”±äºè¿‡è½½æˆ–é™æµè€Œä¸å¯ç”¨æ—¶ï¼Œå®ƒå……å½“ä¿æŠ¤æªæ–½ã€‚ä¸ºç¡®ä¿ä¼˜é›…é™çº§ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨åˆ‡æ¢åˆ°é»˜è®¤æˆ–æ›´å®æƒ çš„æ¨¡å‹ï¼Œä»è€Œç»´æŒæœåŠ¡è¿ç»­æ€§ï¼Œè€Œä¸æ˜¯å®Œå…¨å¤±è´¥ã€‚</mark>

---

## Practical Applications & Use Cases | <mark>å®é™…åº”ç”¨ä¸ç”¨ä¾‹</mark>

Practical use cases include:

<mark>å®é™…ç”¨ä¾‹åŒ…æ‹¬ï¼š</mark>

* **Cost-Optimized LLM Usage**: An agent deciding whether to use a large, expensive LLM for complex tasks or a smaller, more affordable one for simpler queries, based on a budget constraint.

<mark>* <strong>æˆæœ¬ä¼˜åŒ–çš„ LLM ä½¿ç”¨ï¼š</strong>æ™ºèƒ½ä½“æ ¹æ®é¢„ç®—çº¦æŸï¼Œå†³å®šæ˜¯ä½¿ç”¨å¤§å‹ã€æ˜‚è´µçš„ LLM å¤„ç†å¤æ‚ä»»åŠ¡ï¼Œè¿˜æ˜¯ä½¿ç”¨è¾ƒå°ã€æ›´å®æƒ çš„ LLM å¤„ç†ç®€å•æŸ¥è¯¢ã€‚</mark>

* **Latency-Sensitive Operations**: In real-time systems, an agent chooses a faster but potentially less comprehensive reasoning path to ensure a timely response.

<mark>* <strong>å»¶è¿Ÿæ•æ„Ÿæ“ä½œï¼š</strong>åœ¨å®æ—¶ç³»ç»Ÿä¸­ï¼Œæ™ºèƒ½ä½“é€‰æ‹©æ›´å¿«ä½†å¯èƒ½ä¸å¤ªå…¨é¢çš„æ¨ç†è·¯å¾„ï¼Œä»¥ç¡®ä¿åŠæ—¶å“åº”ã€‚</mark>

* **Energy Efficiency**: For agents deployed on edge devices or with limited power, optimizing their processing to conserve battery life.

<mark>* <strong>èƒ½æºæ•ˆç‡ï¼š</strong>å¯¹äºéƒ¨ç½²åœ¨è¾¹ç¼˜è®¾å¤‡æˆ–ç”µåŠ›æœ‰é™çš„æ™ºèƒ½ä½“ï¼Œä¼˜åŒ–å…¶å¤„ç†ä»¥èŠ‚çœç”µæ± å¯¿å‘½ã€‚</mark>

* **Fallback for service reliability**: An agent automatically switches to a backup model when the primary choice is unavailable, ensuring service continuity and graceful degradation.

<mark>* <strong>æœåŠ¡å¯é æ€§çš„åå¤‡æœºåˆ¶ï¼š</strong>å½“ä¸»è¦é€‰æ‹©ä¸å¯ç”¨æ—¶ï¼Œæ™ºèƒ½ä½“è‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ä»½æ¨¡å‹ï¼Œç¡®ä¿æœåŠ¡è¿ç»­æ€§å’Œä¼˜é›…é™çº§ã€‚</mark>

* **Data Usage Management**: An agent opting for summarized data retrieval instead of full dataset downloads to save bandwidth or storage.

<mark>* <strong>æ•°æ®ä½¿ç”¨ç®¡ç†ï¼š</strong>æ™ºèƒ½ä½“é€‰æ‹©æ‘˜è¦æ•°æ®æ£€ç´¢è€Œä¸æ˜¯å®Œæ•´æ•°æ®é›†ä¸‹è½½ï¼Œä»¥èŠ‚çœå¸¦å®½æˆ–å­˜å‚¨ç©ºé—´ã€‚</mark>

* **Adaptive Task Allocation**: In multi-agent systems, agents self-assign tasks based on their current computational load or available time.

<mark>* <strong>è‡ªé€‚åº”ä»»åŠ¡åˆ†é…ï¼š</strong>åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­ï¼Œæ™ºèƒ½ä½“æ ¹æ®å½“å‰è®¡ç®—è´Ÿè½½æˆ–å¯ç”¨æ—¶é—´è‡ªè¡Œåˆ†é…ä»»åŠ¡ã€‚</mark>

---

## Hands-On Code Example | <mark>å®æˆ˜ä»£ç ç¤ºä¾‹</mark>

An intelligent system for answering user questions can assess the difficulty of each question. For simple queries, it utilizes a cost-effective language model such as Gemini Flash. For complex inquiries, a more powerful, but expensive, language model (like Gemini Pro) is considered. The decision to use the more powerful model also depends on resource availability, specifically budget and time constraints. This system dynamically selects appropriate models.

<mark>ä¸€ä¸ªç”¨äºå›ç­”ç”¨æˆ·é—®é¢˜çš„æ™ºèƒ½ç³»ç»Ÿå¯ä»¥è¯„ä¼°æ¯ä¸ªé—®é¢˜çš„éš¾åº¦ã€‚å¯¹äºç®€å•æŸ¥è¯¢ï¼Œå®ƒä½¿ç”¨æˆæœ¬æ•ˆç›Šé«˜çš„è¯­è¨€æ¨¡å‹ï¼Œå¦‚ <code>Gemini Flash</code>ã€‚å¯¹äºå¤æ‚æŸ¥è¯¢ï¼Œä¼šè€ƒè™‘ä½¿ç”¨æ›´å¼ºå¤§ä½†æ˜‚è´µçš„è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ <code>Gemini Pro</code>ï¼‰ã€‚ä½¿ç”¨æ›´å¼ºå¤§æ¨¡å‹çš„å†³å®šè¿˜å–å†³äºèµ„æºå¯ç”¨æ€§ï¼Œç‰¹åˆ«æ˜¯é¢„ç®—å’Œæ—¶é—´çº¦æŸã€‚è¯¥ç³»ç»Ÿä¼šåŠ¨æ€é€‰æ‹©é€‚å½“çš„æ¨¡å‹ã€‚</mark>

For example, consider a travel planner built with a hierarchical agent. The high-level planning, which involves understanding a user's complex request, breaking it down into a multi-step itinerary, and making logical decisions, would be managed by a sophisticated and more powerful LLM like Gemini Pro. This is the "planner" agent that requires a deep understanding of context and the ability to reason.

<mark>ä¾‹å¦‚ï¼Œè€ƒè™‘ä½¿ç”¨å±‚æ¬¡åŒ–æ™ºèƒ½ä½“æ„å»ºçš„æ—…è¡Œè§„åˆ’å™¨ã€‚é«˜å±‚è§„åˆ’æ¶‰åŠç†è§£ç”¨æˆ·çš„å¤æ‚è¯·æ±‚ã€å°†å…¶åˆ†è§£ä¸ºå¤šæ­¥éª¤è¡Œç¨‹å¹¶åšå‡ºé€»è¾‘å†³ç­–ï¼Œè¿™å°†ç”±å¤æ‚ä¸”æ›´å¼ºå¤§çš„ LLMï¼ˆå¦‚ <code>Gemini Pro</code>ï¼‰ç®¡ç†ã€‚è¿™æ˜¯éœ€è¦æ·±å…¥ç†è§£ä¸Šä¸‹æ–‡å’Œæ¨ç†èƒ½åŠ›çš„ã€Œè§„åˆ’å™¨ã€æ™ºèƒ½ä½“ã€‚</mark>

However, once the plan is established, the individual tasks within that plan, such as looking up flight prices, checking hotel availability, or finding restaurant reviews, are essentially simple, repetitive web queries. These "tool function calls" can be executed by a faster and more affordable model like Gemini Flash. It is easier to visualize why the affordable model can be used for these straightforward web searches, while the intricate planning phase requires the greater intelligence of the more advanced model to ensure a coherent and logical travel plan.

<mark>ç„¶è€Œï¼Œä¸€æ—¦ç¡®å®šäº†è®¡åˆ’ï¼Œè¯¥è®¡åˆ’ä¸­çš„å„ä¸ªä»»åŠ¡ï¼ˆå¦‚æŸ¥æ‰¾èˆªç­ä»·æ ¼ã€æ£€æŸ¥é…’åº—å¯ç”¨æ€§æˆ–æŸ¥æ‰¾é¤å…è¯„è®ºï¼‰æœ¬è´¨ä¸Šæ˜¯ç®€å•ã€é‡å¤çš„ç½‘ç»œæŸ¥è¯¢ã€‚è¿™äº›ã€Œå·¥å…·å‡½æ•°è°ƒç”¨ã€å¯ä»¥ç”±æ›´å¿«é€Ÿã€æ›´å®æƒ çš„æ¨¡å‹ï¼ˆå¦‚ <code>Gemini Flash</code>ï¼‰æ‰§è¡Œã€‚å¾ˆå®¹æ˜“ç†è§£ä¸ºä»€ä¹ˆè¿™äº›ç›´æ¥çš„ç½‘ç»œæœç´¢å¯ä»¥ä½¿ç”¨å®æƒ çš„æ¨¡å‹ï¼Œè€Œå¤æ‚çš„è§„åˆ’é˜¶æ®µéœ€è¦æ›´å…ˆè¿›æ¨¡å‹çš„æ›´å¼ºæ™ºèƒ½ï¼Œä»¥ç¡®ä¿è¿è´¯ä¸”åˆä¹é€»è¾‘çš„æ—…è¡Œè®¡åˆ’ã€‚</mark>

Google's ADK supports this approach through its multi-agent architecture, which allows for modular and scalable applications. Different agents can handle specialized tasks. Model flexibility enables the direct use of various Gemini models, including both Gemini Pro and Gemini Flash, or integration of other models through LiteLLM. The ADK's orchestration capabilities support dynamic, LLM-driven routing for adaptive behavior. Built-in evaluation features allow systematic assessment of agent performance, which can be used for system refinement (see the Chapter on Evaluation and Monitoring).

<mark>Google çš„ <code>ADK</code> é€šè¿‡å…¶å¤šæ™ºèƒ½ä½“æ¶æ„æ”¯æŒè¿™ç§æ–¹æ³•ï¼Œæ”¯æŒæ¨¡å—åŒ–å’Œå¯æ‰©å±•çš„åº”ç”¨ç¨‹åºã€‚ä¸åŒçš„æ™ºèƒ½ä½“å¯ä»¥å¤„ç†ä¸“é—¨çš„ä»»åŠ¡ã€‚æ¨¡å‹çµæ´»æ€§æ”¯æŒç›´æ¥ä½¿ç”¨å„ç§ Gemini æ¨¡å‹ï¼ŒåŒ…æ‹¬ <code>Gemini Pro</code> å’Œ <code>Gemini Flash</code>ï¼Œæˆ–é€šè¿‡ <code>LiteLLM</code> é›†æˆå…¶ä»–æ¨¡å‹ã€‚ADK çš„ç¼–æ’åŠŸèƒ½æ”¯æŒåŠ¨æ€çš„ã€ç”± LLM é©±åŠ¨çš„è·¯ç”±ï¼Œä»¥å®ç°è‡ªé€‚åº”è¡Œä¸ºã€‚å†…ç½®çš„è¯„ä¼°åŠŸèƒ½å…è®¸ç³»ç»ŸåŒ–è¯„ä¼°æ™ºèƒ½ä½“æ€§èƒ½ï¼Œå¯ç”¨äºç³»ç»Ÿä¼˜åŒ–ï¼ˆå‚è§è¯„ä¼°ä¸ç›‘æ§ç« èŠ‚ï¼‰ã€‚</mark>

Next, two agents with identical setup but utilizing different models and costs will be defined.

<mark>æ¥ä¸‹æ¥ï¼Œå°†å®šä¹‰ä¸¤ä¸ªè®¾ç½®ç›¸åŒä½†ä½¿ç”¨ä¸åŒæ¨¡å‹å’Œæˆæœ¬çš„æ™ºèƒ½ä½“ã€‚</mark>

```python
# Conceptual Python-like structure, not runnable code

from google.adk.agents import Agent
# from google.adk.models.lite_llm import LiteLlm # If using models not directly supported by ADK's default Agent

# Agent using the more expensive Gemini Pro 2.5
gemini_pro_agent = Agent(
   name="GeminiProAgent",
   model="gemini-2.5-pro", # Placeholder for actual model name if different
   description="A highly capable agent for complex queries.",
   instruction="You are an expert assistant for complex problem-solving."
)

# Agent using the less expensive Gemini Flash 2.5
gemini_flash_agent = Agent(
   name="GeminiFlashAgent",
   model="gemini-2.5-flash", # Placeholder for actual model name if different
   description="A fast and efficient agent for simple queries.",
   instruction="You are a quick assistant for straightforward questions."
)
```

<mark>ä¸Šè¿°ä»£ç å®šä¹‰äº†ä¸¤ä¸ªæ™ºèƒ½ä½“ï¼š<code>GeminiProAgent</code> ä½¿ç”¨æ›´æ˜‚è´µçš„ <code>Gemini Pro 2.5</code> æ¨¡å‹å¤„ç†å¤æ‚æŸ¥è¯¢ï¼Œè€Œ <code>GeminiFlashAgent</code> ä½¿ç”¨æ›´å®æƒ çš„ <code>Gemini Flash 2.5</code> æ¨¡å‹å¤„ç†ç®€å•æŸ¥è¯¢ã€‚</mark>

### Router Agent | <mark>è·¯ç”±æ™ºèƒ½ä½“</mark>

A Router Agent can direct queries based on simple metrics like query length, where shorter queries go to less expensive models and longer queries to more capable models. However, a more sophisticated Router Agent can utilize either LLM or ML models to analyze query nuances and complexity. This LLM router can determine which downstream language model is most suitable. For example, a query requesting a factual recall is routed to a flash model, while a complex query requiring deep analysis is routed to a pro model.

<mark>è·¯ç”±æ™ºèƒ½ä½“å¯ä»¥åŸºäºç®€å•çš„æŒ‡æ ‡ï¼ˆå¦‚æŸ¥è¯¢é•¿åº¦ï¼‰æ¥å¼•å¯¼æŸ¥è¯¢ï¼Œå…¶ä¸­è¾ƒçŸ­çš„æŸ¥è¯¢å‘é€åˆ°æˆæœ¬è¾ƒä½çš„æ¨¡å‹ï¼Œè¾ƒé•¿çš„æŸ¥è¯¢å‘é€åˆ°æ›´å¼ºå¤§çš„æ¨¡å‹ã€‚ç„¶è€Œï¼Œæ›´å¤æ‚çš„è·¯ç”±æ™ºèƒ½ä½“å¯ä»¥åˆ©ç”¨ LLM æˆ– ML æ¨¡å‹æ¥åˆ†ææŸ¥è¯¢çš„ç»†å¾®å·®åˆ«å’Œå¤æ‚æ€§ã€‚è¿™ä¸ª LLM è·¯ç”±å™¨å¯ä»¥ç¡®å®šå“ªä¸ªä¸‹æ¸¸è¯­è¨€æ¨¡å‹æœ€åˆé€‚ã€‚ä¾‹å¦‚ï¼Œè¯·æ±‚äº‹å®å›å¿†çš„æŸ¥è¯¢è¢«è·¯ç”±åˆ° Flash æ¨¡å‹ï¼Œè€Œéœ€è¦æ·±å…¥åˆ†æçš„å¤æ‚æŸ¥è¯¢è¢«è·¯ç”±åˆ° Pro æ¨¡å‹ã€‚</mark>

Optimization techniques can further enhance the LLM router's effectiveness. Prompt tuning involves crafting prompts to guide the router LLM for better routing decisions. Fine-tuning the LLM router on a dataset of queries and their optimal model choices improves its accuracy and efficiency. This dynamic routing capability balances response quality with cost-effectiveness.

<mark>ä¼˜åŒ–æŠ€æœ¯å¯ä»¥è¿›ä¸€æ­¥æé«˜ LLM è·¯ç”±å™¨çš„æœ‰æ•ˆæ€§ã€‚æç¤ºè°ƒä¼˜æ¶‰åŠç²¾å¿ƒè®¾è®¡æç¤ºä»¥æŒ‡å¯¼è·¯ç”±å™¨ LLM åšå‡ºæ›´å¥½çš„è·¯ç”±å†³ç­–ã€‚åœ¨æŸ¥è¯¢åŠå…¶æœ€ä¼˜æ¨¡å‹é€‰æ‹©çš„æ•°æ®é›†ä¸Šå¯¹ LLM è·¯ç”±å™¨è¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥æé«˜å…¶å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚è¿™ç§åŠ¨æ€è·¯ç”±èƒ½åŠ›åœ¨å“åº”è´¨é‡å’Œæˆæœ¬æ•ˆç›Šä¹‹é—´å–å¾—å¹³è¡¡ã€‚</mark>

```python
# Conceptual Python-like structure, not runnable code

from google.adk.agents import Agent, BaseAgent
from google.adk.events import Event
from google.adk.agents.invocation_context import InvocationContext
import asyncio

class QueryRouterAgent(BaseAgent):
   name: str = "QueryRouter"
   description: str = "Routes user queries to the appropriate LLM agent based on complexity."

   async def _run_async_impl(self, context: InvocationContext) -> AsyncGenerator[Event, None]:
       user_query = context.current_message.text # Assuming text input
       query_length = len(user_query.split()) # Simple metric: number of words

       if query_length < 20: # Example threshold for simplicity vs. complexity
           print(f"Routing to Gemini Flash Agent for short query (length: {query_length})")
           # In a real ADK setup, you would 'transfer_to_agent' or directly invoke
           # For demonstration, we'll simulate a call and yield its response
           response = await gemini_flash_agent.run_async(context.current_message)
           yield Event(author=self.name, content=f"Flash Agent processed: {response}")
       else:
           print(f"Routing to Gemini Pro Agent for long query (length: {query_length})")
           response = await gemini_pro_agent.run_async(context.current_message)
           yield Event(author=self.name, content=f"Pro Agent processed: {response}")
```

<mark>ä¸Šè¿°ä»£ç å®ç°äº† <code>QueryRouterAgent</code> ç±»ï¼Œå®ƒæ ¹æ®æŸ¥è¯¢çš„å¤æ‚åº¦å°†ç”¨æˆ·æŸ¥è¯¢è·¯ç”±åˆ°é€‚å½“çš„ LLM æ™ºèƒ½ä½“ã€‚å®ƒä½¿ç”¨ç®€å•çš„æŒ‡æ ‡ï¼ˆæŸ¥è¯¢ä¸­çš„å•è¯æ•°ï¼‰æ¥ç¡®å®šå¤æ‚åº¦ã€‚çŸ­æŸ¥è¯¢ï¼ˆå°‘äº 20 ä¸ªå•è¯ï¼‰è¢«è·¯ç”±åˆ° <code>Gemini Flash Agent</code>ï¼Œè€Œé•¿æŸ¥è¯¢è¢«è·¯ç”±åˆ° <code>Gemini Pro Agent</code>ã€‚</mark>

### Critique Agent | <mark>è¯„è®ºæ™ºèƒ½ä½“</mark>

The Critique Agent evaluates responses from language models, providing feedback that serves several functions. For self-correction, it identifies errors or inconsistencies, prompting the answering agent to refine its output for improved quality. It also systematically assesses responses for performance monitoring, tracking metrics like accuracy and relevance, which are used for optimization.

<mark>è¯„è®ºæ™ºèƒ½ä½“è¯„ä¼°æ¥è‡ªè¯­è¨€æ¨¡å‹çš„å“åº”ï¼Œæä¾›æœåŠ¡äºå¤šç§åŠŸèƒ½çš„åé¦ˆã€‚å¯¹äºè‡ªæˆ‘ä¿®æ­£ï¼Œå®ƒè¯†åˆ«é”™è¯¯æˆ–ä¸ä¸€è‡´ä¹‹å¤„ï¼Œä¿ƒä½¿å›ç­”æ™ºèƒ½ä½“æ”¹è¿›å…¶è¾“å‡ºä»¥æé«˜è´¨é‡ã€‚å®ƒè¿˜ç³»ç»ŸåŒ–åœ°è¯„ä¼°å“åº”ä»¥è¿›è¡Œæ€§èƒ½ç›‘æ§ï¼Œè·Ÿè¸ªå‡†ç¡®æ€§å’Œç›¸å…³æ€§ç­‰æŒ‡æ ‡ï¼Œè¿™äº›æŒ‡æ ‡ç”¨äºä¼˜åŒ–ã€‚</mark>

Additionally, its feedback can signal reinforcement learning or fine-tuning; consistent identification of inadequate Flash model responses, for instance, can refine the router agent's logic. While not directly managing the budget, the Critique Agent contributes to indirect budget management by identifying suboptimal routing choices, such as directing simple queries to a Pro model or complex queries to a Flash model, which leads to poor results. This informs adjustments that improve resource allocation and cost savings.

<mark>æ­¤å¤–ï¼Œå…¶åé¦ˆå¯ä»¥ä¸ºå¼ºåŒ–å­¦ä¹ æˆ–å¾®è°ƒæä¾›ä¿¡å·ï¼›ä¾‹å¦‚ï¼ŒæŒç»­è¯†åˆ« Flash æ¨¡å‹çš„ä¸å……åˆ†å“åº”å¯ä»¥æ”¹è¿›è·¯ç”±æ™ºèƒ½ä½“çš„é€»è¾‘ã€‚è™½ç„¶ä¸ç›´æ¥ç®¡ç†é¢„ç®—ï¼Œä½†è¯„è®ºæ™ºèƒ½ä½“é€šè¿‡è¯†åˆ«æ¬¡ä¼˜è·¯ç”±é€‰æ‹©ï¼ˆå¦‚å°†ç®€å•æŸ¥è¯¢å¼•å¯¼åˆ° Pro æ¨¡å‹æˆ–å°†å¤æ‚æŸ¥è¯¢å¼•å¯¼åˆ° Flash æ¨¡å‹ï¼Œä»è€Œå¯¼è‡´ä¸è‰¯ç»“æœï¼‰æ¥ä¿ƒè¿›é—´æ¥é¢„ç®—ç®¡ç†ã€‚è¿™ä¸ºæ”¹è¿›èµ„æºåˆ†é…å’Œæˆæœ¬èŠ‚çº¦çš„è°ƒæ•´æä¾›äº†ä¿¡æ¯ã€‚</mark>

The Critique Agent can be configured to review either only the generated text from the answering agent or both the original query and the generated text, enabling a comprehensive evaluation of the response's alignment with the initial question.

<mark>è¯„è®ºæ™ºèƒ½ä½“å¯ä»¥é…ç½®ä¸ºä»…å®¡æŸ¥æ¥è‡ªå›ç­”æ™ºèƒ½ä½“çš„ç”Ÿæˆæ–‡æœ¬ï¼Œæˆ–åŒæ—¶å®¡æŸ¥åŸå§‹æŸ¥è¯¢å’Œç”Ÿæˆæ–‡æœ¬ï¼Œä»è€Œèƒ½å¤Ÿå…¨é¢è¯„ä¼°å“åº”ä¸åˆå§‹é—®é¢˜çš„å¯¹é½æƒ…å†µã€‚</mark>

```python
CRITIC_SYSTEM_PROMPT = """
You are the **Critic Agent**, serving as the quality assurance arm of our collaborative research assistant system. Your primary function is to **meticulously review and challenge** information from the Researcher Agent, guaranteeing **accuracy, completeness, and unbiased presentation**.
Your duties encompass:
* **Assessing research findings** for factual correctness, thoroughness, and potential leanings.
* **Identifying any missing data** or inconsistencies in reasoning.
* **Raising critical questions** that could refine or expand the current understanding.
* **Offering constructive suggestions** for enhancement or exploring different angles.
* **Validating that the final output is comprehensive** and balanced.
All criticism must be constructive. Your goal is to fortify the research, not invalidate it. Structure your feedback clearly, drawing attention to specific points for revision. Your overarching aim is to ensure the final research product meets the highest possible quality standards.
"""
```

<mark>ä¸Šè¿°ä»£ç å±•ç¤ºäº†è¯„è®ºæ™ºèƒ½ä½“çš„ç³»ç»Ÿæç¤ºæ¨¡æ¿ <code>CRITIC_SYSTEM_PROMPT</code>ã€‚è¯¥æç¤ºæ˜ç¡®å®šä¹‰äº†æ™ºèƒ½ä½“ä½œä¸ºè´¨é‡ä¿è¯ç»„ä»¶çš„è§’è‰²ï¼ŒèŒè´£åŒ…æ‹¬è¯„ä¼°ç ”ç©¶ç»“æœçš„å‡†ç¡®æ€§ã€è¯†åˆ«ç¼ºå¤±æ•°æ®ã€æå‡ºæ‰¹åˆ¤æ€§é—®é¢˜ã€æä¾›å»ºè®¾æ€§å»ºè®®ï¼Œå¹¶éªŒè¯æœ€ç»ˆè¾“å‡ºçš„å…¨é¢æ€§å’Œå¹³è¡¡æ€§ã€‚</mark>

The Critic Agent operates based on a predefined system prompt that outlines its role, responsibilities, and feedback approach. A well-designed prompt for this agent must clearly establish its function as an evaluator. It should specify the areas for critical focus and emphasize providing constructive feedback rather than mere dismissal. The prompt should also encourage the identification of both strengths and weaknesses, and it must guide the agent on how to structure and present its feedback.

<mark>è¯„è®ºæ™ºèƒ½ä½“åŸºäºé¢„å®šä¹‰çš„ç³»ç»Ÿæç¤ºè¿è¡Œï¼Œè¯¥æç¤ºæ¦‚è¿°äº†å…¶è§’è‰²ã€èŒè´£å’Œåé¦ˆæ–¹æ³•ã€‚ä¸ºè¯¥æ™ºèƒ½ä½“è®¾è®¡è‰¯å¥½çš„æç¤ºå¿…é¡»æ¸…æ¥šåœ°ç¡®ç«‹å…¶ä½œä¸ºè¯„ä¼°è€…çš„åŠŸèƒ½ã€‚å®ƒåº”è¯¥æŒ‡å®šéœ€è¦é‡ç‚¹å…³æ³¨çš„é¢†åŸŸï¼Œå¹¶å¼ºè°ƒæä¾›å»ºè®¾æ€§åé¦ˆè€Œä¸æ˜¯ç®€å•åœ°å¦å®šã€‚æç¤ºè¿˜åº”é¼“åŠ±è¯†åˆ«ä¼˜åŠ¿å’ŒåŠ£åŠ¿ï¼Œå¹¶ä¸”å¿…é¡»æŒ‡å¯¼æ™ºèƒ½ä½“å¦‚ä½•æ„å»ºå’Œå‘ˆç°å…¶åé¦ˆã€‚</mark>

---

## Hands-On Code with OpenAI | <mark>ä½¿ç”¨ OpenAI çš„å®æˆ˜ä»£ç </mark>

This system uses a resource-aware optimization strategy to handle user queries efficiently. It first classifies each query into one of three categories to determine the most appropriate and cost-effective processing pathway. This approach avoids wasting computational resources on simple requests while ensuring complex queries get the necessary attention. The three categories are:

<mark>è¯¥ç³»ç»Ÿä½¿ç”¨èµ„æºæ„ŸçŸ¥ä¼˜åŒ–ç­–ç•¥æ¥é«˜æ•ˆå¤„ç†ç”¨æˆ·æŸ¥è¯¢ã€‚å®ƒé¦–å…ˆå°†æ¯ä¸ªæŸ¥è¯¢åˆ†ç±»ä¸ºä¸‰ä¸ªç±»åˆ«ä¹‹ä¸€ï¼Œä»¥ç¡®å®šæœ€åˆé€‚ä¸”æœ€å…·æˆæœ¬æ•ˆç›Šçš„å¤„ç†è·¯å¾„ã€‚è¿™ç§æ–¹æ³•é¿å…äº†åœ¨ç®€å•è¯·æ±‚ä¸Šæµªè´¹è®¡ç®—èµ„æºï¼ŒåŒæ—¶ç¡®ä¿å¤æ‚æŸ¥è¯¢è·å¾—å¿…è¦çš„å…³æ³¨ã€‚ä¸‰ä¸ªç±»åˆ«æ˜¯ï¼š</mark>

* **simple**: For straightforward questions that can be answered directly without complex reasoning or external data.

<mark>* <strong>simpleï¼ˆç®€å•ï¼‰ï¼š</strong>ç”¨äºå¯ä»¥ç›´æ¥å›ç­”è€Œæ— éœ€å¤æ‚æ¨ç†æˆ–å¤–éƒ¨æ•°æ®çš„ç›´æ¥é—®é¢˜ã€‚</mark>

* **reasoning**: For queries that require logical deduction or multi-step thought processes, which are routed to more powerful models.

<mark>* <strong>reasoningï¼ˆæ¨ç†ï¼‰ï¼š</strong>ç”¨äºéœ€è¦é€»è¾‘æ¨ç†æˆ–å¤šæ­¥éª¤æ€è€ƒè¿‡ç¨‹çš„æŸ¥è¯¢ï¼Œè¿™äº›æŸ¥è¯¢è¢«è·¯ç”±åˆ°æ›´å¼ºå¤§çš„æ¨¡å‹ã€‚</mark>

* **internet_search**: For questions needing current information, which automatically triggers a Google Search to provide an up-to-date answer.

<mark>* <strong>internet_searchï¼ˆç½‘ç»œæœç´¢ï¼‰ï¼š</strong>ç”¨äºéœ€è¦å½“å‰ä¿¡æ¯çš„é—®é¢˜ï¼Œä¼šè‡ªåŠ¨è§¦å‘ Google æœç´¢ä»¥æä¾›æœ€æ–°ç­”æ¡ˆã€‚</mark>

The code is under the MIT license and available on Github: (https://github.com/mahtabsyed/21-Agentic-Patterns/blob/main/16_Resource_Aware_Opt_LLM_Reflection_v2.ipynb)

<mark>è¯¥ä»£ç é‡‡ç”¨ MIT è®¸å¯è¯ï¼Œå¯åœ¨ GitHub ä¸Šè·å–ï¼š(https://github.com/mahtabsyed/21-Agentic-Patterns/blob/main/16_Resource_Aware_Opt_LLM_Reflection_v2.ipynb)</mark>

```python
# MIT License
# Copyright (c) 2025 Mahtab Syed
# https://www.linkedin.com/in/mahtabsyed/

import os
import requests
import json
from dotenv import load_dotenv
from openai import OpenAI

# Load environment variables
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GOOGLE_CUSTOM_SEARCH_API_KEY = os.getenv("GOOGLE_CUSTOM_SEARCH_API_KEY")
GOOGLE_CSE_ID = os.getenv("GOOGLE_CSE_ID")

if not OPENAI_API_KEY or not GOOGLE_CUSTOM_SEARCH_API_KEY or not GOOGLE_CSE_ID:
    raise ValueError(
        "Please set OPENAI_API_KEY, GOOGLE_CUSTOM_SEARCH_API_KEY, and GOOGLE_CSE_ID in your .env file."
    )

client = OpenAI(api_key=OPENAI_API_KEY)

# --- Step 1: Classify the Prompt ---
def classify_prompt(prompt: str) -> dict:
    system_message = {
        "role": "system",
        "content": (
            "You are a classifier that analyzes user prompts and returns one of three categories ONLY:\n\n"
            "- simple\n"
            "- reasoning\n"
            "- internet_search\n\n"
            "Rules:\n"
            "- Use 'simple' for direct factual questions that need no reasoning or current events.\n"
            "- Use 'reasoning' for logic, math, or multi-step inference questions.\n"
            "- Use 'internet_search' if the prompt refers to current events, recent data, or things not in your training data.\n\n"
            "Respond ONLY with JSON like:\n"
            '{ "classification": "simple" }'
        ),
    }

    user_message = {"role": "user", "content": prompt}

    response = client.chat.completions.create(
        model="gpt-4o", messages=[system_message, user_message], temperature=1
    )

    reply = response.choices[0].message.content
    return json.loads(reply)

# --- Step 2: Google Search ---
def google_search(query: str, num_results=1) -> list:
    url = "https://www.googleapis.com/customsearch/v1"
    params = {
        "key": GOOGLE_CUSTOM_SEARCH_API_KEY,
        "cx": GOOGLE_CSE_ID,
        "q": query,
        "num": num_results,
    }

    try:
        response = requests.get(url, params=params)
        response.raise_for_status()
        results = response.json()

        if "items" in results and results["items"]:
            return [
                {
                    "title": item.get("title"),
                    "snippet": item.get("snippet"),
                    "link": item.get("link"),
                }
                for item in results["items"]
            ]
        else:
            return []
    except requests.exceptions.RequestException as e:
        return {"error": str(e)}

# --- Step 3: Generate Response ---
def generate_response(prompt: str, classification: str, search_results=None) -> str:
    if classification == "simple":
        model = "gpt-4o-mini"
        full_prompt = prompt
    elif classification == "reasoning":
        model = "o4-mini"
        full_prompt = prompt
    elif classification == "internet_search":
        model = "gpt-4o"
        # Convert each search result dict to a readable string
        if search_results:
            search_context = "\n".join(
                [
                    f"Title: {item.get('title')}\nSnippet: {item.get('snippet')}\nLink: {item.get('link')}"
                    for item in search_results
                ]
            )
        else:
            search_context = "No search results found."
        full_prompt = f"""Use the following web results to answer the user query:

{search_context}

Query: {prompt}"""

    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": full_prompt}],
        temperature=1,
    )

    return response.choices[0].message.content, model

# --- Step 4: Combined Router ---
def handle_prompt(prompt: str) -> dict:
    classification_result = classify_prompt(prompt)
    classification = classification_result["classification"]

    search_results = None
    if classification == "internet_search":
        search_results = google_search(prompt)

    answer, model = generate_response(prompt, classification, search_results)
    return {"classification": classification, "response": answer, "model": model}

test_prompt = "What is the capital of Australia?"
# test_prompt = "Explain the impact of quantum computing on cryptography."
# test_prompt = "When does the Australian Open 2026 start, give me full date?"

result = handle_prompt(test_prompt)
print("ğŸ” Classification:", result["classification"])
print("ğŸ§  Model Used:", result["model"])
print("ğŸ§  Response:\n", result["response"])
```

<mark>è¯¥ Python ä»£ç å®ç°äº†ä¸€ä¸ªæç¤ºè·¯ç”±ç³»ç»Ÿæ¥å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å®ƒé¦–å…ˆä» <code>.env</code> æ–‡ä»¶åŠ è½½ OpenAI å’Œ Google Custom Search æ‰€éœ€çš„ API å¯†é’¥ã€‚æ ¸å¿ƒåŠŸèƒ½åœ¨äºå°†ç”¨æˆ·çš„æç¤ºåˆ†ç±»ä¸ºä¸‰ä¸ªç±»åˆ«ï¼š<code>simple</code>ï¼ˆç®€å•ï¼‰ã€<code>reasoning</code>ï¼ˆæ¨ç†ï¼‰æˆ– <code>internet_search</code>ï¼ˆç½‘ç»œæœç´¢ï¼‰ã€‚ä¸“é—¨çš„å‡½æ•° <code>classify_prompt</code> åˆ©ç”¨ OpenAI æ¨¡å‹æ‰§è¡Œæ­¤åˆ†ç±»æ­¥éª¤ã€‚å¦‚æœæç¤ºéœ€è¦å½“å‰ä¿¡æ¯ï¼Œåˆ™ä½¿ç”¨ Google Custom Search API æ‰§è¡Œ Google æœç´¢ã€‚å¦ä¸€ä¸ªå‡½æ•° <code>generate_response</code> æ ¹æ®åˆ†ç±»é€‰æ‹©é€‚å½“çš„ OpenAI æ¨¡å‹ç”Ÿæˆæœ€ç»ˆå“åº”ã€‚å¯¹äºç½‘ç»œæœç´¢æŸ¥è¯¢ï¼Œæœç´¢ç»“æœä½œä¸ºä¸Šä¸‹æ–‡æä¾›ç»™æ¨¡å‹ã€‚ä¸»å‡½æ•° <code>handle_prompt</code> åè°ƒæ­¤å·¥ä½œæµç¨‹ï¼Œåœ¨ç”Ÿæˆå“åº”ä¹‹å‰è°ƒç”¨åˆ†ç±»å’Œæœç´¢ï¼ˆå¦‚æœéœ€è¦ï¼‰å‡½æ•°ã€‚å®ƒè¿”å›åˆ†ç±»ã€ä½¿ç”¨çš„æ¨¡å‹å’Œç”Ÿæˆçš„ç­”æ¡ˆã€‚è¯¥ç³»ç»Ÿæœ‰æ•ˆåœ°å°†ä¸åŒç±»å‹çš„æŸ¥è¯¢å¼•å¯¼åˆ°ä¼˜åŒ–çš„æ–¹æ³•ä»¥è·å¾—æ›´å¥½çš„å“åº”ã€‚</mark>

---

## Hands-On Code Example (OpenRouter) | <mark>ä½¿ç”¨ OpenRouter çš„å®æˆ˜ä»£ç </mark>

OpenRouter offers a unified interface to hundreds of AI models via a single API endpoint. It provides automated failover and cost-optimization, with easy integration through your preferred SDK or framework.

<mark>OpenRouter é€šè¿‡å•ä¸ª API ç«¯ç‚¹æä¾›å¯¹æ•°ç™¾ä¸ª AI æ¨¡å‹çš„ç»Ÿä¸€æ¥å£ã€‚å®ƒæä¾›è‡ªåŠ¨æ•…éšœè½¬ç§»å’Œæˆæœ¬ä¼˜åŒ–ï¼Œå¯é€šè¿‡æ‚¨å–œæ¬¢çš„ SDK æˆ–æ¡†æ¶è½»æ¾é›†æˆã€‚</mark>

```python
import requests
import json
response = requests.post(
 url="https://openrouter.ai/api/v1/chat/completions",
 headers={
   "Authorization": "Bearer <OPENROUTER_API_KEY>",
   "HTTP-Referer": "<YOUR_SITE_URL>", # Optional. Site URL for rankings on openrouter.ai.
   "X-Title": "<YOUR_SITE_NAME>", # Optional. Site title for rankings on openrouter.ai.
 },
 data=json.dumps({
   "model": "openai/gpt-4o", # Optional
   "messages": [
     {
       "role": "user",
       "content": "What is the meaning of life?"
     }
   ]
 })
)
```

<mark>æ­¤ä»£ç ç‰‡æ®µä½¿ç”¨ <code>requests</code> åº“ä¸ OpenRouter API äº¤äº’ã€‚å®ƒå‘èŠå¤©å®Œæˆç«¯ç‚¹å‘é€ POST è¯·æ±‚ï¼ŒåŒ…å«ç”¨æˆ·æ¶ˆæ¯ã€‚è¯·æ±‚åŒ…æ‹¬å¸¦æœ‰ API å¯†é’¥çš„æˆæƒæ ‡å¤´å’Œå¯é€‰çš„ç«™ç‚¹ä¿¡æ¯ã€‚ç›®æ ‡æ˜¯ä»æŒ‡å®šçš„è¯­è¨€æ¨¡å‹ï¼ˆåœ¨æœ¬ä¾‹ä¸­ä¸º <code>openai/gpt-4o</code>ï¼‰è·å–å“åº”ã€‚</mark>

Openrouter offers two distinct methodologies for routing and determining the computational model used to process a given request.

<mark>OpenRouter æä¾›ä¸¤ç§ä¸åŒçš„æ–¹æ³•æ¥è·¯ç”±å’Œç¡®å®šç”¨äºå¤„ç†ç»™å®šè¯·æ±‚çš„è®¡ç®—æ¨¡å‹ã€‚</mark>

* **Automated Model Selection**: This function routes a request to an optimized model chosen from a curated set of available models. The selection is predicated on the specific content of the user's prompt. The identifier of the model that ultimately processes the request is returned in the response's metadata.

<mark>* <strong>è‡ªåŠ¨æ¨¡å‹é€‰æ‹©ï¼š</strong>æ­¤åŠŸèƒ½å°†è¯·æ±‚è·¯ç”±åˆ°ä»ç²¾é€‰çš„å¯ç”¨æ¨¡å‹é›†ä¸­é€‰æ‹©çš„ä¼˜åŒ–æ¨¡å‹ã€‚é€‰æ‹©åŸºäºç”¨æˆ·æç¤ºçš„å…·ä½“å†…å®¹ã€‚æœ€ç»ˆå¤„ç†è¯·æ±‚çš„æ¨¡å‹æ ‡è¯†ç¬¦åœ¨å“åº”çš„å…ƒæ•°æ®ä¸­è¿”å›ã€‚</mark>

```json
{
 "model": "openrouter/auto",
 ... // Other params
}
```

* **Sequential Model Fallback**: This mechanism provides operational redundancy by allowing users to specify a hierarchical list of models. The system will first attempt to process the request with the primary model designated in the sequence. Should this primary model fail to respond due to any number of error conditionsâ€”such as service unavailability, rate-limiting, or content filteringâ€”the system will automatically re-route the request to the next specified model in the sequence. This process continues until a model in the list successfully executes the request or the list is exhausted. The final cost of the operation and the model identifier returned in the response will correspond to the model that successfully completed the computation.

<mark>* <strong>é¡ºåºæ¨¡å‹å›é€€ï¼š</strong>æ­¤æœºåˆ¶é€šè¿‡å…è®¸ç”¨æˆ·æŒ‡å®šåˆ†å±‚æ¨¡å‹åˆ—è¡¨æ¥æä¾›æ“ä½œå†—ä½™ã€‚ç³»ç»Ÿå°†é¦–å…ˆå°è¯•ä½¿ç”¨åºåˆ—ä¸­æŒ‡å®šçš„ä¸»è¦æ¨¡å‹å¤„ç†è¯·æ±‚ã€‚å¦‚æœä¸»è¦æ¨¡å‹ç”±äºå¤šç§é”™è¯¯æ¡ä»¶ï¼ˆå¦‚æœåŠ¡ä¸å¯ç”¨ã€é€Ÿç‡é™åˆ¶æˆ–å†…å®¹è¿‡æ»¤ï¼‰è€Œæ— æ³•å“åº”ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨å°†è¯·æ±‚é‡æ–°è·¯ç”±åˆ°åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªæŒ‡å®šæ¨¡å‹ã€‚æ­¤è¿‡ç¨‹å°†æŒç»­è¿›è¡Œï¼Œç›´åˆ°åˆ—è¡¨ä¸­çš„æŸä¸ªæ¨¡å‹æˆåŠŸæ‰§è¡Œè¯·æ±‚æˆ–åˆ—è¡¨è€—å°½ã€‚æ“ä½œçš„æœ€ç»ˆæˆæœ¬å’Œå“åº”ä¸­è¿”å›çš„æ¨¡å‹æ ‡è¯†ç¬¦å°†å¯¹åº”äºæˆåŠŸå®Œæˆè®¡ç®—çš„æ¨¡å‹ã€‚</mark>

```json
{
 "models": ["anthropic/claude-3.5-sonnet", "gryphe/mythomax-l2-13b"],
 ... // Other params
}
```

OpenRouter offers a detailed leaderboard (https://openrouter.ai/rankings) which ranks available AI models based on their cumulative token production. It also offers latest models from different providers (ChatGPT, Gemini, Claude) (see Fig. 1)

<mark>OpenRouter æä¾›è¯¦ç»†çš„æ’è¡Œæ¦œï¼ˆhttps://openrouter.ai/rankingsï¼‰ï¼Œæ ¹æ®ç´¯ç§¯ä»¤ç‰Œç”Ÿäº§é‡å¯¹å¯ç”¨çš„ AI æ¨¡å‹è¿›è¡Œæ’åã€‚å®ƒè¿˜æä¾›æ¥è‡ªä¸åŒæä¾›å•†ï¼ˆChatGPTã€Geminiã€Claudeï¼‰çš„æœ€æ–°æ¨¡å‹ï¼ˆè§å›¾ 1ï¼‰ã€‚</mark>

![OpenRouter Web site](/images/chapter16_fig1.png)

<mark><strong>å›¾ 1ï¼š</strong>OpenRouter ç½‘ç«™ (https://openrouter.ai/)</mark>

---

## Beyond Dynamic Model Switching: A Spectrum of Agent Resource Optimizations | <mark>è¶…è¶ŠåŠ¨æ€æ¨¡å‹åˆ‡æ¢ï¼šæ™ºèƒ½ä½“èµ„æºä¼˜åŒ–çš„å…¨æ™¯</mark>

Resource-aware optimization is paramount in developing intelligent agent systems that operate efficiently and effectively within real-world constraints. Let's see a number of additional techniques:

<mark>èµ„æºæ„ŸçŸ¥ä¼˜åŒ–åœ¨å¼€å‘èƒ½å¤Ÿåœ¨ç°å®ä¸–ç•Œçº¦æŸä¸‹é«˜æ•ˆä¸”æœ‰æ•ˆè¿è¡Œçš„å…·æ™ºèƒ½ä½“ç‰¹æ€§çš„ç³»ç»Ÿä¸­è‡³å…³é‡è¦ã€‚è®©æˆ‘ä»¬çœ‹çœ‹è®¸å¤šå…¶ä»–æŠ€æœ¯ï¼š</mark>

**Dynamic Model Switching** is a critical technique involving the strategic selection of large language models based on the intricacies of the task at hand and the available computational resources. When faced with simple queries, a lightweight, cost-effective LLM can be deployed, whereas complex, multifaceted problems necessitate the utilization of more sophisticated and resource-intensive models.

<mark><strong>åŠ¨æ€æ¨¡å‹åˆ‡æ¢</strong>æ˜¯ä¸€ç§å…³é”®æŠ€æœ¯ï¼Œæ¶‰åŠåŸºäºæ‰‹å¤´ä»»åŠ¡çš„å¤æ‚æ€§å’Œå¯ç”¨è®¡ç®—èµ„æºå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„æˆ˜ç•¥é€‰æ‹©ã€‚é¢å¯¹ç®€å•æŸ¥è¯¢æ—¶ï¼Œå¯ä»¥éƒ¨ç½²è½»é‡çº§ã€æˆæœ¬æ•ˆç›Šé«˜çš„ LLMï¼Œè€Œå¤æ‚çš„å¤šæ–¹é¢é—®é¢˜åˆ™éœ€è¦ä½¿ç”¨æ›´å¤æ‚å’Œèµ„æºå¯†é›†çš„æ¨¡å‹ã€‚</mark>

**Adaptive Tool Use & Selection** ensures agents can intelligently choose from a suite of tools, selecting the most appropriate and efficient one for each specific sub-task, with careful consideration given to factors like API usage costs, latency, and execution time. This dynamic tool selection enhances overall system efficiency by optimizing the use of external APIs and services.

<mark><strong>è‡ªé€‚åº”å·¥å…·ä½¿ç”¨ä¸é€‰æ‹©</strong>ç¡®ä¿æ™ºèƒ½ä½“èƒ½å¤Ÿä»ä¸€ç»„å·¥å…·ä¸­æ™ºèƒ½é€‰æ‹©ï¼Œä¸ºæ¯ä¸ªç‰¹å®šå­ä»»åŠ¡é€‰æ‹©æœ€åˆé€‚å’Œæœ€é«˜æ•ˆçš„å·¥å…·ï¼Œå¹¶ä»”ç»†è€ƒè™‘ API ä½¿ç”¨æˆæœ¬ã€å»¶è¿Ÿå’Œæ‰§è¡Œæ—¶é—´ç­‰å› ç´ ã€‚è¿™ç§åŠ¨æ€å·¥å…·é€‰æ‹©é€šè¿‡ä¼˜åŒ–å¤–éƒ¨ API å’ŒæœåŠ¡çš„ä½¿ç”¨æ¥æé«˜æ•´ä½“ç³»ç»Ÿæ•ˆç‡ã€‚</mark>

**Contextual Pruning & Summarization** plays a vital role in managing the amount of information processed by agents, strategically minimizing the prompt token count and reducing inference costs by intelligently summarizing and selectively retaining only the most relevant information from the interaction history, preventing unnecessary computational overhead.

<mark><strong>ä¸Šä¸‹æ–‡ä¿®å‰ªä¸æ‘˜è¦</strong>åœ¨ç®¡ç†æ™ºèƒ½ä½“å¤„ç†çš„ä¿¡æ¯é‡æ–¹é¢å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œé€šè¿‡æ™ºèƒ½æ‘˜è¦å’Œé€‰æ‹©æ€§åœ°ä»…ä¿ç•™äº¤äº’å†å²ä¸­æœ€ç›¸å…³çš„ä¿¡æ¯æ¥æˆ˜ç•¥æ€§åœ°æœ€å°åŒ–æç¤ºä»¤ç‰Œæ•°å¹¶é™ä½æ¨ç†æˆæœ¬ï¼Œä»è€Œé˜²æ­¢ä¸å¿…è¦çš„è®¡ç®—å¼€é”€ã€‚</mark>

**Proactive Resource Prediction** involves anticipating resource demands by forecasting future workloads and system requirements, which allows for proactive allocation and management of resources, ensuring system responsiveness and preventing bottlenecks.

<mark><strong>ä¸»åŠ¨èµ„æºé¢„æµ‹</strong>æ¶‰åŠé€šè¿‡é¢„æµ‹æœªæ¥å·¥ä½œè´Ÿè½½å’Œç³»ç»Ÿéœ€æ±‚æ¥é¢„æµ‹èµ„æºéœ€æ±‚ï¼Œä»è€Œå…è®¸ä¸»åŠ¨åˆ†é…å’Œç®¡ç†èµ„æºï¼Œç¡®ä¿ç³»ç»Ÿå“åº”æ€§å¹¶é˜²æ­¢ç“¶é¢ˆã€‚</mark>

**Cost-Sensitive Exploration** in multi-agent systems extends optimization considerations to encompass communication costs alongside traditional computational costs, influencing the strategies employed by agents to collaborate and share information, aiming to minimize the overall resource expenditure.

<mark><strong>æˆæœ¬æ•æ„Ÿæ¢ç´¢</strong>åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­å°†ä¼˜åŒ–è€ƒè™‘æ‰©å±•åˆ°åŒ…æ‹¬é€šä¿¡æˆæœ¬ä»¥åŠä¼ ç»Ÿè®¡ç®—æˆæœ¬ï¼Œå½±å“æ™ºèƒ½ä½“åä½œå’Œå…±äº«ä¿¡æ¯æ‰€é‡‡ç”¨çš„ç­–ç•¥ï¼Œæ—¨åœ¨æœ€å°åŒ–æ•´ä½“èµ„æºæ”¯å‡ºã€‚</mark>

**Energy-Efficient Deployment** is specifically tailored for environments with stringent resource constraints, aiming to minimize the energy footprint of intelligent agent systems, extending operational time and reducing overall running costs.

<mark><strong>èƒ½æºé«˜æ•ˆéƒ¨ç½²</strong>ä¸“é—¨é’ˆå¯¹å…·æœ‰ä¸¥æ ¼èµ„æºçº¦æŸçš„ç¯å¢ƒè€Œå®šåˆ¶ï¼Œæ—¨åœ¨æœ€å°åŒ–å…·æ™ºèƒ½ä½“ç‰¹æ€§ç³»ç»Ÿçš„èƒ½æºè¶³è¿¹ï¼Œå»¶é•¿è¿è¡Œæ—¶é—´å¹¶é™ä½æ•´ä½“è¿è¡Œæˆæœ¬ã€‚</mark>

**Parallelization & Distributed Computing Awareness** leverages distributed resources to enhance the processing power and throughput of agents, distributing computational workloads across multiple machines or processors to achieve greater efficiency and faster task completion.

<mark><strong>å¹¶è¡ŒåŒ–ä¸åˆ†å¸ƒå¼è®¡ç®—æ„ŸçŸ¥</strong>åˆ©ç”¨åˆ†å¸ƒå¼èµ„æºæ¥å¢å¼ºæ™ºèƒ½ä½“çš„å¤„ç†èƒ½åŠ›å’Œååé‡ï¼Œå°†è®¡ç®—å·¥ä½œè´Ÿè½½åˆ†å¸ƒåˆ°å¤šå°æœºå™¨æˆ–å¤„ç†å™¨ä¸Šï¼Œä»¥å®ç°æ›´é«˜çš„æ•ˆç‡å’Œæ›´å¿«çš„ä»»åŠ¡å®Œæˆã€‚</mark>

**Learned Resource Allocation Policies** introduce a learning mechanism, enabling agents to adapt and optimize their resource allocation strategies over time based on feedback and performance metrics, improving efficiency through continuous refinement.

<mark><strong>å­¦ä¹ å‹èµ„æºåˆ†é…ç­–ç•¥</strong>å¼•å…¥å­¦ä¹ æœºåˆ¶ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤ŸåŸºäºåé¦ˆå’Œæ€§èƒ½æŒ‡æ ‡éšæ—¶é—´æ¨ç§»é€‚åº”å’Œä¼˜åŒ–å…¶èµ„æºåˆ†é…ç­–ç•¥ï¼Œé€šè¿‡æŒç»­æ”¹è¿›æé«˜æ•ˆç‡ã€‚</mark>

**Graceful Degradation and Fallback Mechanisms** ensure that intelligent agent systems can continue to function, albeit perhaps at a reduced capacity, even when resource constraints are severe, gracefully degrading performance and falling back to alternative strategies to maintain operation and provide essential functionality.

<mark><strong>ä¼˜é›…é™çº§å’Œåå¤‡æœºåˆ¶</strong>ç¡®ä¿å…·æ™ºèƒ½ä½“ç‰¹æ€§çš„ç³»ç»Ÿå³ä½¿åœ¨èµ„æºçº¦æŸä¸¥é‡æ—¶ä¹Ÿèƒ½ç»§ç»­è¿è¡Œï¼ˆå°½ç®¡å¯èƒ½ä»¥é™ä½çš„å®¹é‡ï¼‰ï¼Œä¼˜é›…åœ°é™ä½æ€§èƒ½å¹¶å›é€€åˆ°æ›¿ä»£ç­–ç•¥ä»¥ç»´æŒè¿è¡Œå¹¶æä¾›åŸºæœ¬åŠŸèƒ½ã€‚</mark>

---

## At a Glance | <mark>è¦ç‚¹é€Ÿè§ˆ</mark>

**What**: Resource-Aware Optimization addresses the challenge of managing the consumption of computational, temporal, and financial resources in intelligent systems. LLM-based applications can be expensive and slow, and selecting the best model or tool for every task is often inefficient. This creates a fundamental trade-off between the quality of a system's output and the resources required to produce it. Without a dynamic management strategy, systems cannot adapt to varying task complexities or operate within budgetary and performance constraints.

<mark><strong>é—®é¢˜æ‰€åœ¨ï¼š</strong>èµ„æºæ„ŸçŸ¥ä¼˜åŒ–è§£å†³äº†åœ¨å…·æ™ºèƒ½ä½“ç‰¹æ€§çš„ç³»ç»Ÿä¸­ç®¡ç†è®¡ç®—ã€æ—¶é—´å’Œè´¢åŠ¡èµ„æºæ¶ˆè€—çš„æŒ‘æˆ˜ã€‚åŸºäº LLM çš„åº”ç”¨ç¨‹åºå¯èƒ½æ—¢æ˜‚è´µåˆç¼“æ…¢ï¼Œä¸ºæ¯ä¸ªä»»åŠ¡é€‰æ‹©æœ€ä½³æ¨¡å‹æˆ–å·¥å…·å¾€å¾€æ•ˆç‡ä½ä¸‹ã€‚è¿™åœ¨ç³»ç»Ÿè¾“å‡ºè´¨é‡ä¸ç”Ÿæˆæ‰€éœ€èµ„æºä¹‹é—´äº§ç”Ÿäº†æ ¹æœ¬æ€§æƒè¡¡ã€‚å¦‚æœæ²¡æœ‰åŠ¨æ€ç®¡ç†ç­–ç•¥ï¼Œç³»ç»Ÿæ— æ³•é€‚åº”ä¸åŒçš„ä»»åŠ¡å¤æ‚æ€§æˆ–åœ¨é¢„ç®—å’Œæ€§èƒ½çº¦æŸå†…è¿è¡Œã€‚</mark>

**Why**: The standardized solution is to build an agentic system that intelligently monitors and allocates resources based on the task at hand. This pattern typically employs a "Router Agent" to first classify the complexity of an incoming request. The request is then forwarded to the most suitable LLM or toolâ€”a fast, inexpensive model for simple queries, and a more powerful one for complex reasoning. A "Critique Agent" can further refine the process by evaluating the quality of the response, providing feedback to improve the routing logic over time. This dynamic, multi-agent approach ensures the system operates efficiently, balancing response quality with cost-effectiveness.

<mark><strong>è§£å†³ä¹‹é“ï¼š</strong>æ ‡å‡†åŒ–è§£å†³æ–¹æ¡ˆæ˜¯æ„å»ºä¸€ä¸ªå…·æ™ºèƒ½ä½“ç‰¹æ€§çš„ç³»ç»Ÿï¼Œæ ¹æ®æ‰‹å¤´çš„ä»»åŠ¡æ™ºèƒ½åœ°ç›‘æ§å’Œåˆ†é…èµ„æºã€‚æ­¤æ¨¡å¼é€šå¸¸ä½¿ç”¨ã€Œè·¯ç”±æ™ºèƒ½ä½“ã€é¦–å…ˆå¯¹ä¼ å…¥è¯·æ±‚çš„å¤æ‚æ€§è¿›è¡Œåˆ†ç±»ã€‚ç„¶åå°†è¯·æ±‚è½¬å‘åˆ°æœ€åˆé€‚çš„ LLM æˆ–å·¥å…·â€”â€”ç®€å•æŸ¥è¯¢ä½¿ç”¨å¿«é€Ÿã€å»‰ä»·çš„æ¨¡å‹ï¼Œå¤æ‚æ¨ç†ä½¿ç”¨æ›´å¼ºå¤§çš„æ¨¡å‹ã€‚ã€Œè¯„è®ºæ™ºèƒ½ä½“ã€å¯ä»¥é€šè¿‡è¯„ä¼°å“åº”è´¨é‡è¿›ä¸€æ­¥æ”¹è¿›æµç¨‹ï¼Œæä¾›åé¦ˆä»¥éšæ—¶é—´æ”¹è¿›è·¯ç”±é€»è¾‘ã€‚è¿™ç§åŠ¨æ€çš„å¤šæ™ºèƒ½ä½“æ–¹æ³•ç¡®ä¿ç³»ç»Ÿé«˜æ•ˆè¿è¡Œï¼Œåœ¨å“åº”è´¨é‡å’Œæˆæœ¬æ•ˆç›Šä¹‹é—´å–å¾—å¹³è¡¡ã€‚</mark>

**Rule of thumb**: Use this pattern when operating under strict financial budgets for API calls or computational power, building latency-sensitive applications where quick response times are critical, deploying agents on resource-constrained hardware such as edge devices with limited battery life, programmatically balancing the trade-off between response quality and operational cost, and managing complex, multi-step workflows where different tasks have varying resource requirements.

<mark><strong>ç»éªŒæ³•åˆ™ï¼š</strong>åœ¨ä»¥ä¸‹æƒ…å†µä¸‹ä½¿ç”¨æ­¤æ¨¡å¼ï¼šåœ¨ API è°ƒç”¨æˆ–è®¡ç®—èƒ½åŠ›çš„ä¸¥æ ¼è´¢åŠ¡é¢„ç®—ä¸‹è¿è¡Œï¼›æ„å»ºå¯¹å»¶è¿Ÿæ•æ„Ÿçš„åº”ç”¨ç¨‹åºï¼Œå…¶ä¸­å¿«é€Ÿå“åº”æ—¶é—´è‡³å…³é‡è¦ï¼›åœ¨èµ„æºå—é™çš„ç¡¬ä»¶ï¼ˆå¦‚ç”µæ± å¯¿å‘½æœ‰é™çš„è¾¹ç¼˜è®¾å¤‡ï¼‰ä¸Šéƒ¨ç½²æ™ºèƒ½ä½“ï¼›ä»¥ç¼–ç¨‹æ–¹å¼å¹³è¡¡å“åº”è´¨é‡å’Œè¿è¥æˆæœ¬ä¹‹é—´çš„æƒè¡¡ï¼›ä»¥åŠç®¡ç†å¤æ‚çš„å¤šæ­¥éª¤å·¥ä½œæµç¨‹ï¼Œå…¶ä¸­ä¸åŒä»»åŠ¡å…·æœ‰ä¸åŒçš„èµ„æºéœ€æ±‚ã€‚</mark>

---

## Visual Summary | <mark>å¯è§†åŒ–æ‘˜è¦</mark>

![Resource-Aware Optimization Design Pattern](/images/chapter16_fig2.png)

<mark><strong>å›¾ 2ï¼š</strong>èµ„æºæ„ŸçŸ¥ä¼˜åŒ–è®¾è®¡æ¨¡å¼</mark>

---

## Key Takeaways | <mark>æ ¸å¿ƒè¦ç‚¹</mark>

* **Resource-Aware Optimization is Essential**: Intelligent agents can manage computational, temporal, and financial resources dynamically. Decisions regarding model usage and execution paths are made based on real-time constraints and objectives.

<mark>* <strong>èµ„æºæ„ŸçŸ¥ä¼˜åŒ–è‡³å…³é‡è¦ï¼š</strong>å…·æ™ºèƒ½ä½“ç‰¹æ€§çš„ç³»ç»Ÿå¯ä»¥åŠ¨æ€ç®¡ç†è®¡ç®—ã€æ—¶é—´å’Œè´¢åŠ¡èµ„æºã€‚å…³äºæ¨¡å‹ä½¿ç”¨å’Œæ‰§è¡Œè·¯å¾„çš„å†³ç­–åŸºäºå®æ—¶çº¦æŸå’Œç›®æ ‡åšå‡ºã€‚</mark>

* **Multi-Agent Architecture for Scalability**: Google's ADK provides a multi-agent framework, enabling modular design. Different agents (answering, routing, critique) handle specific tasks.

<mark>* <strong>å¯æ‰©å±•çš„å¤šæ™ºèƒ½ä½“æ¶æ„ï¼š</strong>Google çš„ ADK æä¾›å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ”¯æŒæ¨¡å—åŒ–è®¾è®¡ã€‚ä¸åŒçš„æ™ºèƒ½ä½“ï¼ˆå›ç­”ã€è·¯ç”±ã€è¯„è®ºï¼‰å¤„ç†ç‰¹å®šä»»åŠ¡ã€‚</mark>

* **Dynamic, LLM-Driven Routing**: A Router Agent directs queries to language models (Gemini Flash for simple, Gemini Pro for complex) based on query complexity and budget. This optimizes cost and performance.

<mark>* <strong>åŠ¨æ€çš„ã€ç”± LLM é©±åŠ¨çš„è·¯ç”±ï¼š</strong>è·¯ç”±æ™ºèƒ½ä½“æ ¹æ®æŸ¥è¯¢å¤æ‚æ€§å’Œé¢„ç®—å°†æŸ¥è¯¢å¼•å¯¼åˆ°è¯­è¨€æ¨¡å‹ï¼ˆç®€å•æŸ¥è¯¢ä½¿ç”¨ Gemini Flashï¼Œå¤æ‚æŸ¥è¯¢ä½¿ç”¨ Gemini Proï¼‰ã€‚è¿™ä¼˜åŒ–äº†æˆæœ¬å’Œæ€§èƒ½ã€‚</mark>

* **Critique Agent Functionality**: A dedicated Critique Agent provides feedback for self-correction, performance monitoring, and refining routing logic, enhancing system effectiveness.

<mark>* <strong>è¯„è®ºæ™ºèƒ½ä½“åŠŸèƒ½ï¼š</strong>ä¸“é—¨çš„è¯„è®ºæ™ºèƒ½ä½“ä¸ºè‡ªæˆ‘ä¿®æ­£ã€æ€§èƒ½ç›‘æ§å’Œæ”¹è¿›è·¯ç”±é€»è¾‘æä¾›åé¦ˆï¼Œå¢å¼ºç³»ç»Ÿæœ‰æ•ˆæ€§ã€‚</mark>

* **Optimization Through Feedback and Flexibility**: Evaluation capabilities for critique and model integration flexibility contribute to adaptive and self-improving system behavior.

<mark>* <strong>é€šè¿‡åé¦ˆå’Œçµæ´»æ€§è¿›è¡Œä¼˜åŒ–ï¼š</strong>è¯„è®ºï¼ˆæ™ºèƒ½ä½“ï¼‰çš„è¯„ä¼°èƒ½åŠ›å’Œæ¨¡å‹é›†æˆçµæ´»æ€§æœ‰åŠ©äºè‡ªé€‚åº”å’Œè‡ªæˆ‘æ”¹è¿›çš„ç³»ç»Ÿè¡Œä¸ºã€‚</mark>

* **Additional Resource-Aware Optimizations**: Other methods include Adaptive Tool Use & Selection, Contextual Pruning & Summarization, Proactive Resource Prediction, Cost-Sensitive Exploration in Multi-Agent Systems, Energy-Efficient Deployment, Parallelization & Distributed Computing Awareness, Learned Resource Allocation Policies, Graceful Degradation and Fallback Mechanisms, and Prioritization of Critical Tasks.

<mark>* <strong>å…¶ä»–èµ„æºæ„ŸçŸ¥ä¼˜åŒ–ï¼š</strong>å…¶ä»–æ–¹æ³•åŒ…æ‹¬è‡ªé€‚åº”å·¥å…·ä½¿ç”¨ä¸é€‰æ‹©ã€ä¸Šä¸‹æ–‡ä¿®å‰ªä¸æ‘˜è¦ã€ä¸»åŠ¨èµ„æºé¢„æµ‹ã€å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„æˆæœ¬æ•æ„Ÿæ¢ç´¢ã€èƒ½æºé«˜æ•ˆéƒ¨ç½²ã€å¹¶è¡ŒåŒ–ä¸åˆ†å¸ƒå¼è®¡ç®—æ„ŸçŸ¥ã€å­¦ä¹ å‹èµ„æºåˆ†é…ç­–ç•¥ã€ä¼˜é›…é™çº§å’Œåå¤‡æœºåˆ¶ï¼Œä»¥åŠå…³é”®ä»»åŠ¡ä¼˜å…ˆçº§æ’åºã€‚</mark>

---

## Conclusions | <mark>ç»“è®º</mark>

Resource-aware optimization is essential for the development of intelligent agents, enabling efficient operation within real-world constraints. By managing computational, temporal, and financial resources, agents can achieve optimal performance and cost-effectiveness. Techniques such as dynamic model switching, adaptive tool use, and contextual pruning are crucial for attaining these efficiencies. Advanced strategies, including learned resource allocation policies and graceful degradation, enhance an agent's adaptability and resilience under varying conditions. Integrating these optimization principles into agent design is fundamental for building scalable, robust, and sustainable AI systems.

<mark>èµ„æºæ„ŸçŸ¥ä¼˜åŒ–å¯¹äºå…·æ™ºèƒ½ä½“ç‰¹æ€§ç³»ç»Ÿçš„å¼€å‘è‡³å…³é‡è¦ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨ç°å®ä¸–ç•Œçº¦æŸä¸‹é«˜æ•ˆè¿è¡Œã€‚é€šè¿‡ç®¡ç†è®¡ç®—ã€æ—¶é—´å’Œè´¢åŠ¡èµ„æºï¼Œæ™ºèƒ½ä½“å¯ä»¥å®ç°æœ€ä½³æ€§èƒ½å’Œæˆæœ¬æ•ˆç›Šã€‚åŠ¨æ€æ¨¡å‹åˆ‡æ¢ã€è‡ªé€‚åº”å·¥å…·ä½¿ç”¨å’Œä¸Šä¸‹æ–‡ä¿®å‰ªç­‰æŠ€æœ¯å¯¹äºå®ç°è¿™äº›æ•ˆç‡è‡³å…³é‡è¦ã€‚åŒ…æ‹¬å­¦ä¹ å‹èµ„æºåˆ†é…ç­–ç•¥å’Œä¼˜é›…é™çº§åœ¨å†…çš„é«˜çº§ç­–ç•¥å¢å¼ºäº†æ™ºèƒ½ä½“åœ¨ä¸åŒæ¡ä»¶ä¸‹çš„é€‚åº”æ€§å’ŒéŸ§æ€§ã€‚å°†è¿™äº›ä¼˜åŒ–åŸåˆ™æ•´åˆåˆ°æ™ºèƒ½ä½“è®¾è®¡ä¸­æ˜¯æ„å»ºå¯æ‰©å±•ã€ç¨³å¥å’Œå¯æŒç»­çš„ AI ç³»ç»Ÿçš„åŸºç¡€ã€‚</mark>

---

## References

<mark>å‚è€ƒæ–‡çŒ®</mark>

1. Google's Agent Development Kit (ADK): [google.github.io/adk-docs](https://google.github.io/adk-docs/)
2. Gemini Flash 2.5 & Gemini 2.5 Pro: [aistudio.google.com](https://aistudio.google.com/)
3. OpenRouter: [openrouter.ai/docs/quickstart](https://openrouter.ai/docs/quickstart)

1. <mark>Google æ™ºèƒ½ä½“å¼€å‘å·¥å…·åŒ…ï¼ˆADKï¼‰ï¼š[google.github.io/adk-docs](https://google.github.io/adk-docs/)</mark>
2. <mark>Gemini Flash 2.5 å’Œ Gemini 2.5 Proï¼š[aistudio.google.com](https://aistudio.google.com/)</mark>
3. <mark>OpenRouterï¼š[openrouter.ai/docs/quickstart](https://openrouter.ai/docs/quickstart)</mark>

