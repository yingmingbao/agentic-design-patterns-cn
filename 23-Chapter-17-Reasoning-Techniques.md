------

# Chapter 17: Reasoning Techniques | <mark>ç¬¬ 17 ç« ï¼šæ¨ç†æŠ€æœ¯</mark>

This chapter delves into advanced reasoning methodologies for intelligent agents, focusing on multi-step logical inferences and problem-solving. These techniques go beyond simple sequential operations, making the agent's internal reasoning explicit. This allows agents to break down problems, consider intermediate steps, and reach more robust and accurate conclusions.  A core principle among these advanced methods is the allocation of increased computational resources during inference. This means granting the agent, or the underlying LLM, more processing time or steps to process a query and generate a response. Rather than a quick, single pass, the agent can engage in iterative refinement, explore multiple solution paths, or utilize external tools. This extended processing time during inference often significantly enhances accuracy, coherence, and robustness, especially for complex problems requiring deeper analysis and deliberation.

<mark>æœ¬ç« æ·±å…¥æ¢è®¨äº†æ™ºèƒ½ä½“çš„å…ˆè¿›æ¨ç†æ–¹æ³•ï¼Œé‡ç‚¹ä»‹ç»å¤šæ­¥é€»è¾‘æ¨ç†å’Œé—®é¢˜è§£å†³æŠ€æœ¯ã€‚è¿™äº›æŠ€æœ¯è¶…è¶Šäº†ç®€å•çš„é¡ºåºæ“ä½œï¼Œä½¿æ™ºèƒ½ä½“çš„å†…éƒ¨æ¨ç†è¿‡ç¨‹æ›´åŠ æ˜ç¡®ã€‚è¿™ä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿåˆ†è§£é—®é¢˜ã€è€ƒè™‘ä¸­é—´æ­¥éª¤ï¼Œå¹¶å¾—å‡ºæ›´åŠ ç¨³å¥å’Œå‡†ç¡®çš„ç»“è®ºã€‚åœ¨è¿™äº›å…ˆè¿›æ–¹æ³•ä¸­ï¼Œä¸€ä¸ªæ ¸å¿ƒåŸåˆ™æ˜¯åœ¨æ¨ç†è¿‡ç¨‹ä¸­åˆ†é…æ›´å¤šçš„è®¡ç®—èµ„æºã€‚è¿™æ„å‘³ç€ç»™äºˆæ™ºèƒ½ä½“æˆ–åº•å±‚ LLM æ›´å¤šçš„å¤„ç†æ—¶é—´æˆ–æ­¥éª¤æ¥å¤„ç†æŸ¥è¯¢å¹¶ç”Ÿæˆå“åº”ã€‚æ™ºèƒ½ä½“å¯ä»¥è¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€æ¢ç´¢å¤šç§è§£å†³æ–¹æ¡ˆè·¯å¾„æˆ–åˆ©ç”¨å¤–éƒ¨å·¥å…·ï¼Œè€Œä¸æ˜¯è¿›è¡Œå¿«é€Ÿçš„å•æ¬¡å¤„ç†ã€‚è¿™ç§åœ¨æ¨ç†è¿‡ç¨‹ä¸­å»¶é•¿çš„å¤„ç†æ—¶é—´é€šå¸¸èƒ½æ˜¾è‘—æé«˜å‡†ç¡®æ€§ã€è¿è´¯æ€§å’Œç¨³å¥æ€§ï¼Œå°¤å…¶å¯¹äºéœ€è¦æ·±å…¥åˆ†æå’Œæ€è€ƒçš„å¤æ‚é—®é¢˜ã€‚</mark>

## Practical Applications & Use Cases | <mark>å®é™…åº”ç”¨ä¸ä½¿ç”¨æ¡ˆä¾‹</mark>

Practical applications include:

<mark>å®é™…åº”ç”¨åŒ…æ‹¬ï¼š</mark>

â—**Complex Question Answering**: Facilitating the resolution of multi-hop queries, which necessitate the integration of data from diverse sources and the execution of logical deductions, potentially involving the examination of multiple reasoning paths, and benefiting from extended inference time to synthesize information.

<mark>â—**å¤æ‚é—®ç­”**ï¼šä¿ƒè¿›å¤šè·³æŸ¥è¯¢çš„è§£å†³ï¼Œè¿™ç±»æŸ¥è¯¢éœ€è¦æ•´åˆæ¥è‡ªä¸åŒæ¥æºçš„æ•°æ®å¹¶æ‰§è¡Œé€»è¾‘æ¨ç†ï¼Œå¯èƒ½æ¶‰åŠæ£€æŸ¥å¤šæ¡æ¨ç†è·¯å¾„ï¼Œå¹¶å¾—ç›Šäºæ›´é•¿çš„æ¨ç†æ—¶é—´æ¥ç»¼åˆä¿¡æ¯ã€‚</mark>

â—**Mathematical Problem Solving**: Enabling the division of mathematical problems into smaller, solvable components, illustrating the step-by-step process, and employing code execution for precise computations, where prolonged inference enables more intricate code generation and validation.

<mark>â—**æ•°å­¦é—®é¢˜è§£å†³**ï¼šå°†æ•°å­¦é—®é¢˜åˆ†è§£ä¸ºæ›´å°ã€å¯è§£å†³çš„ç»„æˆéƒ¨åˆ†ï¼Œå±•ç¤ºé€æ­¥è§£å†³è¿‡ç¨‹ï¼Œå¹¶ä½¿ç”¨ä»£ç æ‰§è¡Œè¿›è¡Œç²¾ç¡®è®¡ç®—ï¼Œå…¶ä¸­é•¿æ—¶é—´çš„æ¨ç†èƒ½å¤Ÿæ”¯æŒæ›´å¤æ‚çš„ä»£ç ç”Ÿæˆå’ŒéªŒè¯ã€‚</mark>

â—**Code Debugging and Generation**: Supporting an agent's explanation of its rationale for generating or correcting code, pinpointing potential issues sequentially, and iteratively refining the code based on test results (Self-Correction), leveraging extended inference time for thorough debugging cycles.

<mark>â—**ä»£ç è°ƒè¯•ä¸ç”Ÿæˆ**ï¼šæ”¯æŒæ™ºèƒ½ä½“å¯¹å…¶ç”Ÿæˆæˆ–ä¿®æ­£ä»£ç çš„æ¨ç†ä¾æ®è¿›è¡Œè§£é‡Šï¼Œé¡ºåºè¯†åˆ«æ½œåœ¨é—®é¢˜ï¼Œå¹¶æ ¹æ®æµ‹è¯•ç»“æœè¿­ä»£ä¼˜åŒ–ä»£ç ï¼ˆè‡ªæˆ‘ä¿®æ­£ï¼‰ï¼Œåˆ©ç”¨æ‰©å±•çš„æ¨ç†æ—¶é—´è¿›è¡Œå½»åº•çš„è°ƒè¯•å‘¨æœŸã€‚</mark>

â—**Strategic Planning**: Assisting in the development of comprehensive plans through reasoning across various options, consequences, and preconditions, and adjusting plans based on real-time feedback (ReAct), where extended deliberation can lead to more effective and reliable plans.

<mark>â—**æˆ˜ç•¥è§„åˆ’**ï¼šé€šè¿‡æ¨ç†å„ç§é€‰é¡¹ã€ç»“æœå’Œå…ˆå†³æ¡ä»¶æ¥ååŠ©åˆ¶å®šå…¨é¢è®¡åˆ’ï¼Œå¹¶æ ¹æ®å®æ—¶åé¦ˆï¼ˆReActï¼‰è°ƒæ•´è®¡åˆ’ï¼Œå…¶ä¸­æ·±å…¥çš„æ€è€ƒå¯ä»¥å¯¼è‡´æ›´æœ‰æ•ˆå’Œå¯é çš„è®¡åˆ’ã€‚</mark>

â—**Medical Diagnosis**: Aiding an agent in systematically assessing symptoms, test outcomes, and patient histories to reach a diagnosis, articulating its reasoning at each phase, and potentially utilizing external instruments for data retrieval (ReAct). Increased inference time allows for a more comprehensive differential diagnosis.

<mark>â—**åŒ»ç–—è¯Šæ–­**ï¼šå¸®åŠ©æ™ºèƒ½ä½“ç³»ç»Ÿè¯„ä¼°ç—‡çŠ¶ã€æ£€æŸ¥ç»“æœå’Œæ‚£è€…ç—…å²ä»¥åšå‡ºè¯Šæ–­ï¼Œåœ¨æ¯ä¸ªé˜¶æ®µé˜è¿°å…¶æ¨ç†è¿‡ç¨‹ï¼Œå¹¶å¯èƒ½åˆ©ç”¨å¤–éƒ¨å·¥å…·è¿›è¡Œæ•°æ®æ£€ç´¢ï¼ˆReActï¼‰ã€‚å¢åŠ æ¨ç†æ—¶é—´å¯ä»¥å®ç°æ›´å…¨é¢çš„é‰´åˆ«è¯Šæ–­ã€‚</mark>

â—**Legal Analysis**: Supporting the analysis of legal documents and precedents to formulate arguments or provide guidance, detailing the logical steps taken, and ensuring logical consistency through self-correction. Increased inference time allows for more in-depth legal research and argument construction.

<mark>â—**æ³•å¾‹åˆ†æ**ï¼šæ”¯æŒå¯¹æ³•å¾‹æ–‡ä»¶å’Œåˆ¤ä¾‹çš„åˆ†æï¼Œä»¥åˆ¶å®šè®ºç‚¹æˆ–æä¾›æŒ‡å¯¼ï¼Œè¯¦ç»†è¯´æ˜æ‰€é‡‡å–çš„é€»è¾‘æ­¥éª¤ï¼Œå¹¶é€šè¿‡è‡ªçº æ­£ï¼ˆself-correctionï¼‰ç¡®ä¿é€»è¾‘ä¸€è‡´æ€§ã€‚å¢åŠ æ¨ç†æ—¶é—´å¯ä»¥è¿›è¡Œæ›´æ·±å…¥çš„æ³•å¾‹ç ”ç©¶å’Œè®ºç‚¹æ„å»ºã€‚</mark>

## Reasoning techniques
## <mark>æ¨ç†æŠ€å·§</mark>

To start, let's delve into the core reasoning techniques used to enhance the problem-solving abilities of AI models.

<mark>é¦–å…ˆï¼Œæˆ‘ä»¬æ·±å…¥æ¢ç©¶æ—¨åœ¨æå‡ AI æ¨¡å‹é—®é¢˜è§£å†³èƒ½åŠ›çš„æ ¸å¿ƒæ¨ç†æŠ€å·§ã€‚</mark>

**Chain-of-Thought (CoT)** prompting significantly enhances LLMs' complex reasoning abilities by mimicking a step-by-step thought process (see Fig. 1). Instead of providing a direct answer, CoT prompts guide the model to generate a sequence of intermediate reasoning steps. This explicit breakdown allows LLMs to tackle complex problems by decomposing them into smaller, more manageable sub-problems. This technique markedly improves the model's performance on tasks requiring multi-step reasoning, such as arithmetic, common sense reasoning, and symbolic manipulation. A primary advantage of CoT is its ability to transform a difficult, single-step problem into a series of simpler steps, thereby increasing the transparency of the LLM's reasoning process. This approach not only boosts accuracy but also offers valuable insights into the model's decision-making, aiding in debugging and comprehension. CoT can be implemented using various strategies, including offering few-shot examples that demonstrate step-by-step reasoning or simply instructing the model to "think step by step." Its effectiveness stems from its ability to guide the model's internal processing toward a more deliberate and logical progression. As a result, Chain-of-Thought has become a cornerstone technique for enabling advanced reasoning capabilities in contemporary LLMs. This enhanced transparency and breakdown of complex problems into manageable sub-problems is particularly important for autonomous agents, as it enables them to perform more reliable and auditable actions in complex environments.

<mark>**æ€ç»´é“¾ (CoT)** æç¤ºé€šè¿‡æ¨¡ä»¿é€æ­¥æ€è€ƒçš„è¿‡ç¨‹ï¼ˆå‚è§å›¾ 1ï¼‰ï¼Œæ˜¾è‘—å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤æ‚æ¨ç†èƒ½åŠ›ã€‚CoT æç¤ºå¹¶éç›´æ¥ç»™å‡ºç­”æ¡ˆï¼Œè€Œæ˜¯å¼•å¯¼æ¨¡å‹ç”Ÿæˆä¸€ç³»åˆ—ä¸­é—´æ¨ç†æ­¥éª¤ã€‚è¿™ç§æ¸…æ™°çš„æ‹†è§£ä½¿ LLM èƒ½å¤Ÿå°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºæ›´å°ã€æ›´æ˜“å¤„ç†çš„å­é—®é¢˜ï¼Œä»è€Œæ”»å…‹éš¾é¢˜ã€‚è¿™é¡¹æŠ€æœ¯æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨éœ€è¦å¤šæ­¥æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œä¾‹å¦‚ç®—æœ¯ã€å¸¸è¯†æ¨ç†å’Œç¬¦å·æ“ä½œç­‰ã€‚</mark>

<mark>CoT çš„ä¸€ä¸ªä¸»è¦ä¼˜åŠ¿åœ¨äºå®ƒèƒ½å¤Ÿå°†å›°éš¾çš„å•æ­¥é—®é¢˜è½¬åŒ–ä¸ºä¸€ç³»åˆ—ç®€å•æ­¥éª¤ï¼Œè¿›è€Œæé«˜ LLM æ¨ç†è¿‡ç¨‹çš„é€æ˜åº¦ã€‚è¿™ç§æ–¹æ³•ä¸ä»…æé«˜äº†å‡†ç¡®æ€§ï¼Œè¿˜ä¸ºæ¨¡å‹çš„å†³ç­–æä¾›äº†æœ‰ä»·å€¼çš„æ´å¯Ÿï¼Œæœ‰åŠ©äºè°ƒè¯•å’Œç†è§£ã€‚CoT å¯ä»¥é€šè¿‡å¤šç§ç­–ç•¥å®ç°ï¼ŒåŒ…æ‹¬æä¾›å±•ç¤ºé€æ­¥æ¨ç†çš„å°‘æ ·æœ¬ç¤ºä¾‹ï¼Œæˆ–è€…ç›´æ¥æŒ‡ç¤ºæ¨¡å‹â€œé€æ­¥æ€è€ƒâ€ã€‚å…¶æœ‰æ•ˆæ€§æºäºå®ƒèƒ½å¤Ÿå¼•å¯¼æ¨¡å‹çš„å†…éƒ¨å¤„ç†æµç¨‹æœç€æ›´å®¡æ…ã€æ›´é€»è¾‘åŒ–çš„æ–¹å‘å‘å±•ã€‚å› æ­¤ï¼Œæ€ç»´é“¾å·²æˆä¸ºèµ‹èƒ½å½“ä»£ LLM é«˜çº§æ¨ç†èƒ½åŠ›çš„å…³é”®åŸºçŸ³ã€‚</mark>

<mark>è¿™ç§å¢å¼ºçš„é€æ˜åº¦ï¼Œä»¥åŠå°†å¤æ‚é—®é¢˜æ‹†è§£ä¸ºå¯ç®¡ç†å­é—®é¢˜çš„åšæ³•ï¼Œå¯¹äºè‡ªä¸»æ™ºèƒ½ä½“ï¼ˆAutonomous Agentsï¼‰å°¤ä¸ºé‡è¦ï¼Œå› ä¸ºå®ƒä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­æ‰§è¡Œæ›´å¯é ã€æ›´å¯å®¡è®¡çš„è¡ŒåŠ¨ã€‚</mark>

<img width="800" height="564" alt="image" src="https://github.com/user-attachments/assets/3f0623d5-867b-41e0-9880-ff355a76aace" />

Fig. 1: CoT prompt alongside the detailed, step-by-step response generated by the agent.
<mark>å›¾ 1ï¼šæ€ç»´é“¾æç¤ºä»¥åŠæ™ºèƒ½ä½“ç”Ÿæˆçš„è¯¦ç»†ã€é€æ­¥å“åº”ã€‚</mark>

Let's see an example. It begins with a set of instructions that tell the AI how to think, defining its persona and a clear five-step process to follow. This is the prompt that initiates structured thinking.

<mark>è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªä¾‹å­ã€‚å®ƒé¦–å…ˆåŒ…å«ä¸€ç»„æŒ‡ä»¤ï¼Œå‘Šè¯‰ AI å¦‚ä½•æ€è€ƒï¼Œå®šä¹‰å…¶è§’è‰²ä»¥åŠä¸€ä¸ªæ˜ç¡®çš„äº”æ­¥æµç¨‹ã€‚è¿™æ˜¯å¯åŠ¨ç»“æ„åŒ–æ€è€ƒçš„æç¤ºã€‚</mark>

Following that, the example shows the CoT process in action. The section labeled "Agent's Thought Process" is the internal monologue where the model executes the instructed steps. This is the literal "chain of thought." Finally, the "Agent's Final Answer" is the polished, comprehensive output generated as a result of that careful, step-by-step reasoning process

<mark>éšåï¼Œè¯¥ç¤ºä¾‹å±•ç¤ºäº† CoT è¿‡ç¨‹çš„å®é™…åº”ç”¨ã€‚æ ‡è®°ä¸º"Agent's Thought Process"çš„éƒ¨åˆ†æ˜¯æ¨¡å‹æ‰§è¡ŒæŒ‡å®šæ­¥éª¤æ—¶çš„å†…å¿ƒç‹¬ç™½ï¼Œè¿™å°±æ˜¯å­—é¢æ„ä¹‰ä¸Šçš„"æ€ç»´é“¾"ã€‚æœ€åï¼Œ"Agent's Final Answer"æ˜¯ç»è¿‡ä»”ç»†ã€é€æ­¥æ¨ç†è¿‡ç¨‹åç”Ÿæˆçš„ç²¾ç‚¼ä¸”å…¨é¢çš„è¾“å‡ºã€‚</mark>

```python
You are an Information Retrieval Agent. Your goal is to answer the user's question comprehensively and accurately by thinking step-by-step.
Here's the process you must follow:

1. **Analyze the Query:** Understand the core subject and specific requirements of the user's question. Identify key entities, keywords, and the type of information being sought.
2. **Formulate Search Queries (for Knowledge Base):** Based on your analysis, generate a list of precise search queries that you would use to retrieve relevant information from a knowledge base or external tools.
3. **Simulate Information Retrieval (Self-Correction/Reasoning):**For each search query, mentally consider what kind of information you expect to find. If you were to retrieve the content, what would be the most relevant snippets? Think about potential ambiguities or
missing pieces.
4. **Synthesize Information:** Based on the simulated retrieval and your understanding of the user's original query, synthesize the gathered information into a coherent and complete answer. Ensure all aspects of the query are addressed.
5. **Review and Refine:** Before finalizing, critically evaluate your answer. Is it accurate? Is it comprehensive? Is it easy to understand? Is it concise? If not, identify what needs to be improved and how.

**User Query:** "Explain the main differences between classical computers and quantum computers, and briefly describe a potential application of quantum computing."

**Agent's Thought Process (Internal CoT Output - encouraged by the prompt):**

**Thought 1 (Analyze Query):**
The user wants two things:
1. Main differences between classical and quantum computers.
2. One potential application of quantum computing.
Keywords: classical computers, quantum computers, differences, potential application.

**Thought 2 (Formulate Search Queries):**
- "differences classical vs quantum computers"
- "how quantum computers work"
- "how classical computers work"
- "applications of quantum computing"

**Thought 3 (Simulate Information Retrieval & Identify Key Concepts):**
- From "differences": Expect concepts like bits vs. qubits, superposition, entanglement, probabilistic nature, computational power for specific problems.
- From "how they work": Classical uses transistors/binary. Quantum uses quantum mechanics.
- From "applications": Expect drug discovery, material science, cryptography, optimization.

**Thought 4 (Synthesize Information):**
Classical computers use bits (0 or 1), processing information sequentially. Quantum computers use qubits, which can be 0, 1, or both simultaneously (superposition), and can be entangled, allowing for vastly different computational approaches. This enables them to
solve certain problems (like factoring large numbers or simulating complex molecules) far more efficiently than classical computers. A potential application is in drug discovery, where quantum computers could simulate molecular interactions with unprecedented accuracy,
accelerating the development of new medicines.

**Thought 5 (Review and Refine):**
- Differences are covered (bits vs qubits, superposition, entanglement, processing).
- Application is given (drug discovery).
- Is it brief? Yes.
- Is it clear? Yes.
- Looks good.

**Agent's Final Answer (Output to User):**
Classical computers process information using bits, which can be either a 0 or a 1 at any given time, performing operations sequentially. In contrast, quantum computers utilize qubits, which can exist in multiple states simultaneously through superposition and can be linked through entanglement, allowing for exponentially more complex computations. This fundamental difference enables quantum machines to tackle certain problems, such as simulating molecular structures or breaking complex encryption, that are intractable for even the most powerful classical supercomputers. A significant potential application of quantum computing lies in drug discovery, where its ability to precisely model molecular behavior could revolutionize the development of new pharmaceuticals.

```

```python

ä½ æ˜¯ä¸€åä¿¡æ¯æ£€ç´¢æ™ºèƒ½ä½“ï¼ˆInformation Retrieval Agentï¼‰ã€‚ä½ çš„ç›®æ ‡æ˜¯é€šè¿‡å¾ªåºæ¸è¿›çš„æ€è€ƒï¼Œå…¨é¢ä¸”å‡†ç¡®åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
ä½ å¿…é¡»éµå¾ªä»¥ä¸‹æµç¨‹ï¼š

1. **åˆ†ææŸ¥è¯¢ï¼ˆAnalyze the Queryï¼‰ï¼š** ç†è§£ç”¨æˆ·é—®é¢˜çš„æ ¸å¿ƒä¸»é¢˜å’Œå…·ä½“è¦æ±‚ã€‚è¯†åˆ«å…³é”®å®ä½“ã€å…³é”®è¯ä»¥åŠæ‰€éœ€ä¿¡æ¯çš„ç±»å‹ã€‚
2. **åˆ¶å®šæœç´¢æŸ¥è¯¢ï¼ˆFormulate Search Queries - é’ˆå¯¹çŸ¥è¯†åº“ï¼‰ï¼š** åŸºäºä½ çš„åˆ†æï¼Œç”Ÿæˆä¸€ä»½ç²¾ç¡®çš„æœç´¢æŸ¥è¯¢åˆ—è¡¨ï¼Œç”¨äºä»çŸ¥è¯†åº“æˆ–å¤–éƒ¨å·¥å…·ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ã€‚
3. **æ¨¡æ‹Ÿä¿¡æ¯æ£€ç´¢ï¼ˆSimulate Information Retrieval - è‡ªæˆ‘ä¿®æ­£/æ¨ç†ï¼‰ï¼š** å¯¹äºæ¯é¡¹æœç´¢æŸ¥è¯¢ï¼Œåœ¨è„‘æµ·ä¸­æ€è€ƒé¢„æœŸä¼šæ‰¾åˆ°å“ªç§ä¿¡æ¯ã€‚å¦‚æœæ£€ç´¢åˆ°å†…å®¹ï¼Œå“ªäº›ç‰‡æ®µä¼šæ˜¯æœ€ç›¸å…³çš„ï¼Ÿæ€è€ƒæ½œåœ¨çš„æ­§ä¹‰æˆ–ç¼ºå¤±çš„éƒ¨åˆ†ã€‚
4. **ç»¼åˆä¿¡æ¯ï¼ˆSynthesize Informationï¼‰ï¼š** åŸºäºæ¨¡æ‹Ÿæ£€ç´¢å’Œä½ å¯¹ç”¨æˆ·åŸå§‹æŸ¥è¯¢çš„ç†è§£ï¼Œå°†æ”¶é›†åˆ°çš„ä¿¡æ¯ç»¼åˆæˆä¸€ä¸ªè¿è´¯ä¸”å®Œæ•´çš„ç­”æ¡ˆã€‚ç¡®ä¿å›ç­”äº†æŸ¥è¯¢çš„æ‰€æœ‰æ–¹é¢ã€‚
5. **å®¡æŸ¥ä¸ä¼˜åŒ–ï¼ˆReview and Refineï¼‰ï¼š** åœ¨æœ€ç»ˆç¡®å®šç­”æ¡ˆå‰ï¼Œæ‰¹åˆ¤æ€§åœ°è¯„ä¼°ä½ çš„å›ç­”ã€‚å®ƒæ˜¯å¦å‡†ç¡®ï¼Ÿæ˜¯å¦å…¨é¢ï¼Ÿæ˜¯å¦æ˜“äºç†è§£ï¼Ÿæ˜¯å¦ç®€æ´ï¼Ÿå¦‚æœä¸æ˜¯ï¼Œç¡®å®šéœ€è¦æ”¹è¿›çš„å†…å®¹å’Œæ–¹å¼ã€‚

**ç”¨æˆ·æŸ¥è¯¢ï¼š**â€œè§£é‡Šç»å…¸è®¡ç®—æœºå’Œé‡å­è®¡ç®—æœºçš„ä¸»è¦åŒºåˆ«ï¼Œå¹¶ç®€è¦æè¿°ä¸€ä¸ªé‡å­è®¡ç®—çš„æ½œåœ¨åº”ç”¨ã€‚â€
**æ™ºèƒ½ä½“çš„æ€ç»´è¿‡ç¨‹ï¼ˆå†…éƒ¨æ€ç»´é“¾è¾“å‡º - ç”±æç¤ºé¼“åŠ±ç”Ÿæˆï¼‰**

**æ€è€ƒ 1ï¼ˆåˆ†ææŸ¥è¯¢ï¼‰ï¼š**
ç”¨æˆ·å¸Œæœ›å¾—åˆ°ä¸¤ç‚¹ä¿¡æ¯ï¼š
1. ç»å…¸è®¡ç®—æœºä¸é‡å­è®¡ç®—æœºçš„ä¸»è¦åŒºåˆ«ã€‚
2. ä¸€ä¸ªæ½œåœ¨çš„é‡å­è®¡ç®—åº”ç”¨ã€‚
å…³é”®è¯ï¼šç»å…¸è®¡ç®—æœºã€é‡å­è®¡ç®—æœºã€åŒºåˆ«ã€æ½œåœ¨åº”ç”¨ã€‚

æ€è€ƒ 2ï¼ˆåˆ¶å®šæœç´¢æŸ¥è¯¢ï¼‰ï¼š
- â€œç»å…¸è®¡ç®—æœº vs é‡å­è®¡ç®—æœº åŒºåˆ«â€
- â€œé‡å­è®¡ç®—æœºå¦‚ä½•å·¥ä½œâ€
- â€œç»å…¸è®¡ç®—æœºå¦‚ä½•å·¥ä½œâ€
- â€œé‡å­è®¡ç®—åº”ç”¨â€

**æ€è€ƒ 3ï¼ˆæ¨¡æ‹Ÿä¿¡æ¯æ£€ç´¢ä¸è¯†åˆ«å…³é”®æ¦‚å¿µï¼‰ï¼š**
- ä»â€œåŒºåˆ«â€ä¸­ï¼šé¢„æœŸä¼šæ‰¾åˆ°æ¯”ç‰¹ï¼ˆbitsï¼‰ vs é‡å­æ¯”ç‰¹ï¼ˆqubitsï¼‰ã€å åŠ æ€ï¼ˆsuperpositionï¼‰ã€çº ç¼ ï¼ˆentanglementï¼‰ã€æ¦‚ç‡æ€§è´¨ã€è§£å†³ç‰¹å®šé—®é¢˜çš„è®¡ç®—èƒ½åŠ›ç­‰æ¦‚å¿µã€‚
- ä»â€œå¦‚ä½•å·¥ä½œâ€ä¸­ï¼šç»å…¸è®¡ç®—æœºä½¿ç”¨æ™¶ä½“ç®¡/äºŒè¿›åˆ¶ã€‚é‡å­è®¡ç®—æœºä½¿ç”¨é‡å­åŠ›å­¦ã€‚
- ä»â€œåº”ç”¨â€ä¸­ï¼šé¢„æœŸä¼šæ‰¾åˆ°è¯ç‰©å‘ç°ã€ææ–™ç§‘å­¦ã€å¯†ç å­¦ã€ä¼˜åŒ–ç­‰ã€‚

**æ€è€ƒ 4ï¼ˆç»¼åˆä¿¡æ¯ï¼‰ï¼š**
ç»å…¸è®¡ç®—æœºä½¿ç”¨æ¯”ç‰¹ï¼ˆ0 æˆ– 1ï¼‰ï¼Œå¹¶æŒ‰é¡ºåºå¤„ç†ä¿¡æ¯ã€‚é‡å­è®¡ç®—æœºä½¿ç”¨é‡å­æ¯”ç‰¹ï¼Œé‡å­æ¯”ç‰¹å¯ä»¥åŒæ—¶å¤„äº 0ã€1 æˆ–ä¸¤è€…å…¼å…·çš„çŠ¶æ€ï¼ˆå åŠ æ€ï¼‰ï¼Œå¹¶ä¸”å¯ä»¥ç›¸äº’çº ç¼ ï¼Œä»è€Œå®ç°æˆªç„¶ä¸åŒçš„è®¡ç®—æ–¹æ³•ã€‚è¿™ä½¿å®ƒä»¬èƒ½å¤Ÿæ¯”ç»å…¸è®¡ç®—æœºæ›´é«˜æ•ˆåœ°è§£å†³æŸäº›ç‰¹å®šé—®é¢˜ï¼ˆå¦‚åˆ†è§£å¤§æ•°æˆ–æ¨¡æ‹Ÿå¤æ‚åˆ†å­ï¼‰ã€‚ä¸€ä¸ªæ½œåœ¨çš„åº”ç”¨æ˜¯è¯ç‰©å‘ç°ï¼Œé‡å­è®¡ç®—æœºå¯ä»¥å‰æ‰€æœªæœ‰åœ°å‡†ç¡®æ¨¡æ‹Ÿåˆ†å­ç›¸äº’ä½œç”¨ï¼Œä»è€ŒåŠ é€Ÿæ–°è¯çš„ç ”å‘ã€‚

**æ€è€ƒ 5ï¼ˆå®¡æŸ¥ä¸ä¼˜åŒ–ï¼‰ï¼š**
- åŒºåˆ«å·²æ¶µç›–ï¼ˆæ¯”ç‰¹ vs é‡å­æ¯”ç‰¹ã€å åŠ æ€ã€çº ç¼ ã€å¤„ç†æ–¹å¼ï¼‰ã€‚
- åº”ç”¨å·²ç»™å‡ºï¼ˆè¯ç‰©å‘ç°ï¼‰ã€‚
- æ˜¯å¦ç®€æ´ï¼Ÿæ˜¯ã€‚
- æ˜¯å¦æ¸…æ™°ï¼Ÿæ˜¯ã€‚
- çœ‹èµ·æ¥ä¸é”™ã€‚

**æ™ºèƒ½ä½“çš„æœ€ç»ˆç­”æ¡ˆï¼ˆè¾“å‡ºç»™ç”¨æˆ·ï¼‰ï¼š**
ç»å…¸è®¡ç®—æœºä½¿ç”¨æ¯”ç‰¹æ¥å¤„ç†ä¿¡æ¯ï¼Œæ¯”ç‰¹åœ¨ä»»ä¸€æ—¶åˆ»åªèƒ½æ˜¯ 0 æˆ– 1ï¼Œå¹¶æŒ‰é¡ºåºæ‰§è¡Œæ“ä½œã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œé‡å­è®¡ç®—æœºåˆ©ç”¨é‡å­æ¯”ç‰¹ï¼Œé‡å­æ¯”ç‰¹å¯ä»¥é€šè¿‡å åŠ æ€åŒæ—¶å­˜åœ¨äºå¤šç§çŠ¶æ€ï¼Œå¹¶é€šè¿‡çº ç¼ ç›¸äº’å…³è”ï¼Œä»è€Œå®ç°æŒ‡æ•°çº§å¤æ‚çš„è®¡ç®—ã€‚è¿™ç§æ ¹æœ¬æ€§çš„å·®å¼‚ä½¿å¾—é‡å­æœºå™¨èƒ½å¤Ÿè§£å†³æŸäº›é—®é¢˜ï¼ˆä¾‹å¦‚æ¨¡æ‹Ÿåˆ†å­ç»“æ„æˆ–ç ´è§£å¤æ‚åŠ å¯†ï¼‰ï¼Œè¿™äº›é—®é¢˜å¯¹äºæœ€å¼ºå¤§çš„ç»å…¸è¶…çº§è®¡ç®—æœºæ¥è¯´éƒ½æ˜¯æ— æ³•æ”»å…‹çš„ã€‚é‡å­è®¡ç®—çš„ä¸€ä¸ªé‡è¦çš„æ½œåœ¨åº”ç”¨åœ¨äºè¯ç‰©å‘ç°ï¼Œå®ƒç²¾ç¡®å»ºæ¨¡åˆ†å­è¡Œä¸ºçš„èƒ½åŠ›å¯ä»¥å½»åº•æ”¹å˜æ–°è¯çš„ç ”å‘ã€‚

```

**Tree-of-Thought (ToT)** is a reasoning technique that builds upon Chain-of-Thought(CoT). It allows large language models to explore multiple reasoning paths by branching into different intermediate steps, forming a tree structure (see Fig. 2) This approach supports complex problem-solving by enabling backtracking,self-correction, and exploration of alternative solutions. Maintaining a tree of possibilities allows the model to evaluate various reasoning trajectories before finalizing an answer. This iterative process enhances the model's ability to handle challenging tasks that require strategic planning and decision-making.
<mark>**æ€ç»´æ ‘ï¼ˆToTï¼‰** æ˜¯ä¸€ç§å»ºç«‹åœ¨æ€ç»´é“¾ï¼ˆChain-of-Thought, CoTï¼‰åŸºç¡€ä¸Šçš„æ¨ç†æŠ€å·§ã€‚å®ƒå…è®¸å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡åˆ†æ”¯åˆ°ä¸åŒçš„ä¸­é—´æ­¥éª¤ï¼Œæ¢ç´¢å¤šæ¡æ¨ç†è·¯å¾„ï¼Œä»è€Œå½¢æˆä¸€ä¸ªæ ‘çŠ¶ç»“æ„ï¼ˆå‚è§å›¾ 2ï¼‰ã€‚è¿™ç§æ–¹æ³•é€šè¿‡æ”¯æŒå›æº¯ã€è‡ªæˆ‘ä¿®æ­£å’Œæ¢ç´¢æ›¿ä»£è§£å†³æ–¹æ¡ˆï¼Œæ¥æ”¯æŒå¤æ‚çš„è§£é¢˜è¿‡ç¨‹ã€‚ç»´æŠ¤ä¸€æ£µå¯èƒ½æ€§ä¹‹æ ‘ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æœ€ç»ˆç¡®å®šç­”æ¡ˆä¹‹å‰è¯„ä¼°å„ç§æ¨ç†è½¨è¿¹ã€‚è¿™ç§è¿­ä»£è¿‡ç¨‹å¢å¼ºäº†æ¨¡å‹å¤„ç†éœ€è¦æˆ˜ç•¥è§„åˆ’å’Œå†³ç­–åˆ¶å®šçš„æŒ‘æˆ˜æ€§ä»»åŠ¡çš„èƒ½åŠ›ã€‚</mark>
<img width="2048" height="1445" alt="image" src="https://github.com/user-attachments/assets/c0bb1ace-e01e-4231-b730-7f166ec7478d" />
Fig.2: Example of Tree of Thoughts
å›¾ 2ï¼šæ€ç»´æ ‘ç¤ºä¾‹

**Self-correction**, also known as self-refinement, is a crucial aspect of an agent's reasoning process, particularly within Chain-of-Thought prompting. It involves the agent's internal evaluation of its generated content and intermediate thought processes. This critical review enables the agent to identify ambiguities, information gaps, or inaccuracies in its understanding or solutions. This iterative cycle of reviewing and refining allows the agent to adjust its approach, improve response quality, and ensure accuracy and thoroughness before delivering a final output. This internal critique enhances the agent's capacity to produce reliable and high-quality results, as demonstrated in examples within the dedicated Chapter 4.

<mark>**è‡ªæˆ‘ä¿®æ­£ï¼ˆSelf-correctionï¼‰**ï¼Œä¹Ÿç§°ä¸ºè‡ªæˆ‘ç²¾ç‚¼ï¼ˆself-refinementï¼‰ï¼Œæ˜¯æ™ºèƒ½ä½“æ¨ç†è¿‡ç¨‹çš„å…³é”®æ–¹é¢ï¼Œå°¤å…¶æ˜¯åœ¨æ€ç»´é“¾æç¤ºä¸­ã€‚å®ƒæ¶‰åŠæ™ºèƒ½ä½“å¯¹å…¶ç”Ÿæˆçš„å†…å®¹å’Œä¸­é—´æ€ç»´è¿‡ç¨‹è¿›è¡Œå†…éƒ¨è¯„ä¼°ã€‚è¿™ç§æ‰¹åˆ¤æ€§å®¡æŸ¥ä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿè¯†åˆ«å…¶ç†è§£æˆ–è§£å†³æ–¹æ¡ˆä¸­çš„æ­§ä¹‰ã€ä¿¡æ¯ç©ºç™½æˆ–ä¸å‡†ç¡®ä¹‹å¤„ã€‚è¿™ç§å®¡æŸ¥å’Œç²¾ç‚¼çš„è¿­ä»£å¾ªç¯å…è®¸æ™ºèƒ½ä½“è°ƒæ•´å…¶æ–¹æ³•ã€æé«˜å“åº”è´¨é‡ï¼Œå¹¶ç¡®ä¿åœ¨äº¤ä»˜æœ€ç»ˆè¾“å‡ºå‰çš„å‡†ç¡®æ€§å’Œå½»åº•æ€§ã€‚è¿™ç§å†…éƒ¨æ‰¹åˆ¤å¢å¼ºäº†æ™ºèƒ½ä½“ç”Ÿæˆå¯é å’Œé«˜è´¨é‡ç»“æœçš„èƒ½åŠ›ï¼Œæ­£å¦‚ä¸“é—¨çš„ç¬¬ 4 ç« ç¤ºä¾‹æ‰€ç¤ºã€‚</mark>


This example demonstrates a systematic process of self-correction, crucial for refining AI-generated content. It involves an iterative loop of drafting, reviewing against original requirements, and implementing specific improvements. The illustration begins by outlining the AI's function as a "Self-Correction Agent" with a67defined five-step analytical and revision workflow. Following this, a subpar "InitialDraft" of a social media post is presented. The "Self-Correction Agent's Thought Process" forms the core of the demonstration. Here, the Agent critically evaluates the draft according to its instructions, pinpointing weaknesses such as low engagement and a vague call to action. It then suggests concrete enhancements, including the use of more impactful verbs and emojis. The process concludes with the "Final Revised Content," a polished and notably improved version that integrates the self-identified adjustments.

<mark>è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†ä¸€ä¸ªç³»ç»ŸåŒ–çš„è‡ªæˆ‘ä¿®æ­£è¿‡ç¨‹ï¼Œè¿™å¯¹äºç²¾ç‚¼ AI ç”Ÿæˆçš„å†…å®¹è‡³å…³é‡è¦ã€‚å®ƒæ¶‰åŠä¸€ä¸ªèµ·è‰ã€å¯¹ç…§åŸå§‹è¦æ±‚è¿›è¡Œå®¡æŸ¥ï¼Œä»¥åŠå®æ–½å…·ä½“æ”¹è¿›çš„è¿­ä»£å¾ªç¯ã€‚è¯¥ç¤ºä¾‹é¦–å…ˆæ¦‚è¿°äº† AI ä½œä¸ºâ€œè‡ªæˆ‘ä¿®æ­£æ™ºèƒ½ä½“â€ï¼ˆSelf-Correction Agentï¼‰çš„åŠŸèƒ½ï¼Œå¹¶å®šä¹‰äº†ä¸€ä¸ªæ˜ç¡®çš„äº”æ­¥åˆ†æå’Œä¿®è®¢å·¥ä½œæµã€‚éšåï¼Œå‘ˆç°äº†ä¸€ä»½è´¨é‡æ¬ ä½³çš„ç¤¾äº¤åª’ä½“å¸–å­â€œåˆå§‹è‰ç¨¿â€ï¼ˆInitial Draftï¼‰ã€‚â€œè‡ªæˆ‘ä¿®æ­£æ™ºèƒ½ä½“çš„æ€ç»´è¿‡ç¨‹â€ï¼ˆSelf-Correction Agent's Thought Processï¼‰æ„æˆäº†æ¼”ç¤ºçš„æ ¸å¿ƒã€‚åœ¨è¿™ä¸ªç¯èŠ‚ï¼Œæ™ºèƒ½ä½“æ ¹æ®æŒ‡ä»¤æ‰¹åˆ¤æ€§åœ°è¯„ä¼°è‰ç¨¿ï¼ŒæŒ‡å‡ºäº†è¯¸å¦‚å‚ä¸åº¦ä½å’Œè¡ŒåŠ¨å·å¬æ¨¡ç³Šç­‰å¼±ç‚¹ã€‚ç„¶åï¼Œå®ƒæå‡ºäº†å…·ä½“çš„æ”¹è¿›å»ºè®®ï¼ŒåŒ…æ‹¬ä½¿ç”¨æ›´å…·å½±å“åŠ›çš„åŠ¨è¯å’Œè¡¨æƒ…ç¬¦å·ã€‚æ•´ä¸ªè¿‡ç¨‹æœ€ç»ˆä»¥â€œæœ€ç»ˆä¿®è®¢å†…å®¹â€ï¼ˆFinal Revised Contentï¼‰æ”¶å°¾ï¼Œè¿™æ˜¯ä¸€ä¸ªæ•´åˆäº†è‡ªæˆ‘è¯†åˆ«è°ƒæ•´åçš„ã€æ›´ç²¾ç‚¼ä¸”æ˜¾è‘—æ”¹å–„çš„ç‰ˆæœ¬ã€‚</mark>

```python
You are a highly critical and detail-oriented Self-Correction Agent.
Your task is to review a previously generated piece of content against its original requirements and identify areas for improvement.
Your goal is to refine the content to be more accurate, comprehensive, engaging, and aligned with the prompt.

Here's the process you must follow for self-correction:

1. **Understand Original Requirements:** Review the initial prompt/requirements that led to the content's creation. What was the *original intent*? What were the key constraints or goals?
2. **Analyze Current Content:** Read the provided content carefully.
3. **Identify Discrepancies/Weaknesses:** Compare the current content against the original requirements. Look for:
* **Accuracy Issues:** Are there any factual errors or misleading statements?
* **Completeness Gaps:** Does it fully address all aspects of the original prompt? Is anything missing?
* **Clarity & Coherence:** Is the language clear, concise, and easy to understand? Does it flow logically?
* **Tone & Style:** Does it match the desired tone and style (e.g., professional, engaging, concise)?
* **Engagement:** Is it captivating? Does it hold the reader's attention?
* **Redundancy/Verbosity:** Can any parts be condensed or removed without losing meaning?
4. **Propose Specific Improvements:** For each identified weakness, suggest concrete and actionable changes. Do not just state the problem; propose a solution.
5. **Generate Revised Content:** Based on your proposed improvements, rewrite the original content to incorporate all the necessary changes. Ensure the revised content is polished and ready for final use.

**Original Prompt/Requirements:** "Write a short, engaging social media post (max 150 characters) announcing a new eco-friendly product line: 'GreenTech Gadgets'."

**Initial Draft (Content to be corrected):**

"We have new products. They are green and techy. Buy GreenTech Gadgets now!"

**Self-Correction Agent's Thought Process (Internal CoT Output - encouraged by the prompt):**

**Thought 1 (Understand Original Requirements):**
- Short, engaging social media post.
- Max 150 characters.
- Announce "GreenTech Gadgets" - a new eco-friendly product line.

**Thought 2 (Analyze Current Content):**
"We have new products. They are green and techy. Buy GreenTech Gadgets now!" (64 characters)

**Thought 3 (Identify Discrepancies/Weaknesses):**
- **Engaging:** Not very engaging. Sounds generic.
- **Eco-friendly:** Implied by "green," but not explicitly highlighted as a benefit.
- **Call to Action:** "Buy now!" is okay, but could be more compelling.
- **Conciseness:** It's short, but could use stronger verbs/phrasing.
- **Impact:** Lacks excitement for a new product launch.

**Thought 4 (Propose Specific Improvements):**
- Make it more dynamic and exciting.
- Emphasize "eco-friendly" and "innovation."
- Use stronger verbs and more active voice.
- Add a clear, engaging call to action.
- Ensure it stays under 150 characters.

**Thought 5 (Generate Revised Content):**
Original: "We have new products. They are green and techy. Buy GreenTech Gadgets now!"
Revised idea 1: "ğŸŒ± Discover GreenTech Gadgets! Our new eco-friendly line blends innovation with sustainability. Go green, go smart! Shop now! #EcoFriendly #GreenTech" (148 characters)
This revision directly addresses eco-friendliness, uses emojis for engagement, adds a stronger call to action, and includes relevant hashtags while staying within the character limit.

**Self-Correction Agent's Final Revised Content (Output to User):**

ğŸŒ± Discover GreenTech Gadgets! Our new eco-friendly line blends innovation with sustainability. Go green, go smart! Shop now! #EcoFriendly #GreenTech

```

```python

è‡ªæˆ‘ä¿®æ­£æ™ºèƒ½ä½“ï¼ˆSelf-Correction Agentï¼‰å·¥ä½œæµç¨‹ç¤ºä¾‹
ä½ æ˜¯ä¸€åé«˜åº¦æ‰¹åˆ¤æ€§ä¸”æ³¨é‡ç»†èŠ‚çš„è‡ªæˆ‘ä¿®æ­£æ™ºèƒ½ä½“ï¼ˆSelf-Correction Agentï¼‰ã€‚

ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®åŸå§‹è¦æ±‚å®¡æŸ¥ä¸€æ®µå…ˆå‰ç”Ÿæˆçš„å†…å®¹ï¼Œå¹¶è¯†åˆ«å‡ºéœ€è¦æ”¹è¿›çš„åŒºåŸŸã€‚ä½ çš„ç›®æ ‡æ˜¯ç²¾ç‚¼å†…å®¹ï¼Œä½¿å…¶æ›´å‡†ç¡®ã€æ›´å…¨é¢ã€æ›´å…·å¸å¼•åŠ›ï¼Œå¹¶ä¸æç¤ºï¼ˆPromptï¼‰ä¿æŒä¸€è‡´ã€‚

ä½ å¿…é¡»éµå¾ªä»¥ä¸‹è‡ªæˆ‘ä¿®æ­£æµç¨‹ï¼š

1. **ç†è§£åŸå§‹è¦æ±‚ï¼ˆUnderstand Original Requirementsï¼‰ï¼š **å®¡æŸ¥ç”Ÿæˆè¯¥å†…å®¹çš„åˆå§‹æç¤º/è¦æ±‚ã€‚åŸå§‹æ„å›¾æ˜¯ä»€ä¹ˆï¼Ÿå…³é”®çš„çº¦æŸæˆ–ç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ
2. **åˆ†æç°æœ‰å†…å®¹ï¼ˆAnalyze Current Contentï¼‰ï¼š** ä»”ç»†é˜…è¯»æä¾›çš„å†…å®¹ã€‚
3. **è¯†åˆ«å·®å¼‚/å¼±ç‚¹ï¼ˆIdentify Discrepancies/Weaknessesï¼‰ï¼š** å°†ç°æœ‰å†…å®¹ä¸åŸå§‹è¦æ±‚è¿›è¡Œæ¯”è¾ƒã€‚æŸ¥æ‰¾ä»¥ä¸‹é—®é¢˜ï¼š
* **å‡†ç¡®æ€§é—®é¢˜ï¼š** æ˜¯å¦å­˜åœ¨ä»»ä½•äº‹å®é”™è¯¯æˆ–è¯¯å¯¼æ€§é™ˆè¿°ï¼Ÿ
* **å®Œæ•´æ€§ç¼ºå¤±ï¼š** æ˜¯å¦å……åˆ†è§£å†³äº†åŸå§‹æç¤ºçš„æ‰€æœ‰æ–¹é¢ï¼Ÿæ˜¯å¦é—æ¼äº†ä»»ä½•å†…å®¹ï¼Ÿ
* **æ¸…æ™°åº¦ä¸è¿è´¯æ€§ï¼š** è¯­è¨€æ˜¯å¦æ¸…æ™°ã€ç®€æ´ã€æ˜“äºç†è§£ï¼Ÿé€»è¾‘æµç¨‹æ˜¯å¦é¡ºç•…ï¼Ÿ
* **è¯­æ°”ä¸é£æ ¼ï¼š** æ˜¯å¦ä¸æ‰€éœ€çš„è¯­æ°”å’Œé£æ ¼ï¼ˆä¾‹å¦‚ï¼Œä¸“ä¸šã€å¼•äººå…¥èƒœã€ç®€æ´ï¼‰åŒ¹é…ï¼Ÿ
* **å‚ä¸åº¦ï¼š** æ˜¯å¦å¼•äººæ³¨ç›®ï¼Ÿèƒ½å¦å¸å¼•è¯»è€…çš„æ³¨æ„åŠ›ï¼Ÿ
* **å†—ä½™/å•°å—¦ï¼š** æ˜¯å¦å¯ä»¥åœ¨ä¸å¤±åŸæ„çš„æƒ…å†µä¸‹ï¼Œå¯¹æŸäº›éƒ¨åˆ†è¿›è¡Œç²¾ç®€æˆ–åˆ é™¤ï¼Ÿ
4. **æå‡ºå…·ä½“æ”¹è¿›å»ºè®®ï¼ˆPropose Specific Improvementsï¼‰ï¼š** å¯¹äºè¯†åˆ«å‡ºçš„æ¯ä¸ªå¼±ç‚¹ï¼Œæå‡ºå…·ä½“ä¸”å¯æ“ä½œçš„æ›´æ”¹ã€‚ä¸è¦åªé™ˆè¿°é—®é¢˜ï¼Œè¿˜è¦æå‡ºè§£å†³æ–¹æ¡ˆã€‚
5. **ç”Ÿæˆä¿®è®¢å†…å®¹ï¼ˆGenerate Revised Contentï¼‰ï¼š** åŸºäºä½ æå‡ºçš„æ”¹è¿›å»ºè®®ï¼Œé‡å†™åŸå§‹å†…å®¹ä»¥çº³å…¥æ‰€æœ‰å¿…è¦çš„æ›´æ”¹ã€‚ç¡®ä¿ä¿®è®¢åçš„å†…å®¹ç»è¿‡æ¶¦é¥°ï¼Œå¯ä¾›æœ€ç»ˆä½¿ç”¨ã€‚

åŸå§‹æç¤º/è¦æ±‚ï¼š â€œæ’°å†™ä¸€ç¯‡ç®€çŸ­ã€å¼•äººå…¥èƒœçš„ç¤¾äº¤åª’ä½“å¸–å­ï¼ˆæœ€å¤š 150 ä¸ªå­—ç¬¦ï¼‰ï¼Œå®£å¸ƒæ–°çš„ç¯ä¿äº§å“çº¿ï¼šâ€˜GreenTech Gadgetsâ€™ã€‚â€

**ï¼ˆåˆå§‹è‰ç¨¿ - å¾…ä¿®æ­£çš„å†…å®¹ï¼‰ï¼š**

â€œæˆ‘ä»¬æœ‰æ–°äº§å“ã€‚å®ƒä»¬æ˜¯ç»¿è‰²çš„ï¼Œç§‘æŠ€æ„Ÿåè¶³ã€‚ç°åœ¨å°±è´­ä¹° GreenTech Gadgets å§ï¼â€

**è‡ªæˆ‘ä¿®æ­£æ™ºèƒ½ä½“çš„æ€ç»´è¿‡ç¨‹ï¼ˆå†…éƒ¨æ€ç»´é“¾è¾“å‡º - ç”±æç¤ºé¼“åŠ±ç”Ÿæˆï¼‰ï¼š**

**æ€è€ƒ 1ï¼ˆç†è§£åŸå§‹è¦æ±‚ï¼‰ï¼š**
ç®€çŸ­ã€å¼•äººå…¥èƒœçš„ç¤¾äº¤åª’ä½“å¸–å­ã€‚
æœ€å¤š 150 ä¸ªå­—ç¬¦ã€‚
å®£å¸ƒâ€œGreenTech Gadgetsâ€â€”â€”ä¸€æ¡æ–°çš„ç¯ä¿äº§å“çº¿ã€‚

**æ€è€ƒ 2ï¼ˆåˆ†æç°æœ‰å†…å®¹ï¼‰ï¼š**
â€œæˆ‘ä»¬æœ‰æ–°äº§å“ã€‚å®ƒä»¬æ˜¯ç»¿è‰²çš„ï¼Œç§‘æŠ€æ„Ÿåè¶³ã€‚ç°åœ¨å°±è´­ä¹° GreenTech Gadgets å§ï¼â€ï¼ˆ64 ä¸ªå­—ç¬¦ï¼‰

**æ€è€ƒ 3ï¼ˆè¯†åˆ«å·®å¼‚/å¼±ç‚¹ï¼‰ï¼š**
å‚ä¸åº¦ï¼š ä¸å¤Ÿå¸å¼•äººã€‚å¬èµ·æ¥å¾ˆæ™®é€šã€‚
ç¯ä¿æ€§ï¼š ç”±â€œç»¿è‰²â€ï¼ˆgreenï¼‰æš—ç¤ºï¼Œä½†æ²¡æœ‰æ˜ç¡®å¼ºè°ƒå…¶ç›Šå¤„ã€‚
è¡ŒåŠ¨å·å¬ï¼š â€œç°åœ¨å°±è´­ä¹°ï¼â€å°šå¯ï¼Œä½†å¯ä»¥æ›´æœ‰è¯´æœåŠ›ã€‚
ç®€æ´æ€§ï¼š è™½ç„¶çŸ­ï¼Œä½†å¯ä»¥ä½¿ç”¨æ›´å¼ºæœ‰åŠ›çš„åŠ¨è¯/æªè¾ã€‚
å½±å“åŠ›ï¼š ç¼ºä¹æ–°äº§å“å‘å¸ƒåº”æœ‰çš„å…´å¥‹æ„Ÿã€‚

**æ€è€ƒ 4ï¼ˆæå‡ºå…·ä½“æ”¹è¿›å»ºè®®ï¼‰ï¼š**
ä½¿å…¶æ›´å…·æ´»åŠ›å’Œå…´å¥‹æ„Ÿã€‚
å¼ºè°ƒ**â€œç¯ä¿â€å’Œâ€œåˆ›æ–°â€**ã€‚
ä½¿ç”¨æ›´å¼ºæœ‰åŠ›çš„åŠ¨è¯å’Œæ›´ä¸»åŠ¨çš„è¯­æ€ã€‚
æ·»åŠ ä¸€ä¸ªæ¸…æ™°ã€å¼•äººå…¥èƒœçš„è¡ŒåŠ¨å·å¬ã€‚
ç¡®ä¿å­—ç¬¦æ•°ä¿æŒåœ¨ 150 ä»¥å†…ã€‚

**æ€è€ƒ 5ï¼ˆç”Ÿæˆä¿®è®¢å†…å®¹ï¼‰ï¼š**
åŸç¨¿ï¼šâ€œæˆ‘ä»¬æœ‰æ–°äº§å“ã€‚å®ƒä»¬æ˜¯ç»¿è‰²çš„ï¼Œç§‘æŠ€æ„Ÿåè¶³ã€‚ç°åœ¨å°±è´­ä¹° GreenTech Gadgets å§ï¼â€
ä¿®è®¢æ€è·¯ 1ï¼šâ€œğŸŒ± æ¢ç´¢ GreenTech Gadgetsï¼ æˆ‘ä»¬çš„æ–°ç¯ä¿ç³»åˆ—èåˆäº†åˆ›æ–°ä¸å¯æŒç»­æ€§ã€‚é€‰æ‹©ç»¿è‰²ï¼Œé€‰æ‹©æ™ºæ…§ï¼ç«‹å³è´­ä¹°ï¼ #EcoFriendly #GreenTechâ€ï¼ˆ148 ä¸ªå­—ç¬¦ï¼‰
è¿™æ¬¡ä¿®è®¢ç›´æ¥å¼ºè°ƒäº†ç¯ä¿æ€§ï¼Œä½¿ç”¨äº†è¡¨æƒ…ç¬¦å·æ¥å¢åŠ å‚ä¸åº¦ï¼Œæ·»åŠ äº†æ›´å¼ºæœ‰åŠ›çš„è¡ŒåŠ¨å·å¬ï¼Œå¹¶åŒ…å«äº†ç›¸å…³æ ‡ç­¾ï¼ŒåŒæ—¶ä¿æŒåœ¨å­—ç¬¦é™åˆ¶å†…ã€‚

**è‡ªæˆ‘ä¿®æ­£æ™ºèƒ½ä½“çš„æœ€ç»ˆä¿®è®¢å†…å®¹ï¼ˆè¾“å‡ºç»™ç”¨æˆ·ï¼‰ï¼š**
ğŸŒ± æ¢ç´¢ GreenTech Gadgetsï¼ æˆ‘ä»¬çš„æ–°ç¯ä¿ç³»åˆ—èåˆäº†åˆ›æ–°ä¸å¯æŒç»­æ€§ã€‚é€‰æ‹©ç»¿è‰²ï¼Œé€‰æ‹©æ™ºæ…§ï¼ç«‹å³è´­ä¹°ï¼ #EcoFriendly #GreenTech
```

Fundamentally, this technique integrates a quality control measure directly into the Agent's content generation, yielding more refined, precise, and superior results that more effectively meet intricate user demands.

<mark>ä»æ ¹æœ¬ä¸Šè¯´ï¼Œè¿™é¡¹æŠ€å·§å°†è´¨é‡æ§åˆ¶æªæ–½ç›´æ¥æ•´åˆåˆ°æ™ºèƒ½ä½“ï¼ˆAgentï¼‰çš„å†…å®¹ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œä»è€Œäº§ç”Ÿæ›´ç²¾ç‚¼ã€æ›´ç²¾ç¡®ã€æ›´ä¼˜è´¨çš„ç»“æœï¼Œèƒ½æ›´æœ‰æ•ˆåœ°æ»¡è¶³å¤æ‚çš„ç”¨æˆ·éœ€æ±‚ã€‚</mark>

**Program-Aided Language Models (PALMs)** integrate LLMs with symbolic reasoning capabilities. This integration allows the LLM to generate and execute code, such as Python, as part of its problem-solving process. PALMs offload complex calculations, logical operations, and data manipulation to a deterministic programming environment. This approach utilizes the strengths of traditional programming for tasks where LLMs might exhibit limitations in accuracy or consistency. When faced with symbolic challenges, the model can produce code, execute it, and convert the results into natural language. This hybrid methodology combines the LLM's understanding and generation abilities with precise computation, enabling the model to address a wider range of complex problems with potentially increased reliability and accuracy. This is important for agents as it allows them to perform more accurate and reliable actions by leveraging precise computation alongside their understanding and generation capabilities. An example is the use of external tools within Google's ADK for generating code.

<mark>**ç¨‹åºè¾…åŠ©è¯­è¨€æ¨¡å‹ï¼ˆProgram-Aided Language Models, PALMsï¼‰** å°†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ç¬¦å·æ¨ç†èƒ½åŠ›ç›¸ç»“åˆã€‚è¿™ç§é›†æˆå…è®¸ LLM åœ¨é—®é¢˜è§£å†³è¿‡ç¨‹ä¸­ç”Ÿæˆå¹¶æ‰§è¡Œä»£ç ï¼Œä¾‹å¦‚ Pythonã€‚PALMs å°†å¤æ‚çš„è®¡ç®—ã€é€»è¾‘æ“ä½œå’Œæ•°æ®å¤„ç†å·¥ä½œè½¬ç§»åˆ°ä¸€ä¸ªç¡®å®šçš„ç¼–ç¨‹ç¯å¢ƒä¸­ã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨äº†ä¼ ç»Ÿç¼–ç¨‹çš„ä¼˜åŠ¿ï¼Œæ¥å¤„ç† LLM åœ¨å‡†ç¡®æ€§æˆ–ä¸€è‡´æ€§æ–¹é¢å¯èƒ½è¡¨ç°å‡ºå±€é™æ€§çš„ä»»åŠ¡ã€‚å½“é¢ä¸´ç¬¦å·æŒ‘æˆ˜æ—¶ï¼Œæ¨¡å‹å¯ä»¥ç”Ÿæˆä»£ç ã€æ‰§è¡Œä»£ç ï¼Œå¹¶å°†ç»“æœè½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€ã€‚è¿™ç§æ··åˆæ–¹æ³•å°† LLM çš„ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ä¸ç²¾ç¡®è®¡ç®—ç›¸ç»“åˆï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè§£å†³æ›´å¹¿æ³›çš„å¤æ‚é—®é¢˜ï¼Œå¹¶æœ‰å¯èƒ½æé«˜å¯é æ€§å’Œå‡†ç¡®æ€§ã€‚è¿™å¯¹æ™ºèƒ½ä½“æ¥è¯´è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒå…è®¸æ™ºèƒ½ä½“é€šè¿‡åˆ©ç”¨ç²¾ç¡®è®¡ç®—ä»¥åŠè‡ªèº«çš„ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œæ‰§è¡Œæ›´å‡†ç¡®ã€æ›´å¯é çš„è¡ŒåŠ¨ã€‚ä¸€ä¸ªä¾‹å­æ˜¯ Google ADK ä¸­ä½¿ç”¨å¤–éƒ¨å·¥å…·æ¥ç”Ÿæˆä»£ç ã€‚</mark>

```python
from google.adk.tools import agent_tool
from google.adk.agents import Agent
from google.adk.tools import google_search
from google.adk.code_executors import BuiltInCodeExecutor
search_agent = Agent(
   model='gemini-2.0-flash',
   name='SearchAgent',
   instruction="""
   You're a specialist in Google Search
   """,
   tools=[google_search],
)
coding_agent = Agent(
   model='gemini-2.0-flash',
   name='CodeAgent',
   instruction="""
   You're a specialist in Code Execution
9
10
""",
   code_executor=[BuiltInCodeExecutor],
)
root_agent = Agent(
   name="RootAgent",
   model="gemini-2.0-flash",
   description="Root Agent",
   tools=[agent_tool.AgentTool(agent=search_agent),
agent_tool.AgentTool(agent=coding_agent)],
)
```

**Reinforcement Learning with Verifiable Rewards (RLVR):** While effective, the standard Chain-of-Thought (CoT) prompting used by many LLMs is a somewhat basic approach to reasoning. It generates a single, predetermined line of thought without adapting to the complexity of the problem. To overcome these limitations, a new class of specialized "reasoning models" has been developed. These models operate differently by dedicating a variable amount of "thinking" time before providing an answer. This "thinking" process produces a more extensive and dynamic Chain-of-Thought that can be thousands of tokens long. This extended reasoning allows for more complex behaviors like self-correction and backtracking, with the model dedicating more effort to harder problems. The key innovation enabling these models is a training strategy called Reinforcement Learning from Verifiable Rewards (RLVR). By training the model on problems with known correct answers (like math or code), it learns through trial and error to generate effective, long-form reasoning. This allows the model to evolve its problem-solving abilities without direct human supervision. Ultimately, these reasoning models don't just produce an answer; they generate a "reasoning trajectory" that demonstrates advanced skills like planning, monitoring, and evaluation. This enhanced ability to reason and strategize is fundamental to the development of autonomous AI agents, which can break down and solve complex tasks with minimal human intervention.

<mark>**å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learning with Verifiable Rewards, RLVRï¼‰ï¼š** å°½ç®¡æœ‰æ•ˆï¼Œä½†è®¸å¤š LLM ä½¿ç”¨çš„æ ‡å‡†æ€ç»´é“¾ï¼ˆChain-of-Thought, CoTï¼‰æç¤ºæ˜¯ä¸€ç§ç›¸å¯¹åŸºç¡€çš„æ¨ç†æ–¹æ³•ã€‚å®ƒä¼šç”Ÿæˆä¸€æ¡å•ä¸€ã€é¢„å®šçš„æ€ç»´è·¯çº¿ï¼Œè€Œæ— æ³•é€‚åº”é—®é¢˜çš„å¤æ‚æ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œä¸€ç±»æ–°å‹çš„ä¸“ä¸š**ã€Œæ¨ç†æ¨¡å‹ã€å·²è¢«å¼€å‘å‡ºæ¥ã€‚è¿™äº›æ¨¡å‹çš„è¿ä½œæ–¹å¼æœ‰æ‰€ä¸åŒï¼Œå®ƒä»¬ä¼šåœ¨æä¾›ç­”æ¡ˆä¹‹å‰æŠ•å…¥å¯å˜æ—¶é•¿çš„ã€Œæ€è€ƒã€æ—¶é—´ã€‚è¿™ä¸ªã€Œæ€è€ƒã€è¿‡ç¨‹ä¼šäº§ç”Ÿæ›´å¹¿æ³›ã€æ›´å…·åŠ¨æ€æ€§çš„æ€ç»´é“¾**ï¼Œé•¿åº¦å¯è¾¾æ•°åƒä¸ª Tokenã€‚è¿™ç§æ‰©å±•çš„æ¨ç†èƒ½å¤Ÿæ”¯æŒæ›´å¤æ‚çš„è¡Œä¸ºï¼Œä¾‹å¦‚è‡ªæˆ‘ä¿®æ­£å’Œå›æº¯ï¼Œæ¨¡å‹ä¼šé’ˆå¯¹éš¾åº¦æ›´é«˜çš„é—®é¢˜æŠ•å…¥æ›´å¤šç²¾åŠ›ã€‚èµ‹èƒ½è¿™äº›æ¨¡å‹çš„å…³é”®åˆ›æ–°æ˜¯ä¸€ç§åä¸ºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰çš„è®­ç»ƒç­–ç•¥ã€‚é€šè¿‡åœ¨å·²çŸ¥æ­£ç¡®ç­”æ¡ˆçš„é—®é¢˜ä¸Šï¼ˆä¾‹å¦‚æ•°å­¦æˆ–ä»£ç ï¼‰å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œæ¨¡å‹é€šè¿‡è¯•é”™å­¦ä¹ ç”Ÿæˆæœ‰æ•ˆçš„é•¿ç¯‡æ¨ç†ã€‚è¿™ä½¿å¾—æ¨¡å‹æ— éœ€ç›´æ¥çš„äººç±»ç›‘ç£å³å¯æ¼”åŒ–å…¶é—®é¢˜è§£å†³èƒ½åŠ›ã€‚æœ€ç»ˆï¼Œè¿™äº›æ¨ç†æ¨¡å‹ä¸ä»…ä¼šäº§ç”Ÿç­”æ¡ˆï¼Œè¿˜ä¼šç”Ÿæˆä¸€æ¡ã€Œæ¨ç†è½¨è¿¹ã€ï¼Œå±•ç¤ºå‡ºè§„åˆ’ã€ç›‘æ§å’Œè¯„ä¼°ç­‰é«˜çº§æŠ€èƒ½ã€‚è¿™ç§å¢å¼ºçš„æ¨ç†å’Œç­–ç•¥åˆ¶å®šèƒ½åŠ›ï¼Œæ˜¯è‡ªä¸» AI æ™ºèƒ½ä½“å‘å±•çš„åŸºçŸ³ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿä»¥æœ€å°‘çš„äººå·¥å¹²é¢„æ¥æ‹†è§£å’Œè§£å†³å¤æ‚çš„ä»»åŠ¡ã€‚</mark>

**ReAct** (Reasoning and Acting, see Fig. 3, where KB stands for Knowledge Base) is a paradigm that integrates Chain-of-Thought (CoT) prompting with an agent's ability to interact with external environments through tools. Unlike generative models that produce a final answer, a ReAct agent reasons about which actions to take. This reasoning phase involves an internal planning process, similar to CoT, where the agent determines its next steps, considers available tools, and anticipates outcomes. Following this, the agent acts by executing a tool or function call, such as querying a database, performing a calculation, or interacting with an API.

<mark>**ReAct**ï¼ˆæ¨ç†ä¸è¡ŒåŠ¨ï¼Œå‚è§å›¾ 3ï¼Œå…¶ä¸­ KB ä»£è¡¨çŸ¥è¯†åº“ï¼‰æ˜¯ä¸€ç§å°†æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºä¸æ™ºèƒ½ä½“é€šè¿‡å·¥å…·ä¸å¤–éƒ¨ç¯å¢ƒè¿›è¡Œäº¤äº’èƒ½åŠ›ç›¸ç»“åˆçš„èŒƒå¼ã€‚ä¸ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆçš„ç”Ÿæˆæ¨¡å‹ä¸åŒï¼ŒReAct æ™ºèƒ½ä½“ä¼šæ¨ç†è¦é‡‡å–å“ªäº›è¡ŒåŠ¨ã€‚è¿™ä¸ªæ¨ç†é˜¶æ®µæ¶‰åŠä¸€ä¸ªç±»ä¼¼äº CoT çš„å†…éƒ¨è§„åˆ’è¿‡ç¨‹ï¼Œæ™ºèƒ½ä½“åœ¨å…¶ä¸­ç¡®å®šå…¶åç»­æ­¥éª¤ã€è€ƒè™‘å¯ç”¨çš„å·¥å…·å¹¶é¢„æµ‹ç»“æœã€‚éšåï¼Œæ™ºèƒ½ä½“é€šè¿‡æ‰§è¡Œå·¥å…·æˆ–å‡½æ•°è°ƒç”¨ï¼ˆä¾‹å¦‚æŸ¥è¯¢æ•°æ®åº“ã€æ‰§è¡Œè®¡ç®—æˆ–ä¸ API äº¤äº’ï¼‰æ¥é‡‡å–è¡ŒåŠ¨ã€‚</mark>

<img width="2048" height="1445" alt="image" src="https://github.com/user-attachments/assets/6ddc2354-f418-4271-a920-792a5bd0ff05" />

Fig.3: Reasoning and Act
<mark>å›¾ 3ï¼šæ¨ç†ä¸è¡ŒåŠ¨</mark>

ReAct operates in an interleaved manner: the agent executes an action, observes the outcome, and incorporates this observation into subsequent reasoning. This iterative loop of â€œThought, Action, Observation, Thought...â€ allows the agent to dynamically adapt its plan, correct errors, and achieve goals requiring multiple interactions with the environment. This provides a more robust and flexible problem-solving approach compared to linear CoT, as the agent responds to real-time feedback. By combining language model understanding and generation with the capability to use tools, ReAct enables agents to perform complex tasks requiring both reasoning and practical execution. This approach is crucial for agents as it allows them to not only reason but also to practically execute steps and interact with dynamic environments.

<mark>ReAct ä»¥äº¤é”™çš„æ–¹å¼è¿ä½œï¼šæ™ºèƒ½ä½“æ‰§è¡Œä¸€ä¸ªåŠ¨ä½œï¼Œè§‚å¯Ÿç»“æœï¼Œå¹¶å°†æ­¤è§‚å¯Ÿç»“æœçº³å…¥éšåçš„æ¨ç†ä¸­ã€‚è¿™ç§ã€Œæ€è€ƒã€è¡ŒåŠ¨ã€è§‚å¯Ÿã€æ€è€ƒâ€¦â€¦ã€çš„è¿­ä»£å¾ªç¯å…è®¸æ™ºèƒ½ä½“åŠ¨æ€åœ°è°ƒæ•´å…¶è®¡åˆ’ã€ä¿®æ­£é”™è¯¯ï¼Œå¹¶å®ç°éœ€è¦ä¸ç¯å¢ƒè¿›è¡Œå¤šæ¬¡äº¤äº’çš„ç›®æ ‡ã€‚ç”±äºæ™ºèƒ½ä½“å¯¹å®æ—¶åé¦ˆä½œå‡ºå“åº”ï¼Œå› æ­¤ä¸çº¿æ€§ CoT ç›¸æ¯”ï¼Œè¿™æä¾›äº†ä¸€ç§æ›´ç¨³å¥å’Œçµæ´»çš„é—®é¢˜è§£å†³æ–¹æ³•ã€‚é€šè¿‡å°†è¯­è¨€æ¨¡å‹çš„ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ä¸ä½¿ç”¨å·¥å…·çš„èƒ½åŠ›ç›¸ç»“åˆï¼ŒReAct ä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿæ‰§è¡Œæ—¢éœ€è¦æ¨ç†åˆéœ€è¦å®é™…æ‰§è¡Œçš„å¤æ‚ä»»åŠ¡ã€‚è¿™ç§æ–¹æ³•å¯¹æ™ºèƒ½ä½“è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒä½¿æ™ºèƒ½ä½“ä¸ä»…èƒ½å¤Ÿæ¨ç†ï¼Œè¿˜èƒ½å®é™…æ‰§è¡Œæ­¥éª¤å¹¶ä¸åŠ¨æ€ç¯å¢ƒè¿›è¡Œäº¤äº’ã€‚</mark>

**CoD** (Chain of Debates) is a formal AI framework proposed by Microsoft where multiple, diverse models collaborate and argue to solve a problem, moving beyond a single AI's "chain of thought." This system operates like an AI council meeting, where different models present initial ideas, critique each other's reasoning, and exchange counterarguments. The primary goal is to enhance accuracy, reduce bias, and improve the overall quality of the final answer by leveraging collective intelligence. Functioning as an AI version of peer review, this method creates a transparent and trustworthy record of the reasoning process. Ultimately, it represents a shift from a solitary Agent providing an answer to a collaborative team of Agents working together to find a more robust and validated solution.

<mark>**CoD**ï¼ˆè¾©è®ºé“¾ï¼ŒChain of Debatesï¼‰æ˜¯å¾®è½¯æå‡ºçš„ä¸€ç§æ­£å¼ AI æ¡†æ¶ï¼Œå…¶ä¸­å¤šä¸ªã€ä¸åŒçš„æ¨¡å‹ååŒåˆä½œå¹¶è¿›è¡Œè¾©è®ºæ¥è§£å†³é—®é¢˜ï¼Œè¶…è¶Šäº†å•ä¸ª AI çš„ã€Œæ€ç»´é“¾ã€ã€‚è¯¥ç³»ç»Ÿè¿ä½œèµ·æ¥å°±åƒä¸€ä¸ª AI å§”å‘˜ä¼šä¼šè®®ï¼Œä¸åŒçš„æ¨¡å‹æå‡ºåˆå§‹æƒ³æ³•ã€æ‰¹åˆ¤å½¼æ­¤çš„æ¨ç†ï¼Œå¹¶äº¤æ¢åé©³æ„è§ã€‚å…¶ä¸»è¦ç›®æ ‡æ˜¯é€šè¿‡åˆ©ç”¨é›†ä½“æ™ºæ…§ï¼Œæé«˜æœ€ç»ˆç­”æ¡ˆçš„å‡†ç¡®æ€§ã€å‡å°‘åè§å¹¶æ”¹å–„æ•´ä½“è´¨é‡ã€‚è¯¥æ–¹æ³•å……å½“ AI ç‰ˆçš„åŒè¡Œè¯„å®¡ï¼Œåˆ›å»ºäº†ä¸€ä¸ªé€æ˜ä¸”å€¼å¾—ä¿¡èµ–çš„æ¨ç†è¿‡ç¨‹è®°å½•ã€‚æœ€ç»ˆï¼Œå®ƒä»£è¡¨äº†ä¸€ç§è½¬å˜ï¼Œå³ä»ä¸€ä¸ªå•ç‹¬çš„æ™ºèƒ½ä½“æä¾›ç­”æ¡ˆï¼Œè½¬å‘ä¸€ä¸ªæ™ºèƒ½ä½“åä½œå›¢é˜Ÿå…±åŒå¯»æ‰¾ä¸€ä¸ªæ›´ç¨³å¥ã€ç»è¿‡éªŒè¯çš„è§£å†³æ–¹æ¡ˆã€‚</mark>

GoD (Graph of Debates) is an advanced Agentic framework that reimagines discussion as a dynamic, non-linear network rather than a simple chain. In this model, arguments are individual nodes connected by edges that signify relationships like 'supports' or 'refutes,' reflecting the multi-threaded nature of real debate. This structure allows new lines of inquiry to dynamically branch off, evolve independently, and even merge over time. A conclusion is reached not at the end of a sequence, but by identifying the most robust and well-supported cluster of arguments within the entire graph. In this context, "well-supported" refers to knowledge that is firmly established and verifiable. This can include information considered to be ground truth, which means it is inherently correct and widely accepted as fact. Additionally, it encompasses factual evidence obtained through search grounding, where information is validated against external sources and real-world data. Finally, it also pertains to a consensus reached by multiple models during a debate, indicating a high degree of agreement and confidence in the information presented. This comprehensive approach ensures a more robust and reliable foundation for the information being discussed. This approach provides a more holistic and realistic model for complex, collaborative AI reasoning.

<mark>GoDï¼ˆè¾©è®ºå›¾ï¼ŒGraph of Debatesï¼‰æ˜¯ä¸€ç§å…ˆè¿›çš„å…·æ™ºèƒ½ä½“ç‰¹æ€§ï¼ˆAgenticï¼‰æ¡†æ¶ï¼Œå®ƒå°†è®¨è®ºé‡æ–°æ„æƒ³ä¸ºä¸€ä¸ªåŠ¨æ€ã€éçº¿æ€§çš„ç½‘ç»œï¼Œè€Œä¸æ˜¯ä¸€ä¸ªç®€å•çš„é“¾æ¡ã€‚åœ¨è¿™ä¸ªæ¨¡å‹ä¸­ï¼Œè®ºç‚¹æ˜¯å•ç‹¬çš„èŠ‚ç‚¹ï¼Œé€šè¿‡è¡¨ç¤ºã€Œæ”¯æŒã€æˆ–ã€Œåé©³ã€ç­‰å…³ç³»çš„è¾¹è¿æ¥èµ·æ¥ï¼Œåæ˜ äº†çœŸå®è¾©è®ºçš„å¤šçº¿ç¨‹ç‰¹æ€§ã€‚è¿™ç§ç»“æ„å…è®¸æ–°çš„æ¢ç©¶è·¯çº¿åŠ¨æ€åœ°åˆ†æ”¯å‡ºæ¥ã€ç‹¬ç«‹æ¼”åŒ–ï¼Œç”šè‡³éšæ—¶é—´æ¨ç§»è€Œåˆå¹¶ã€‚ç»“è®ºçš„å¾—å‡ºå¹¶éåœ¨åºåˆ—çš„æœ«å°¾ï¼Œè€Œæ˜¯é€šè¿‡è¯†åˆ«æ•´ä¸ªå›¾ä¸­æœ€ç¨³å¥å’Œå¾—åˆ°å……åˆ†æ”¯æŒçš„è®ºç‚¹é›†ç¾¤ã€‚åœ¨è¿™ç§èƒŒæ™¯ä¸‹ï¼Œã€Œå¾—åˆ°å……åˆ†æ”¯æŒã€æŒ‡çš„æ˜¯åšå®šç¡®ç«‹ä¸”å¯éªŒè¯çš„çŸ¥è¯†ã€‚è¿™å¯ä»¥åŒ…æ‹¬è¢«è®¤ä¸ºæ˜¯åŸºç¡€äº‹å®ï¼ˆground truthï¼‰çš„ä¿¡æ¯ï¼Œå³å…¶æœ¬è´¨ä¸Šæ­£ç¡®å¹¶è¢«å¹¿æ³›æ¥å—ä¸ºäº‹å®ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜åŒ…æ‹¬é€šè¿‡æœç´¢æº¯æºï¼ˆsearch groundingï¼‰è·å¾—çš„äº‹å®è¯æ®ï¼Œå³ä¿¡æ¯å·²æ ¹æ®å¤–éƒ¨æ¥æºå’ŒçœŸå®ä¸–ç•Œæ•°æ®è¿›è¡Œäº†éªŒè¯ã€‚æœ€åï¼Œå®ƒä¹Ÿæ¶‰åŠå¤šä¸ªæ¨¡å‹åœ¨è¾©è®ºä¸­è¾¾æˆçš„å…±è¯†ï¼Œè¡¨æ˜å¯¹æ‰€å‘ˆç°ä¿¡æ¯çš„é«˜åº¦ä¸€è‡´æ€§å’Œä¿¡å¿ƒã€‚è¿™ç§ç»¼åˆæ–¹æ³•ç¡®ä¿äº†æ‰€è®¨è®ºä¿¡æ¯å…·æœ‰æ›´ç¨³å¥å’Œå¯é çš„åŸºç¡€ã€‚è¿™ç§æ–¹æ³•ä¸ºå¤æ‚ã€åä½œçš„ AI æ¨ç†æä¾›äº†ä¸€ä¸ªæ›´æ•´ä½“ã€æ›´çœŸå®çš„æ¨¡å‹ã€‚</mark>

**MASS (optional advanced topic):** An in-depth analysis of the design of multi-agent systems reveals that their effectiveness is critically dependent on both the quality of the prompts used to program individual agents and the topology that dictates their interactions. The complexity of designing these systems is significant, as it involves a vast and intricate search space. To address this challenge, a novel framework called Multi-Agent System Search (MASS) was developed to automate and optimize the design of MAS.

<mark>**MASSï¼ˆå¯é€‰è¿›é˜¶ä¸»é¢˜ï¼‰ï¼š** å¯¹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMulti-Agent Systems, MASï¼‰è®¾è®¡çš„æ·±å…¥åˆ†æè¡¨æ˜ï¼Œå…¶æœ‰æ•ˆæ€§å…³é”®å–å†³äºç”¨äºç¼–ç¨‹å•ä¸ªæ™ºèƒ½ä½“çš„æç¤ºï¼ˆPromptï¼‰è´¨é‡ä»¥åŠå†³å®šå…¶äº¤äº’çš„æ‹“æ‰‘ç»“æ„ã€‚è®¾è®¡è¿™äº›ç³»ç»Ÿçš„å¤æ‚æ€§éå¸¸é«˜ï¼Œå› ä¸ºå®ƒæ¶‰åŠä¸€ä¸ªåºå¤§è€Œé”™ç»¼å¤æ‚çš„æœç´¢ç©ºé—´ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œå¼€å‘äº†ä¸€ä¸ªåä¸º**å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæœç´¢ï¼ˆMASSï¼‰**çš„æ–°é¢–æ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨åŒ–å’Œä¼˜åŒ– MAS çš„è®¾è®¡ã€‚</mark>

MASS employs a multi-stage optimization strategy that systematically navigates the complex design space by interleaving prompt and topology optimization (see Fig. 4)

<mark>MASS é‡‡ç”¨ä¸€ç§å¤šé˜¶æ®µä¼˜åŒ–ç­–ç•¥ï¼Œé€šè¿‡äº¤é”™è¿›è¡Œæç¤ºä¼˜åŒ–å’Œæ‹“æ‰‘ä¼˜åŒ–ï¼Œç³»ç»Ÿåœ°å¯¼èˆªå¤æ‚çš„è®¾è®¡ç©ºé—´ï¼ˆå‚è§å›¾ 4ï¼‰ã€‚</mark>

**1. Block-Level Prompt Optimization:** The process begins with a local optimization of prompts for individual agent types, or "blocks," to ensure each component performs its role effectively before being integrated into a larger system. This initial step is crucial as it ensures that the subsequent topology optimization builds upon well-performing agents, rather than suffering from the compounding impact of poorly configured ones. For example, when optimizing for the HotpotQA dataset, the prompt for a "Debator" agent is creatively framed to instruct it to act as an "expert fact-checker for a major publication". Its optimized task is to meticulously review proposed answers from other agents, cross-reference them with provided context passages, and identify any inconsistencies or unsupported claims. This specialized role-playing prompt, discovered during block-level optimization, aims to make the debator agent highly effective at synthesizing information before it's even placed into a larger workflow.

<mark>**1. å—çº§æç¤ºä¼˜åŒ–ï¼ˆBlock-Level Prompt Optimizationï¼‰ï¼š** è¯¥è¿‡ç¨‹ä»å¯¹å•ä¸ªæ™ºèƒ½ä½“ç±»å‹æˆ–ã€Œå—ã€çš„æç¤ºè¿›è¡Œå±€éƒ¨ä¼˜åŒ–å¼€å§‹ï¼Œä»¥ç¡®ä¿æ¯ä¸ªç»„ä»¶åœ¨é›†æˆåˆ°æ›´å¤§ç³»ç»Ÿä¹‹å‰éƒ½èƒ½æœ‰æ•ˆåœ°æ‰§è¡Œå…¶è§’è‰²ã€‚è¿™ä¸€åˆå§‹æ­¥éª¤è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒèƒ½ç¡®ä¿åç»­çš„æ‹“æ‰‘ä¼˜åŒ–æ˜¯å»ºç«‹åœ¨è¡¨ç°è‰¯å¥½çš„æ™ºèƒ½ä½“ä¹‹ä¸Šçš„ï¼Œè€Œä¸æ˜¯å› é…ç½®ä¸ä½³çš„æ™ºèƒ½ä½“è€Œé­å—å¤åˆå½±å“ã€‚ä¾‹å¦‚ï¼Œåœ¨é’ˆå¯¹ HotpotQA æ•°æ®é›†è¿›è¡Œä¼˜åŒ–æ—¶ï¼Œã€Œè¾©è®ºè€…ã€æ™ºèƒ½ä½“çš„æç¤ºè¢«åˆ›é€ æ€§åœ°æ„æ€ï¼ŒæŒ‡ç¤ºå…¶æ‰®æ¼”**ã€ŒæŸä¸»è¦å‡ºç‰ˆç‰©çš„ä¸“å®¶äº‹å®æ ¸æŸ¥å‘˜ã€ã€‚å…¶ä¼˜åŒ–åçš„ä»»åŠ¡æ˜¯ä»”ç»†å®¡æŸ¥å…¶ä»–æ™ºèƒ½ä½“æå‡ºçš„ç­”æ¡ˆï¼Œå°†å…¶ä¸æä¾›çš„ä¸Šä¸‹æ–‡æ®µè½è¿›è¡Œäº¤å‰å¼•ç”¨ï¼Œå¹¶è¯†åˆ«ä»»ä½•ä¸ä¸€è‡´æˆ–æœªå¾—åˆ°æ”¯æŒçš„è®ºæ–­ã€‚è¿™ä¸ªåœ¨å—çº§ä¼˜åŒ–è¿‡ç¨‹ä¸­å‘ç°çš„ä¸“ä¸šè§’è‰²æ‰®æ¼”æç¤º**ï¼Œæ—¨åœ¨ä½¿è¾©è®ºè€…æ™ºèƒ½ä½“åœ¨è¢«æ”¾å…¥æ›´å¤§å·¥ä½œæµä¹‹å‰ï¼Œå°±èƒ½é«˜æ•ˆåœ°ç»¼åˆä¿¡æ¯ã€‚</mark>

**2. Workflow Topology Optimization:** Following local optimization, MASS optimizes the workflow topology by selecting and arranging different agent interactions from a customizable design space. To make this search efficient, MASS employs an influence-weighted method. This method calculates the "incremental influence" of each topology by measuring its performance gain relative to a baseline agent and uses these scores to guide the search toward more promising combinations. For instance, when optimizing for the MBPP coding task, the topology search discovers that a specific hybrid workflow is most effective. The best-found topology is not a simple structure but a combination of an iterative refinement process with external tool use. Specifically, it consists of one predictor agent that engages in several rounds of reflection, with its code being verified by one executor agent that runs the code against test cases. This discovered workflow shows that for coding, a structure that combines iterative self-correction with external verification is superior to simpler MAS designs.

<mark>**2. å·¥ä½œæµæ‹“æ‰‘ä¼˜åŒ–ï¼ˆWorkflow Topology Optimizationï¼‰ï¼š** åœ¨å±€éƒ¨ä¼˜åŒ–ä¹‹åï¼ŒMASS é€šè¿‡ä»å¯å®šåˆ¶çš„è®¾è®¡ç©ºé—´ä¸­é€‰æ‹©å’Œæ’åˆ—ä¸åŒçš„æ™ºèƒ½ä½“äº¤äº’ï¼Œæ¥ä¼˜åŒ–å·¥ä½œæµæ‹“æ‰‘ã€‚ä¸ºäº†æé«˜æœç´¢æ•ˆç‡ï¼ŒMASS é‡‡ç”¨äº†ä¸€ç§å½±å“åŠ æƒæ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡æµ‹é‡æ¯ç§æ‹“æ‰‘ç»“æ„ç›¸å¯¹äºåŸºçº¿æ™ºèƒ½ä½“çš„æ€§èƒ½å¢ç›Šï¼Œè®¡ç®—å…¶ã€Œå¢é‡å½±å“ã€ï¼Œå¹¶ä½¿ç”¨è¿™äº›åˆ†æ•°æ¥æŒ‡å¯¼æœç´¢ï¼Œä½¿å…¶å€¾å‘äºæ›´æœ‰å‰é€”çš„ç»„åˆã€‚ä¾‹å¦‚ï¼Œåœ¨é’ˆå¯¹ MBPP ç¼–ç ä»»åŠ¡è¿›è¡Œä¼˜åŒ–æ—¶ï¼Œæ‹“æ‰‘æœç´¢å‘ç°ç‰¹å®šçš„æ··åˆå·¥ä½œæµæœ€ä¸ºæœ‰æ•ˆã€‚å‘ç°çš„æœ€ä½³æ‹“æ‰‘ç»“æ„å¹¶éä¸€ä¸ªç®€å•çš„ç»“æ„ï¼Œè€Œæ˜¯è¿­ä»£ç²¾ç‚¼è¿‡ç¨‹ä¸å¤–éƒ¨å·¥å…·ä½¿ç”¨çš„ç»„åˆã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒåŒ…å«ä¸€ä¸ªè¿›è¡Œå¤šè½®åæ€çš„é¢„æµ‹æ™ºèƒ½ä½“ï¼Œå…¶ä»£ç ç”±ä¸€ä¸ªé’ˆå¯¹æµ‹è¯•ç”¨ä¾‹è¿è¡Œä»£ç çš„æ‰§è¡Œæ™ºèƒ½ä½“è¿›è¡ŒéªŒè¯ã€‚è¿™ä¸ªè¢«å‘ç°çš„å·¥ä½œæµè¡¨æ˜ï¼Œå¯¹äºç¼–ç ä»»åŠ¡ï¼Œå°†è¿­ä»£è‡ªæˆ‘ä¿®æ­£ä¸å¤–éƒ¨éªŒè¯ç›¸ç»“åˆçš„ç»“æ„ä¼˜äºæ›´ç®€å•çš„ MAS è®¾è®¡ã€‚</mark>

<img width="1356" height="542" alt="image" src="https://github.com/user-attachments/assets/f98f633b-dced-479d-bc52-d57ad6e5a992" />
Fig. 4: (Courtesy of the Authors): The Multi-Agent System Search (MASS) Framework is a three-stage optimization process that navigates a search space encompassing optimizable prompts (instructions and demonstrations) and configurable agent building blocks (Aggregate, Reflect, Debate, Summarize, and Tool-use). The first stage, Block-level Prompt Optimization, independently optimizes prompts for each agent module. Stage two, Workflow Topology Optimization, samples valid system configurations from an influence-weighted design space, integrating the optimized prompts. The final stage, Workflow-level Prompt Optimization, involves a second round of prompt optimization for the entire multi-agent system after the optimal workflow from Stage two has been identified.

<mark>å›¾ 4ï¼šï¼ˆä½œè€…ä¾›å›¾ï¼‰ï¼šå¤šæ™ºèƒ½ä½“ç³»ç»Ÿæœç´¢ï¼ˆMulti-Agent System Search, MASSï¼‰æ¡†æ¶æ˜¯ä¸€ä¸ªä¸‰é˜¶æ®µçš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œå®ƒåœ¨ä¸€ä¸ªåŒ…å«å¯ä¼˜åŒ–æç¤ºï¼ˆæŒ‡ä»¤å’Œæ¼”ç¤ºï¼‰å’Œå¯é…ç½®æ™ºèƒ½ä½“æ„å»ºæ¨¡å—ï¼ˆèšåˆã€åæ€ã€è¾©è®ºã€æ€»ç»“å’Œå·¥å…·ä½¿ç”¨ï¼‰çš„æœç´¢ç©ºé—´ä¸­è¿›è¡Œå¯¼èˆªã€‚ç¬¬ä¸€é˜¶æ®µï¼Œå—çº§æç¤ºä¼˜åŒ–ï¼Œç‹¬ç«‹ä¼˜åŒ–æ¯ä¸ªæ™ºèƒ½ä½“æ¨¡å—çš„æç¤ºã€‚ç¬¬äºŒé˜¶æ®µï¼Œå·¥ä½œæµæ‹“æ‰‘ä¼˜åŒ–ï¼Œä»å½±å“åŠ æƒçš„è®¾è®¡ç©ºé—´ä¸­é‡‡æ ·æœ‰æ•ˆçš„ç³»ç»Ÿé…ç½®ï¼Œå¹¶æ•´åˆä¼˜åŒ–åçš„æç¤ºã€‚æœ€ç»ˆé˜¶æ®µï¼Œå·¥ä½œæµçº§æç¤ºä¼˜åŒ–ï¼Œåœ¨ç¡®å®šç¬¬äºŒé˜¶æ®µçš„æœ€ä½³å·¥ä½œæµä¹‹åï¼Œå¯¹æ•´ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿè¿›è¡Œç¬¬äºŒè½®æç¤ºä¼˜åŒ–ã€‚</mark>

**3. Workflow-Level Prompt Optimization:** The final stage involves a global optimization of the entire system's prompts. After identifying the best-performing topology, the prompts are fine-tuned as a single, integrated entity to ensure they are tailored for orchestration and that agent interdependencies are optimized. As an example, after finding the best topology for the DROP dataset, the final optimization stage refines the "Predictor" agent's prompt. The final, optimized prompt is highly detailed, beginning by providing the agent with a summary of the dataset itself, noting its focus on "extractive question answering" and "numerical information". It then includes few-shot examples of correct question-answering behavior and frames the core instruction as a high-stakes scenario: "You are a highly specialized AI tasked with extracting critical numerical information for an urgent news report. A live broadcast is relying on your accuracy and speed". This multi-faceted prompt, combining meta-knowledge, examples, and role-playing, is tuned specifically for the final workflow to maximize accuracy. 

<mark>** 3. å·¥ä½œæµçº§æç¤ºä¼˜åŒ–ï¼ˆWorkflow-Level Prompt Optimizationï¼‰ï¼š** æœ€ç»ˆé˜¶æ®µæ¶‰åŠå¯¹æ•´ä¸ªç³»ç»Ÿæç¤ºçš„å…¨å±€ä¼˜åŒ–ã€‚åœ¨è¯†åˆ«å‡ºæ€§èƒ½æœ€ä½³çš„æ‹“æ‰‘ç»“æ„åï¼Œå°†æç¤ºä½œä¸ºå•ä¸€ã€é›†æˆçš„å®ä½“è¿›è¡Œå¾®è°ƒï¼Œä»¥ç¡®ä¿å®ƒä»¬é€‚åº”ç¼–æ’ï¼Œå¹¶ä¼˜åŒ–æ™ºèƒ½ä½“ä¹‹é—´çš„ç›¸äº’ä¾èµ–å…³ç³»ã€‚ä¾‹å¦‚ï¼Œåœ¨æ‰¾åˆ° DROP æ•°æ®é›†çš„æœ€ä½³æ‹“æ‰‘ç»“æ„åï¼Œæœ€ç»ˆä¼˜åŒ–é˜¶æ®µä¼šç²¾ç‚¼**ã€Œé¢„æµ‹æ™ºèƒ½ä½“ã€ï¼ˆ"Predictor" agentï¼‰çš„æç¤ºã€‚æœ€ç»ˆä¼˜åŒ–åçš„æç¤ºé«˜åº¦è¯¦ç»†**ï¼Œé¦–å…ˆå‘æ™ºèƒ½ä½“æä¾›æ•°æ®é›†æœ¬èº«çš„æ‘˜è¦ï¼ŒæŒ‡å‡ºå…¶ä¾§é‡äº**ã€ŒæŠ½å–å¼é—®ç­”ã€ï¼ˆ"extractive question answering"ï¼‰å’Œã€Œæ•°å€¼ä¿¡æ¯ã€ï¼ˆ"numerical information"ï¼‰ã€‚ç„¶åï¼Œå®ƒåŒ…å«å°‘é‡ç¤ºä¾‹**ï¼ˆfew-shot examplesï¼‰ï¼Œå±•ç¤ºæ­£ç¡®çš„é—®ç­”è¡Œä¸ºï¼Œå¹¶å°†æ ¸å¿ƒæŒ‡ä»¤æ¡†å®šä¸ºä¸€ä¸ªé«˜é£é™©åœºæ™¯ï¼šã€Œä½ æ˜¯ä¸€ä¸ªé«˜åº¦ä¸“ä¸šåŒ–çš„ AIï¼Œä»»åŠ¡æ˜¯ä¸ºä¸€ç¯‡ç´§æ€¥æ–°é—»æŠ¥é“æå–å…³é”®çš„æ•°å€¼ä¿¡æ¯ã€‚ä¸€æ¬¡ç°åœºç›´æ’­æ­£ä¾èµ–ä½ çš„å‡†ç¡®æ€§å’Œé€Ÿåº¦ã€ã€‚è¿™ç§ç»“åˆäº†å…ƒçŸ¥è¯†ã€ç¤ºä¾‹å’Œè§’è‰²æ‰®æ¼”çš„å¤šæ–¹é¢æç¤ºï¼Œæ˜¯ä¸“é—¨é’ˆå¯¹æœ€ç»ˆå·¥ä½œæµè¿›è¡Œè°ƒä¼˜çš„ï¼Œä»¥æœ€å¤§é™åº¦åœ°æé«˜å‡†ç¡®æ€§ã€‚</mark>

Key Findings and Principles: Experiments demonstrate that MAS optimized by MASS significantly outperform existing manually designed systems and other automated design methods across a range of tasks. The key design principles for effective MAS, as derived from this research, are threefold:

<mark>å…³é”®å‘ç°ä¸åŸåˆ™ï¼š å®éªŒè¯æ˜ï¼Œç» MASS ä¼˜åŒ–çš„ MAS åœ¨ä¸€ç³»åˆ—ä»»åŠ¡ä¸­çš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰æ‰‹åŠ¨è®¾è®¡çš„ç³»ç»Ÿå’Œå…¶ä»–è‡ªåŠ¨åŒ–è®¾è®¡æ–¹æ³•ã€‚æ ¹æ®è¿™é¡¹ç ”ç©¶å¾—å‡ºçš„æœ‰æ•ˆ MAS çš„å…³é”®è®¾è®¡åŸåˆ™æœ‰ä¸‰ç‚¹ï¼š</mark>

  * Optimize individual agents with high-quality prompts before composing them.
  * Construct MAS by composing influential topologies rather than exploring an unconstrained search space.
  * Model and optimize the interdependencies between agents through a final, workflow-level joint optimization.

<mark>
  * åœ¨ç»„åˆæ™ºèƒ½ä½“ä¹‹å‰ï¼Œä½¿ç”¨é«˜è´¨é‡çš„æç¤ºæ¥ä¼˜åŒ–å•ä¸ªæ™ºèƒ½ä½“ã€‚
  * é€šè¿‡ç»„åˆæœ‰å½±å“åŠ›çš„æ‹“æ‰‘ç»“æ„æ¥æ„å»º MASï¼Œè€Œä¸æ˜¯æ¢ç´¢æ— çº¦æŸçš„æœç´¢ç©ºé—´ã€‚
  * é€šè¿‡æœ€ç»ˆçš„å·¥ä½œæµçº§è”åˆä¼˜åŒ–ï¼Œå¯¹æ™ºèƒ½ä½“ä¹‹é—´çš„ç›¸äº’ä¾èµ–å…³ç³»è¿›è¡Œå»ºæ¨¡å’Œä¼˜åŒ–ã€‚</mark>

Building on our discussion of key reasoning techniques, let's first examine a core performance principle: the Scaling Inference Law for LLMs. This law states that a model's performance predictably improves as the computational resources allocated to it increase. We can see this principle in action in complex systems like Deep Research, where an AI agent leverages these resources to autonomously investigate a topic by breaking it down into sub-questions, using Web search as a tool, and synthesizing its findings.

<mark>åœ¨æˆ‘ä»¬è®¨è®ºäº†å…³é”®æ¨ç†æŠ€å·§ä¹‹åï¼Œé¦–å…ˆè®©æˆ‘ä»¬è€ƒå¯Ÿä¸€ä¸ªæ ¸å¿ƒæ€§èƒ½åŸåˆ™ï¼šLLM çš„æ¨ç†æ‰©å±•å®šå¾‹ï¼ˆScaling Inference Law for LLMsï¼‰ã€‚è¯¥å®šå¾‹æŒ‡å‡ºï¼Œéšç€åˆ†é…ç»™æ¨¡å‹çš„è®¡ç®—èµ„æºå¢åŠ ï¼Œæ¨¡å‹çš„æ€§èƒ½ä¼šå¯é¢„æµ‹åœ°æé«˜ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œåœ¨åƒæ·±åº¦ç ”ç©¶ï¼ˆDeep Researchï¼‰è¿™æ ·çš„å¤æ‚ç³»ç»Ÿä¸­ï¼Œè¿™ä¸ªåŸåˆ™æ­£åœ¨å‘æŒ¥ä½œç”¨ï¼ŒAI æ™ºèƒ½ä½“åˆ©ç”¨è¿™äº›èµ„æºè‡ªä¸»è°ƒæŸ¥ä¸€ä¸ªä¸»é¢˜ï¼šå°†å…¶åˆ†è§£ä¸ºå­é—®é¢˜ï¼Œä½¿ç”¨ Web æœç´¢ä½œä¸ºå·¥å…·ï¼Œå¹¶ç»¼åˆå…¶å‘ç°ã€‚</mark>

**Deep Research.** The term "Deep Research" describes a category of AI Agentic tools designed to act as tireless, methodical research assistants. Major platforms in this space include Perplexity AI, Google's Gemini research capabilities, and OpenAI's advanced functions within ChatGPT (see Fig.5).

<mark>** æ·±åº¦ç ”ç©¶ï¼ˆDeep Researchï¼‰ã€‚** ã€Œæ·±åº¦ç ”ç©¶ã€ä¸€è¯æè¿°äº†ä¸€ç±»å…·æ™ºèƒ½ä½“ç‰¹æ€§ï¼ˆAgenticï¼‰çš„ AI å·¥å…·ï¼Œå®ƒä»¬æ—¨åœ¨å……å½“ä¸çŸ¥ç–²å€¦ã€æœ‰æ¡ä¸ç´Šçš„ç ”ç©¶åŠ©ç†ã€‚è¯¥é¢†åŸŸçš„ä¸»è¦å¹³å°åŒ…æ‹¬ Perplexity AIã€Google Gemini çš„ç ”ç©¶èƒ½åŠ›ä»¥åŠ OpenAI ChatGPT å†…éƒ¨çš„é«˜çº§åŠŸèƒ½ï¼ˆå‚è§å›¾ 5ï¼‰ã€‚</mark>

<img width="1314" height="1402" alt="image" src="https://github.com/user-attachments/assets/a475f47d-69c3-4dd7-8e8b-2827e1d0f17d" />

Fig. 5: Google Deep Research for Information Gathering
<mark>å›¾ 5ï¼šç”¨äºä¿¡æ¯æ”¶é›†çš„ Google Deep Research</mark>

A fundamental shift introduced by these tools is the change in the search process itself. A standard search provides immediate links, leaving the work of synthesis to you. Deep Research operates on a different model. Here, you task an AI with a complex query and grant it a "time budget"â€”usually a few minutes. In return for this patience, you receive a detailed report.

<mark>è¿™äº›å·¥å…·å¸¦æ¥çš„ä¸€ä¸ªæ ¹æœ¬æ€§è½¬å˜æ˜¯æœç´¢è¿‡ç¨‹æœ¬èº«çš„æ”¹å˜ã€‚æ ‡å‡†æœç´¢ä¼šç«‹å³æä¾›é“¾æ¥ï¼Œå°†ç»¼åˆæ•´ç†çš„å·¥ä½œç•™ç»™ä½ ã€‚è€Œæ·±åº¦ç ”ç©¶åˆ™é‡‡ç”¨ä¸åŒçš„æ¨¡å¼ã€‚åœ¨è¿™é‡Œï¼Œä½ ç»™ AI åˆ†é…ä¸€ä¸ªå¤æ‚çš„æŸ¥è¯¢ä»»åŠ¡ï¼Œå¹¶æˆäºˆå®ƒä¸€ä¸ªã€Œæ—¶é—´é¢„ç®—ã€â€”â€”é€šå¸¸æ˜¯å‡ åˆ†é’Ÿã€‚ä½œä¸ºè¿™ç§è€å¿ƒçš„å›æŠ¥ï¼Œä½ å°†æ”¶åˆ°ä¸€ä»½è¯¦ç»†çš„æŠ¥å‘Šã€‚</mark>

During this time, the AI works on your behalf in an agentic way. It autonomously performs a series of sophisticated steps that would be incredibly time-consuming for a person:

<mark>åœ¨æ­¤æœŸé—´ï¼ŒAI ä»¥ä¸€ç§å…·æ™ºèƒ½ä½“ç‰¹æ€§çš„æ–¹å¼ä¸ºä½ å·¥ä½œã€‚å®ƒè‡ªä¸»æ‰§è¡Œä¸€ç³»åˆ—å¤æ‚ä¸”å¯¹äººæ¥è¯´æå…¶è€—æ—¶çš„æ­¥éª¤ï¼š</mark>

1. Initial Exploration: It runs multiple, targeted searches based on your initial prompt.

<mark>1. åˆå§‹æ¢ç´¢ï¼š å®ƒæ ¹æ®ä½ çš„åˆå§‹æç¤ºè¿è¡Œå¤šä¸ªæœ‰é’ˆå¯¹æ€§çš„æœç´¢ã€‚</mark>

2. Reasoning and Refinement: It reads and analyzes the first wave of results, synthesizes the findings, and critically identifies gaps, contradictions, or areas that require more detail.

<mark>2. æ¨ç†ä¸ç²¾ç‚¼ï¼š å®ƒé˜…è¯»å’Œåˆ†æç¬¬ä¸€æ³¢ç»“æœï¼Œç»¼åˆå…¶å‘ç°ï¼Œå¹¶æ‰¹åˆ¤æ€§åœ°è¯†åˆ«å‡ºç©ºç™½ã€çŸ›ç›¾æˆ–éœ€è¦æ›´å¤šç»†èŠ‚çš„é¢†åŸŸã€‚</mark>

3. Follow-up Inquiry: Based on its internal reasoning, it conducts new, more nuanced searches to fill those gaps and deepen its understanding.

<mark>3. åç»­æ¢ç©¶ï¼š åŸºäºå…¶å†…éƒ¨æ¨ç†ï¼Œå®ƒè¿›è¡Œæ–°çš„ã€æ›´ç»†è‡´çš„æœç´¢ï¼Œä»¥å¡«è¡¥è¿™äº›ç©ºç™½å¹¶åŠ æ·±å…¶ç†è§£ã€‚</mark>

4. Final Synthesis: After several rounds of this iterative searching and reasoning, it compiles all the validated information into a single, cohesive, and structured summary.

<mark>4. æœ€ç»ˆç»¼åˆï¼š ç»è¿‡å¤šè½®è¿™ç§è¿­ä»£æœç´¢å’Œæ¨ç†åï¼Œå®ƒå°†æ‰€æœ‰ç»è¿‡éªŒè¯çš„ä¿¡æ¯æ±‡ç¼–æˆä¸€ä¸ªå•ä¸€ã€æœ‰å‡èšåŠ›ä¸”ç»“æ„åŒ–çš„æ‘˜è¦ã€‚</mark>

This systematic approach ensures a comprehensive and well-reasoned response, significantly enhancing the efficiency and depth of information gathering, thereby facilitating more agentic decision-making.

<mark>è¿™ç§ç³»ç»Ÿæ–¹æ³•ç¡®ä¿äº†å…¨é¢ä¸”æœ‰å……åˆ†ç†ç”±æ”¯æŒçš„å“åº”ï¼Œæ˜¾è‘—æé«˜äº†ä¿¡æ¯æ”¶é›†çš„æ•ˆç‡å’Œæ·±åº¦ï¼Œä»è€Œä¿ƒè¿›äº†æ›´å…·æ™ºèƒ½ä½“ç‰¹æ€§çš„å†³ç­–åˆ¶å®šã€‚</mark>

## Scaling Inference Law
This critical principle dictates the relationship between an LLM's performance and the computational resources allocated during its operational phase, known as inference. The Inference Scaling Law differs from the more familiar scaling laws for training, which focus on how model quality improves with increased data volume and computational power during a model's creation. Instead, this law specifically examines the dynamic trade-offs that occur when an LLM is actively generating an output or answer.
A cornerstone of this law is the revelation that superior results can frequently be achieved from a comparatively smaller LLM by augmenting the computational investment at inference time. This doesn't necessarily mean using a more powerful GPU, but rather employing more sophisticated or resource-intensive inference strategies. A prime example of such a strategy is instructing the model to generate multiple potential answersâ€”perhaps through techniques like diverse beam search or self-consistency methodsâ€”and then employing a selection mechanism to identify the most optimal output. This iterative refinement or multiple-candidate generation process demands more computational cycles but can significantly elevate the quality of the final response.
This principle offers a crucial framework for informed and economically sound decision-making in the deployment of Agents systems. It challenges the intuitive notion that a larger model will always yield better performance. The law posits that a smaller model, when granted a more substantial "thinking budget" during inference, can occasionally surpass the performance of a much larger model that relies on a simpler, less computationally intensive generation process. The "thinking budget" here refers to the additional computational steps or complex algorithms applied during inference, allowing the smaller model to explore a wider range of possibilities or apply more rigorous internal checks before settling on an answer.
Consequently, the Scaling Inference Law becomes fundamental to constructing efficient and cost-effective Agentic systems. It provides a methodology for meticulously balancing several interconnected factors:
â—**Model Size**: Smaller models are inherently less demanding in terms of memory and storage.
â—**Response Latency**: While increased inference-time computation can add to latency, the law helps identify the point at which the performance gains outweigh this increase, or how to strategically apply computation to avoid excessive delays.
â—**Operational Cost**: Deploying and running larger models typically incurs higher ongoing operational costs due to increased power consumption and infrastructure requirements. The law demonstrates how to optimize performance without unnecessarily escalating these costs.
By understanding and applying the Scaling Inference Law, developers and organizations can make strategic choices that lead to optimal performance for specific agentic applications, ensuring that computational resources are allocated where they will have the most significant impact on the quality and utility of the LLM's output. This allows for more nuanced and economically viable approaches to AI deployment, moving beyond a simple "bigger is better" paradigm.
<mark>
## æ¨ç†ç¼©æ”¾å®šå¾‹ 
è¿™ä¸€å…³é”®åŸåˆ™è§„å®šäº†åœ¨ LLM çš„è¿è¡Œé˜¶æ®µï¼ˆç§°ä¸ºæ¨ç†ï¼‰æœŸé—´ï¼ŒLLM çš„æ€§èƒ½ä¸åˆ†é…ç»™å®ƒçš„è®¡ç®—èµ„æºä¹‹é—´çš„å…³ç³»ã€‚æ¨ç†ç¼©æ”¾å®šå¾‹ä¸åŒäºæ›´ç†Ÿæ‚‰çš„è®­ç»ƒç¼©æ”¾å®šå¾‹ï¼Œåè€…ä¾§é‡äºæ¨¡å‹çš„è´¨é‡å¦‚ä½•éšç€æ¨¡å‹åˆ›å»ºè¿‡ç¨‹ä¸­çš„æ•°æ®é‡å’Œè®¡ç®—èƒ½åŠ›çš„å¢åŠ è€Œæé«˜ã€‚ç›¸åï¼Œè¯¥å®šå¾‹ä¸“é—¨ç ”ç©¶å½“ LLM ç§¯æç”Ÿæˆè¾“å‡ºæˆ–ç­”æ¡ˆæ—¶æ‰€å‘ç”Ÿçš„åŠ¨æ€æƒè¡¡ã€‚

è¯¥å®šå¾‹çš„ä¸€ä¸ªåŸºçŸ³åœ¨äºæ­ç¤ºï¼Œé€šè¿‡åœ¨æ¨ç†æ—¶å¢åŠ è®¡ç®—æŠ•å…¥ï¼Œå¾€å¾€å¯ä»¥ä»å°å‹çš„ LLM ä¸­è·å¾—æ›´ä¼˜çš„ç»“æœã€‚è¿™ä¸ä¸€å®šæ„å‘³ç€ä½¿ç”¨æ›´å¼ºå¤§çš„ GPUï¼Œè€Œæ˜¯é‡‡ç”¨æ›´å¤æ‚æˆ–èµ„æºå¯†é›†å‹çš„æ¨ç†ç­–ç•¥ã€‚è¿™ç§ç­–ç•¥çš„ä¸€ä¸ªä¸»è¦ä¾‹å­æ˜¯æŒ‡ç¤ºæ¨¡å‹ç”Ÿæˆå¤šä¸ªæ½œåœ¨ç­”æ¡ˆâ€”â€”å¯èƒ½é€šè¿‡è¯¸å¦‚å¤šæ ·åŒ–æŸæœç´¢æˆ–è‡ªæ´½æ€§æ–¹æ³•ç­‰æŠ€æœ¯â€”â€”ç„¶åé‡‡ç”¨é€‰æ‹©æœºåˆ¶æ¥ç¡®å®šæœ€ä¼˜è¾“å‡ºã€‚è¿™ç§è¿­ä»£ç²¾ç‚¼æˆ–å¤šå€™é€‰ç”Ÿæˆè¿‡ç¨‹éœ€è¦æ›´å¤šçš„è®¡ç®—å‘¨æœŸï¼Œä½†å¯ä»¥æ˜¾è‘—æå‡æœ€ç»ˆå“åº”çš„è´¨é‡ã€‚

è¯¥åŸåˆ™ä¸º Agent ç³»ç»Ÿéƒ¨ç½²ä¸­çš„çŸ¥æƒ…ä¸”ç»æµåˆç†çš„å†³ç­–æä¾›äº†ä¸€ä¸ªå…³é”®æ¡†æ¶ã€‚å®ƒæŒ‘æˆ˜äº†â€œæ›´å¤§çš„æ¨¡å‹æ€»æ˜¯èƒ½å¸¦æ¥æ›´å¥½æ€§èƒ½â€çš„ç›´è§‚è§‚å¿µã€‚è¯¥å®šå¾‹è®¤ä¸ºï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œä¸€ä¸ªè¾ƒå°çš„æ¨¡å‹å¦‚æœè·å¾—æ›´å¤šçš„â€œæ€è€ƒé¢„ç®—â€ï¼Œæœ‰æ—¶å¯ä»¥è¶…è¶Šä¸€ä¸ªä¾èµ–äºæ›´ç®€å•ã€è®¡ç®—å¯†é›†åº¦è¾ƒä½çš„ç”Ÿæˆè¿‡ç¨‹çš„æ›´å¤§æ¨¡å‹ã€‚â€œæ€è€ƒé¢„ç®—â€åœ¨æ­¤æŒ‡çš„æ˜¯åœ¨æ¨ç†è¿‡ç¨‹ä¸­åº”ç”¨çš„é¢å¤–è®¡ç®—æ­¥éª¤æˆ–å¤æ‚ç®—æ³•ï¼Œå®ƒå…è®¸è¾ƒå°çš„æ¨¡å‹åœ¨æœ€ç»ˆç¡®å®šç­”æ¡ˆä¹‹å‰æ¢ç´¢æ›´å¹¿æ³›çš„å¯èƒ½æ€§æˆ–åº”ç”¨æ›´ä¸¥æ ¼çš„å†…éƒ¨æ£€æŸ¥ã€‚

å› æ­¤ï¼Œæ¨ç†ç¼©æ”¾å®šå¾‹å¯¹äºæ„å»ºé«˜æ•ˆä¸”æˆæœ¬æ•ˆç›Šé«˜çš„ Agentic ç³»ç»Ÿè‡³å…³é‡è¦ã€‚å®ƒæä¾›äº†ä¸€ç§ç»†è‡´å¹³è¡¡å‡ ä¸ªç›¸äº’å…³è”å› ç´ çš„æ–¹æ³•ï¼š

â—**æ¨¡å‹å¤§å°**ï¼šè¾ƒå°çš„æ¨¡å‹åœ¨å†…å­˜å’Œå­˜å‚¨æ–¹é¢æœ¬èº«è¦æ±‚è¾ƒä½ã€‚
â—**å“åº”å»¶è¿Ÿ**ï¼šè™½ç„¶å¢åŠ æ¨ç†æ—¶é—´çš„è®¡ç®—ä¼šå¢åŠ å»¶è¿Ÿï¼Œä½†è¯¥å®šå¾‹æœ‰åŠ©äºç¡®å®šæ€§èƒ½æå‡è¶…è¿‡è¿™ç§å¢åŠ çš„é˜ˆå€¼ï¼Œæˆ–è€…å¦‚ä½•ç­–ç•¥æ€§åœ°åº”ç”¨è®¡ç®—ä»¥é¿å…è¿‡åº¦çš„å»¶è¿Ÿã€‚
â—**è¿è¥æˆæœ¬**ï¼šéƒ¨ç½²å’Œè¿è¡Œæ›´å¤§çš„æ¨¡å‹é€šå¸¸ä¼šå› æ›´é«˜çš„åŠŸè€—å’ŒåŸºç¡€è®¾æ–½éœ€æ±‚è€Œäº§ç”Ÿæ›´é«˜çš„æŒç»­è¿è¥æˆæœ¬ã€‚è¯¥å®šå¾‹å±•ç¤ºäº†å¦‚ä½•åœ¨ä¸ä¸å¿…è¦åœ°å¢åŠ è¿™äº›æˆæœ¬çš„æƒ…å†µä¸‹ä¼˜åŒ–æ€§èƒ½ã€‚

é€šè¿‡ç†è§£å’Œåº”ç”¨æ¨ç†ç¼©æ”¾å®šå¾‹ï¼Œå¼€å‘äººå‘˜å’Œç»„ç»‡å¯ä»¥åšå‡ºæˆ˜ç•¥é€‰æ‹©ï¼Œä»è€Œä¸ºç‰¹å®šçš„ agentic åº”ç”¨å¸¦æ¥æœ€ä½³æ€§èƒ½ï¼Œç¡®ä¿è®¡ç®—èµ„æºè¢«åˆ†é…åˆ°å¯¹ LLM è¾“å‡ºçš„è´¨é‡å’Œæ•ˆç”¨å½±å“æœ€å¤§çš„åœ°æ–¹ã€‚è¿™ä½¿å¾— AI éƒ¨ç½²èƒ½å¤Ÿé‡‡å–æ›´ç»†è‡´ã€ç»æµä¸Šå¯è¡Œçš„æ–¹â€‹â€‹æ³•ï¼Œè¶…è¶Šç®€å•çš„â€œè¶Šå¤§è¶Šå¥½â€çš„èŒƒå¼ã€‚
</mark>

## Hands-On Code Example
The DeepSearch code, open-sourced by Google, is available through the gemini-fullstack-langgraph-quickstart repository (Fig. 6). This repository provides a template for developers to construct full-stack AI agents using Gemini 2.5 and the LangGraph orchestration framework. This open-source stack facilitates experimentation with agent-based architectures and can be integrated with local LLLMs such as Gemma. It utilizes Docker and modular project scaffolding for rapid prototyping. It should be noted that this release serves as a well-structured demonstration and is not intended as a production-ready backend.

<mark>Google å¼€æºçš„ DeepSearch ä»£ç ï¼Œå¯é€šè¿‡ gemini-fullstack-langgraph-quickstart ä»“åº“ï¼ˆå›¾ 6ï¼‰è·å–ã€‚è¯¥ä»“åº“ä¸ºå¼€å‘è€…æä¾›äº†ä¸€ä¸ªæ¨¡æ¿ï¼Œç”¨äºä½¿ç”¨ Gemini 2.5 å’Œ LangGraph ç¼–æ’æ¡†æ¶æ„å»ºå…¨æ ˆ AI æ™ºèƒ½ä½“ã€‚è¿™ç§å¼€æºæ ˆä¾¿äºè¿›è¡Œæ™ºèƒ½ä½“æ¶æ„çš„å®éªŒï¼Œå¹¶å¯ä¸ Gemma ç­‰æœ¬åœ° LLM é›†æˆã€‚å®ƒåˆ©ç”¨ Docker å’Œæ¨¡å—åŒ–é¡¹ç›®è„šæ‰‹æ¶å®ç°å¿«é€ŸåŸå‹å¼€å‘ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæœ¬æ¬¡å‘å¸ƒæ˜¯ä¸€ä¸ªç»“æ„è‰¯å¥½çš„æ¼”ç¤ºï¼Œå¹¶éç”¨äºç”Ÿäº§ç¯å¢ƒçš„åç«¯ã€‚</mark>

<img width="959" height="1474" alt="image" src="https://github.com/user-attachments/assets/32c32593-42c9-41eb-bd69-d31137dd22cc" />

Fig. 6: (Courtesy of authors) Example of DeepSearch with multiple Reflection steps
å›¾ 6ï¼šï¼ˆä½œè€…æä¾›ï¼‰åŒ…å«å¤šä¸ªåæ€æ­¥éª¤çš„ DeepSearch ç¤ºä¾‹

This project provides a full-stack application featuring a React frontend and a LangGraph backend, designed for advanced research and conversational AI. A LangGraph agent dynamically generates search queries using Google Gemini models and integrates web research via the Google Search API. The system employs reflective reasoning to identify knowledge gaps, refine searches iteratively, and synthesize answers with citations. The frontend and backend support hot-reloading. The project's structure includes separate frontend/ and backend/ directories. Requirements for setup include Node.js, npm, Python 3.8+, and a Google Gemini API key. After configuring the API key in the backend's .env file, dependencies for both the backend (using pip install .) and frontend (npm install) can be installed. Development servers can be run concurrently with make dev or individually. The backend agent, defined in backend/src/agent/graph.py, generates initial search queries, conducts web research, performs knowledge gap analysis, refines queries iteratively, and synthesizes a cited answer using a Gemini model. Production deployment involves the backend server delivering a static frontend build and requires Redis for streaming real-time output and a Postgres database for managing data. A Docker image can be built and run using docker-compose up, which also requires a LangSmith API key for the docker-compose.yml example. The application utilizes React with Vite, Tailwind CSS, Shadcn UI, LangGraph, and Google Gemini. The project is licensed under the Apache License 2.0.
<mark>
æœ¬é¡¹ç›®æä¾›äº†ä¸€ä¸ªå…¨æ ˆåº”ç”¨ç¨‹åºï¼ŒåŒ…å« React å‰ç«¯å’Œ LangGraph åç«¯ï¼Œä¸“ä¸ºé«˜çº§ç ”ç©¶å’Œå¯¹è¯å¼ AI è®¾è®¡ã€‚LangGraph æ™ºèƒ½ä½“ä½¿ç”¨ Google Gemini æ¨¡å‹åŠ¨æ€ç”Ÿæˆæœç´¢æŸ¥è¯¢ï¼Œå¹¶é€šè¿‡ Google Search API é›†æˆç½‘ç»œç ”ç©¶ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨åæ€æ€§æ¨ç†æ¥è¯†åˆ«çŸ¥è¯†å·®è·ï¼Œè¿­ä»£åœ°ä¼˜åŒ–æœç´¢ï¼Œå¹¶åˆæˆå¸¦å¼•ç”¨çš„ç­”æ¡ˆã€‚å‰ç«¯å’Œåç«¯æ”¯æŒçƒ­é‡è½½ã€‚é¡¹ç›®çš„ç»“æ„åŒ…æ‹¬ç‹¬ç«‹çš„ frontend/ å’Œ backend/ ç›®å½•ã€‚è®¾ç½®è¦æ±‚åŒ…æ‹¬ Node.jsã€npmã€Python 3.8+ å’Œ Google Gemini API å¯†é’¥ã€‚åœ¨é…ç½®å¥½åç«¯ .env æ–‡ä»¶ä¸­çš„ API å¯†é’¥åï¼Œå¯ä»¥å®‰è£…åç«¯ï¼ˆä½¿ç”¨ pip install .ï¼‰å’Œå‰ç«¯ï¼ˆnpm installï¼‰çš„ä¾èµ–é¡¹ã€‚å¼€å‘æœåŠ¡å™¨å¯ä»¥ä¸ make dev åŒæ—¶è¿è¡Œï¼Œæˆ–å•ç‹¬è¿è¡Œã€‚åç«¯æ™ºèƒ½ä½“å®šä¹‰åœ¨ backend/src/agent/graph.py ä¸­ï¼Œè´Ÿè´£ç”Ÿæˆåˆå§‹æœç´¢æŸ¥è¯¢ï¼Œè¿›è¡Œç½‘ç»œç ”ç©¶ï¼Œæ‰§è¡ŒçŸ¥è¯†å·®è·åˆ†æï¼Œè¿­ä»£ä¼˜åŒ–æŸ¥è¯¢ï¼Œå¹¶ä½¿ç”¨ Gemini æ¨¡å‹åˆæˆå¸¦å¼•ç”¨çš„ç­”æ¡ˆã€‚ç”Ÿäº§éƒ¨ç½²æ¶‰åŠåç«¯æœåŠ¡å™¨äº¤ä»˜é™æ€å‰ç«¯æ„å»ºï¼Œå¹¶éœ€è¦ Redis è¿›è¡Œå®æ—¶è¾“å‡ºæµå¼ä¼ è¾“ï¼Œä»¥åŠ Postgres æ•°æ®åº“ç”¨äºç®¡ç†æ•°æ®ã€‚å¯ä»¥ä½¿ç”¨ docker-compose up æ„å»ºå’Œè¿è¡Œ Docker é•œåƒï¼Œè¯¥é•œåƒä¹Ÿéœ€è¦ LangSmith API å¯†é’¥ç”¨äº docker-compose.yml ç¤ºä¾‹ã€‚è¯¥åº”ç”¨ç¨‹åºä½¿ç”¨ Viteã€Tailwind CSSã€Shadcn UIã€LangGraph å’Œ Google Gemini çš„ Reactã€‚è¯¥é¡¹ç›®æ ¹æ® Apache License 2.0 æˆæƒã€‚
</mark>

```python
# Create our Agent Graph
builder = StateGraph(OverallState, config_schema=Configuration)
# Define the nodes we will cycle between
builder.add_node("generate_query", generate_query)
builder.add_node("web_research", web_research)
builder.add_node("reflection", reflection)
builder.add_node("finalize_answer", finalize_answer)
# Set the entrypoint as `generate_query`
# This means that this node is the first one called
builder.add_edge(START, "generate_query")
# Add conditional edge to continue with search queries in a parallel
branch
builder.add_conditional_edges(
   "generate_query", continue_to_web_research, ["web_research"]
)
# Reflect on the web research
builder.add_edge("web_research", "reflection")
# Evaluate the research
builder.add_conditional_edges(
   "reflection", evaluate_research, ["web_research",
"finalize_answer"]
)
# Finalize the answer
builder.add_edge("finalize_answer", END)
graph = builder.compile(name="pro-search-agent")
```
Fig.4: Example of DeepSearch with LangGraph (code from backend/src/agent/graph.py)
## So, what do agents think?
In summary, an agent's thinking process is a structured approach that combines reasoning and acting to solve problems. This method allows an agent to explicitly plan its steps, monitor its progress, and interact with external tools to gather information.
At its core, the agent's "thinking" is facilitated by a powerful LLM. This LLM generates a series of thoughts that guide the agent's subsequent actions. The process typically follows a thought-action-observation loop:
1. **Thought**:Theagentfirstgeneratesatextualthoughtthatbreaksdownthe problem, formulates a plan, or analyzes the current situation. This internal monologue makes the agent's reasoning process transparent and steerable.
2. **Action**:Basedonthethought,theagentselectsanactionfromapredefined, discrete set of options. For example, in a question-answering scenario, the action space might include searching online, retrieving information from a specific webpage, or providing a final answer.
3. **Observation**:Theagentthenreceivesfeedbackfromitsenvironmentbasedon the action taken. This could be the results of a web search or the content of a webpage.
This cycle repeats, with each observation informing the next thought, until the agent determines that it has reached a final solution and performs a "finish" action.
The effectiveness of this approach relies on the advanced reasoning and planning capabilities of the underlying LLM. To guide the agent, the ReAct framework often employs few-shot learning, where the LLM is provided with examples of human-like problem-solving trajectories. These examples demonstrate how to effectively combine thoughts and actions to solve similar tasks.
The frequency of an agent's thoughts can be adjusted depending on the task. For knowledge-intensive reasoning tasks like fact-checking, thoughts are typically interleaved with every action to ensure a logical flow of information gathering and reasoning. In contrast, for decision-making tasks that require many actions, such as navigating a simulated environment, thoughts may be used more sparingly, allowing the agent to decide when thinking is necessary
<mark>
## é‚£ä¹ˆï¼Œæ™ºèƒ½ä½“æ˜¯å¦‚ä½•æ€è€ƒçš„ï¼Ÿ

ç®€å•æ¥è¯´ï¼Œæ™ºèƒ½ä½“çš„æ€è€ƒè¿‡ç¨‹æ˜¯ä¸€ç§ç»“æ„åŒ–çš„æ–¹æ³•ï¼Œå®ƒç»“åˆäº†æ¨ç†å’Œè¡ŒåŠ¨æ¥è§£å†³é—®é¢˜ã€‚è¿™ç§æ–¹æ³•å…è®¸æ™ºèƒ½ä½“æ˜ç¡®åœ°è§„åˆ’å…¶æ­¥éª¤ã€ç›‘æ§å…¶è¿›å±•ï¼Œå¹¶ä¸å¤–éƒ¨å·¥å…·äº¤äº’ä»¥æ”¶é›†ä¿¡æ¯ã€‚

å…¶æ ¸å¿ƒåœ¨äºï¼Œæ™ºèƒ½ä½“çš„â€œæ€è€ƒâ€æ˜¯ç”±ä¸€ä¸ªå¼ºå¤§çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„ã€‚è¿™ä¸ª LLM ä¼šç”Ÿæˆä¸€ç³»åˆ—æŒ‡å¯¼æ™ºèƒ½ä½“åç»­è¡ŒåŠ¨çš„æ€è€ƒã€‚æ•´ä¸ªè¿‡ç¨‹é€šå¸¸éµå¾ªä¸€ä¸ªâ€œæ€è€ƒ-è¡ŒåŠ¨-è§‚å¯Ÿâ€çš„å¾ªç¯ï¼š

1.  **æ€è€ƒ (Thought)**ï¼šæ™ºèƒ½ä½“é¦–å…ˆç”Ÿæˆä¸€ä¸ªæ–‡æœ¬æ€è€ƒï¼Œè¯¥æ€è€ƒä¼šåˆ†è§£é—®é¢˜ã€åˆ¶å®šè®¡åˆ’æˆ–åˆ†æå½“å‰æƒ…å†µã€‚è¿™ç§å†…éƒ¨çš„ç‹¬ç™½ä½¿å¾—æ™ºèƒ½ä½“çš„æ¨ç†è¿‡ç¨‹é€æ˜ä¸”å¯æ§ã€‚
2.  **è¡ŒåŠ¨ (Action)**ï¼šåŸºäºæ€è€ƒï¼Œæ™ºèƒ½ä½“ä»ä¸€ç»„é¢„å®šä¹‰çš„ã€ç¦»æ•£çš„é€‰é¡¹ä¸­é€‰æ‹©ä¸€ä¸ªè¡ŒåŠ¨ã€‚ä¾‹å¦‚ï¼Œåœ¨é—®ç­”åœºæ™¯ä¸­ï¼Œè¡ŒåŠ¨ç©ºé—´å¯èƒ½åŒ…æ‹¬åœ¨çº¿æœç´¢ã€ä»ç‰¹å®šç½‘é¡µæ£€ç´¢ä¿¡æ¯ï¼Œæˆ–æä¾›æœ€ç»ˆç­”æ¡ˆã€‚
3.  **è§‚å¯Ÿ (Observation)**ï¼šæ™ºèƒ½ä½“éšåæ ¹æ®æ‰€é‡‡å–çš„è¡ŒåŠ¨æ¥æ”¶æ¥è‡ªç¯å¢ƒçš„åé¦ˆã€‚è¿™å¯èƒ½æ˜¯ç½‘ç»œæœç´¢çš„ç»“æœï¼Œæˆ–æ˜¯ç½‘é¡µçš„å†…å®¹ã€‚

è¿™ä¸ªå¾ªç¯ä¼šä¸æ–­é‡å¤ï¼Œæ¯ä¸€æ¬¡è§‚å¯Ÿéƒ½ä¼šä¸ºä¸‹ä¸€æ¬¡æ€è€ƒæä¾›ä¿¡æ¯ï¼Œç›´åˆ°æ™ºèƒ½ä½“ç¡®å®šå®ƒå·²è¾¾æˆæœ€ç»ˆè§£å†³æ–¹æ¡ˆå¹¶æ‰§è¡Œâ€œå®Œæˆâ€è¡ŒåŠ¨ã€‚

è¿™ç§æ–¹æ³•çš„æœ‰æ•ˆæ€§ä¾èµ–äºåº•å±‚ LLM çš„é«˜çº§æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ã€‚ä¸ºäº†æŒ‡å¯¼æ™ºèƒ½ä½“ï¼ŒReAct æ¡†æ¶é€šå¸¸é‡‡ç”¨å°‘æ ·æœ¬å­¦ä¹ ï¼ˆfew-shot learningï¼‰ï¼Œå³å‘ LLM æä¾›ç±»ä¼¼äººç±»è§£å†³é—®é¢˜è½¨è¿¹çš„ç¤ºä¾‹ã€‚è¿™äº›ç¤ºä¾‹æ¼”ç¤ºäº†å¦‚ä½•æœ‰æ•ˆåœ°ç»“åˆæ€è€ƒå’Œè¡ŒåŠ¨æ¥è§£å†³ç›¸ä¼¼çš„ä»»åŠ¡ã€‚

æ™ºèƒ½ä½“çš„æ€è€ƒé¢‘ç‡å¯ä»¥æ ¹æ®ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚å¯¹äºåƒäº‹å®æ ¸æŸ¥è¿™æ ·çš„çŸ¥è¯†å¯†é›†å‹æ¨ç†ä»»åŠ¡ï¼Œæ€è€ƒé€šå¸¸ä¼šä¸æ¯ä¸ªè¡ŒåŠ¨äº¤ç»‡è¿›è¡Œï¼Œä»¥ç¡®ä¿ä¿¡æ¯æ”¶é›†å’Œæ¨ç†çš„é€»è¾‘æµç¨‹ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¯¹äºéœ€è¦è®¸å¤šè¡ŒåŠ¨çš„å†³ç­–ä»»åŠ¡ï¼Œä¾‹å¦‚åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­å¯¼èˆªï¼Œæ€è€ƒå¯èƒ½ä¼šæ›´è°¨æ…åœ°ä½¿ç”¨ï¼Œè®©æ™ºèƒ½ä½“è‡ªè¡Œå†³å®šä½•æ—¶éœ€è¦æ€è€ƒã€‚
</mark>

## At a Glance
**What**: Complex problem-solving often requires more than a single, direct answer, posing a significant challenge for AI. The core problem is enabling AI agents to tackle multi-step tasks that demand logical inference, decomposition, and strategic planning. Without a structured approach, agents may fail to handle intricacies, leading to inaccurate or incomplete conclusions. These advanced reasoning methodologies aim to make an agent's internal "thought" process explicit, allowing it to systematically work through challenges.
**Why**: The standardized solution is a suite of reasoning techniques that provide a structured framework for an agent's problem-solving process. Methodologies like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) guide LLMs to break down problems and explore multiple solution paths. Self-Correction allows for the iterative refinement of answers, ensuring higher accuracy. Agentic frameworks like ReAct integrate reasoning with action, enabling agents to interact with external tools and environments to gather information and adapt their plans. This combination of explicit reasoning, exploration, refinement, and tool use creates more robust, transparent, and capable AI systems.
**Rule of thumb**: Use these reasoning techniques when a problem is too complex for a single-pass answer and requires decomposition, multi-step logic, interaction with external data sources or tools, or strategic planning and adaptation. They are ideal for tasks where showing the "work" or thought process is as important as the final answer.

<mark>
## è¦ç‚¹é€Ÿè§ˆ

**é—®é¢˜æ‰€åœ¨ (What)**ï¼šå¤æ‚é—®é¢˜çš„è§£å†³å¸¸å¸¸éœ€è¦ä¸æ­¢ä¸€ä¸ªç›´æ¥ç­”æ¡ˆï¼Œè¿™å¯¹ AI æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚æ ¸å¿ƒé—®é¢˜æ˜¯å¦‚ä½•è®© AI æ™ºèƒ½ä½“èƒ½å¤Ÿå¤„ç†éœ€è¦é€»è¾‘æ¨ç†ã€åˆ†è§£å’Œæˆ˜ç•¥è§„åˆ’çš„å¤šæ­¥éª¤ä»»åŠ¡ã€‚æ²¡æœ‰ç»“æ„åŒ–çš„æ–¹æ³•ï¼Œæ™ºèƒ½ä½“å¯èƒ½æ— æ³•å¤„ç†å…¶ä¸­çš„å¤æ‚æ€§ï¼Œå¯¼è‡´ä¸å‡†ç¡®æˆ–ä¸å®Œæ•´çš„ç»“è®ºã€‚è¿™äº›é«˜çº§æ¨ç†æ–¹æ³•æ—¨åœ¨ä½¿æ™ºèƒ½ä½“çš„å†…éƒ¨â€œæ€è€ƒâ€è¿‡ç¨‹æ˜ç¡®åŒ–ï¼Œä»è€Œä½¿å…¶èƒ½å¤Ÿç³»ç»Ÿåœ°åº”å¯¹æŒ‘æˆ˜ã€‚

**è§£å†³ä¹‹é“ (Why)**ï¼šæ ‡å‡†åŒ–çš„è§£å†³æ–¹æ¡ˆæ˜¯ä¸€ç³»åˆ—æ¨ç†æŠ€æœ¯ï¼Œä¸ºæ™ºèƒ½ä½“çš„è§£å†³é—®é¢˜è¿‡ç¨‹æä¾›äº†ä¸€ä¸ªç»“æ„åŒ–æ¡†æ¶ã€‚åƒæ€ç»´é“¾ï¼ˆChain-of-Thought, CoTï¼‰å’Œæ€ç»´æ ‘ï¼ˆTree-of-Thought, ToTï¼‰è¿™æ ·çš„æ–¹æ³•å¯ä»¥æŒ‡å¯¼ LLM åˆ†è§£é—®é¢˜å¹¶æ¢ç´¢å¤šç§è§£å†³æ–¹æ¡ˆè·¯å¾„ã€‚è‡ªæˆ‘çº æ­£ï¼ˆSelf-Correctionï¼‰åˆ™å…è®¸å¯¹ç­”æ¡ˆè¿›è¡Œè¿­ä»£æ”¹è¿›ï¼Œç¡®ä¿æ›´é«˜çš„å‡†ç¡®æ€§ã€‚ReAct ç­‰æ™ºèƒ½ä½“æ¡†æ¶å°†æ¨ç†ä¸è¡ŒåŠ¨ç›¸ç»“åˆï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿä¸å¤–éƒ¨å·¥å…·å’Œç¯å¢ƒäº¤äº’ä»¥æ”¶é›†ä¿¡æ¯å¹¶è°ƒæ•´å…¶è®¡åˆ’ã€‚è¿™ç§æ˜¾æ€§æ¨ç†ã€æ¢ç´¢ã€æ”¹è¿›å’Œå·¥å…·ä½¿ç”¨çš„ç»“åˆï¼Œåˆ›é€ äº†æ›´å¼ºå¤§ã€æ›´é€æ˜ã€èƒ½åŠ›æ›´å¼ºçš„ AI ç³»ç»Ÿã€‚

**ç»éªŒæ³•åˆ™ (Rule of thumb)**ï¼šå½“ä¸€ä¸ªé—®é¢˜è¿‡äºå¤æ‚ï¼Œæ— æ³•ä¸€æ¬¡æ€§ç»™å‡ºç­”æ¡ˆï¼Œéœ€è¦åˆ†è§£ã€å¤šæ­¥é€»è¾‘ã€ä¸å¤–éƒ¨æ•°æ®æºæˆ–å·¥å…·äº¤äº’ï¼Œæˆ–æ˜¯éœ€è¦æˆ˜ç•¥è§„åˆ’å’Œé€‚åº”æ—¶ï¼Œè¯·ä½¿ç”¨è¿™äº›æ¨ç†æŠ€æœ¯ã€‚å®ƒä»¬éå¸¸é€‚ç”¨äºé‚£äº›å±•ç¤ºâ€œå·¥ä½œè¿‡ç¨‹â€æˆ–æ€è€ƒè¿‡ç¨‹ä¸æœ€ç»ˆç­”æ¡ˆåŒç­‰é‡è¦çš„ä»»åŠ¡ã€‚
</mark>
**Visual summary**
<mark>**å¯è§†åŒ–æ€»ç»“**</mark>
<img width="2048" height="1445" alt="image" src="https://github.com/user-attachments/assets/8b2dd85e-4c14-4582-809a-ecf9410f0f51" />
Fig. 7: Reasoning design pattern
<mark>å›¾ 7ï¼šæ¨ç†è®¾è®¡æ¨¡å¼</mark>

â— By making their reasoning explicit, agents can formulate transparent, multi-step plans, which is the foundational capability for autonomous action and user trust.
<mark>â— é€šè¿‡æ˜ç¡®å…¶æ¨ç†è¿‡ç¨‹ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿåˆ¶å®šé€æ˜çš„ã€å¤šæ­¥éª¤çš„è®¡åˆ’ï¼Œè¿™æ˜¯å®ç°è‡ªä¸»è¡ŒåŠ¨å’Œèµ¢å¾—ç”¨æˆ·ä¿¡ä»»çš„åŸºç¡€èƒ½åŠ›ã€‚</mark>

â— The ReAct framework provides agents with their core operational loop, empowering them to move beyond mere reasoning and interact with external tools to dynamically act and adapt within an environment.
<mark>â— ReAct æ¡†æ¶ä¸ºæ™ºèƒ½ä½“æä¾›äº†å…¶æ ¸å¿ƒçš„æ“ä½œå¾ªç¯ï¼Œä½¿å…¶èƒ½å¤Ÿè¶…è¶Šå•çº¯çš„æ¨ç†ï¼Œå¹¶ä¸å¤–éƒ¨å·¥å…·è¿›è¡Œäº¤äº’ï¼Œä»è€Œåœ¨ç¯å¢ƒä¸­åŠ¨æ€åœ°è¡ŒåŠ¨å’Œé€‚åº”ã€‚</mark>

â— The Scaling Inference Law implies an agent's performance is not just about its underlying model size, but its allocated "thinking time," allowing for more deliberate and higher-quality autonomous actions.
<mark>â— æ¨ç†ç¼©æ”¾å®šå¾‹ï¼ˆScaling Inference Lawï¼‰è¡¨æ˜ï¼Œæ™ºèƒ½ä½“çš„æ€§èƒ½ä¸ä»…ä¸å…¶åº•å±‚æ¨¡å‹çš„å¤§å°æœ‰å…³ï¼Œè¿˜ä¸å…¶åˆ†é…çš„â€œæ€è€ƒæ—¶é—´â€æœ‰å…³ï¼Œè¿™ä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿæ‰§è¡Œæ›´å®¡æ…ã€æ›´é«˜è´¨é‡çš„è‡ªä¸»è¡ŒåŠ¨ã€‚</mark>

â— Chain-of-Thought (CoT) serves as an agent's internal monologue, providing a structured way to formulate a plan by breaking a complex goal into a sequence of manageable actions.
<mark>â— æ€ç»´é“¾ï¼ˆChain-of-Thought, CoTï¼‰ä½œä¸ºæ™ºèƒ½ä½“çš„å†…éƒ¨ç‹¬ç™½ï¼Œé€šè¿‡å°†å¤æ‚çš„ç›®æ ‡åˆ†è§£ä¸ºä¸€ç³»åˆ—å¯ç®¡ç†çš„æ“ä½œï¼Œæä¾›äº†ä¸€ç§ç»“æ„åŒ–çš„æ–¹æ³•æ¥åˆ¶å®šè®¡åˆ’ã€‚</mark>

â— Tree-of-Thought and Self-Correction give agents the crucial ability to deliberate, allowing them to evaluate multiple strategies, backtrack from errors, and improve their own plans before execution.
<mark>â— æ€ç»´æ ‘ï¼ˆTree-of-Thoughtï¼‰å’Œè‡ªæˆ‘ä¿®æ­£ï¼ˆSelf-Correctionï¼‰èµ‹äºˆæ™ºèƒ½ä½“å…³é”®çš„å®¡è®®èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨æ‰§è¡Œå‰è¯„ä¼°å¤šç§ç­–ç•¥ã€ä»é”™è¯¯ä¸­å›æº¯å¹¶æ”¹è¿›è‡ªå·±çš„è®¡åˆ’ã€‚</mark>

â— Collaborative frameworks like Chain of Debates (CoD) signal the shift from solitary agents to multi-agent systems, where teams of agents can reason together to tackle more complex problems and reduce individual biases.
<mark>â— åƒè¾©è®ºé“¾ï¼ˆChain of Debates, CoDï¼‰è¿™æ ·çš„åä½œæ¡†æ¶æ ‡å¿—ç€ä»å•ä¸ªæ™ºèƒ½ä½“å‘å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„è½¬å˜ï¼Œåœ¨è¿™ç§ç³»ç»Ÿä¸­ï¼Œæ™ºèƒ½ä½“å›¢é˜Ÿå¯ä»¥ååŒæ¨ç†ï¼Œä»¥è§£å†³æ›´å¤æ‚çš„é—®é¢˜å¹¶å‡å°‘ä¸ªä½“åè§ã€‚</mark>

â— Applications like Deep Research demonstrate how these techniques culminate in agents that can execute complex, long-running tasks, such as in-depth investigation, completely autonomously on a user's behalf.
<mark>â— åƒæ·±åº¦ç ”ç©¶ï¼ˆDeep Researchï¼‰è¿™æ ·çš„åº”ç”¨å±•ç¤ºäº†è¿™äº›æŠ€æœ¯å¦‚ä½•æœ€ç»ˆè½¬åŒ–ä¸ºæ™ºèƒ½ä½“ï¼Œä½¿å…¶èƒ½å¤Ÿå®Œå…¨ä»£è¡¨ç”¨æˆ·è‡ªä¸»æ‰§è¡Œå¤æ‚çš„ã€è€—æ—¶é•¿çš„ä»»åŠ¡ï¼Œä¾‹å¦‚æ·±åº¦è°ƒæŸ¥ã€‚</mark>

â— To build effective teams of agents, frameworks like MASS automate the optimization of how individual agents are instructed and how they interact, ensuring the entire multi-agent system performs optimally.
<mark>â— ä¸ºäº†æ„å»ºæœ‰æ•ˆçš„æ™ºèƒ½ä½“å›¢é˜Ÿï¼ŒMASS ç­‰æ¡†æ¶è‡ªåŠ¨åŒ–äº†å•ä¸ªæ™ºèƒ½ä½“æŒ‡ä»¤çš„ä¸‹å‘æ–¹å¼åŠå…¶äº¤äº’çš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œç¡®ä¿æ•´ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿèƒ½å¤Ÿä»¥æœ€ä½³çŠ¶æ€è¿è¡Œã€‚</mark>

â— By integrating these reasoning techniques, we build agents that are not just automated but truly autonomous, capable of being trusted to plan, act, and solve complex problems without direct supervision.
<mark>â— é€šè¿‡æ•´åˆè¿™äº›æ¨ç†æŠ€æœ¯ï¼Œæˆ‘ä»¬æ„å»ºçš„æ™ºèƒ½ä½“ä¸ä»…æ˜¯è‡ªåŠ¨åŒ–çš„ï¼Œæ›´æ˜¯çœŸæ­£è‡ªä¸»çš„ï¼Œèƒ½å¤Ÿè¢«ä¿¡ä»»åœ¨æ— éœ€ç›´æ¥ç›‘ç£çš„æƒ…å†µä¸‹è¿›è¡Œè§„åˆ’ã€è¡ŒåŠ¨å’Œè§£å†³å¤æ‚é—®é¢˜ã€‚</mark>

---

## Conclusions
<mark>## ç»“è¯­</mark>

Modern AI is evolving from passive tools into autonomous agents, capable of tackling complex goals through structured reasoning.
<mark>ç°ä»£äººå·¥æ™ºèƒ½æ­£ä»è¢«åŠ¨çš„å·¥å…·æ¼”å˜ä¸ºèƒ½å¤Ÿé€šè¿‡ç»“æ„åŒ–æ¨ç†æ¥è¾¾æˆå¤æ‚ç›®æ ‡çš„è‡ªä¸»æ™ºèƒ½ä½“ã€‚</mark>

This agentic behavior begins with an internal monologue, powered by techniques like Chain-of-Thought (CoT), which allows an agent to formulate a coherent plan before acting.
<mark>è¿™ç§å…·æ™ºèƒ½ä½“ç‰¹æ€§çš„è¡Œä¸ºå§‹äºå†…éƒ¨ç‹¬ç™½ï¼Œç”±æ€ç»´é“¾ï¼ˆCoTï¼‰ç­‰æŠ€æœ¯é©±åŠ¨ï¼Œä½¿æ™ºèƒ½ä½“åœ¨è¡ŒåŠ¨å‰èƒ½å¤Ÿåˆ¶å®šè¿è´¯çš„è®¡åˆ’ã€‚</mark>

True autonomy requires deliberation, which agents achieve through Self-Correction and Tree-of-Thought (ToT), enabling them to evaluate multiple strategies and independently improve their own work.
<mark>çœŸæ­£çš„è‡ªä¸»æ€§éœ€è¦å®¡è®®ï¼Œæ™ºèƒ½ä½“é€šè¿‡è‡ªæˆ‘ä¿®æ­£å’Œæ€ç»´æ ‘ï¼ˆToTï¼‰æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œä½¿å…¶èƒ½å¤Ÿè¯„ä¼°å¤šç§ç­–ç•¥å¹¶ç‹¬ç«‹æ”¹è¿›è‡ªèº«å·¥ä½œã€‚</mark>

The pivotal leap to fully agentic systems comes from the ReAct framework, which empowers an agent to move beyond thinking and start acting by using external tools.
<mark>å‘å®Œå…¨å…·æ™ºèƒ½ä½“ç‰¹æ€§çš„ç³»ç»Ÿè¿ˆè¿›çš„å…³é”®ä¸€æ­¥æ¥è‡ªäº ReAct æ¡†æ¶ï¼Œå®ƒä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿé€šè¿‡ä½¿ç”¨å¤–éƒ¨å·¥å…·ï¼Œè¶…è¶Šå•çº¯çš„æ€è€ƒè€Œå¼€å§‹è¡ŒåŠ¨ã€‚</mark>

This establishes the core agentic loop of thought, action, and observation, allowing the agent to dynamically adapt its strategy based on environmental feedback.
<mark>è¿™ç¡®ç«‹äº†æ€è€ƒã€è¡ŒåŠ¨å’Œè§‚å¯Ÿçš„æ ¸å¿ƒæ™ºèƒ½ä½“å¾ªç¯ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿæ ¹æ®ç¯å¢ƒåé¦ˆåŠ¨æ€åœ°è°ƒæ•´å…¶ç­–ç•¥ã€‚</mark>

An agent's capacity for deep deliberation is fueled by the Scaling Inference Law, where more computational "thinking time" directly translates into more robust autonomous actions.
<mark>æ™ºèƒ½ä½“æ·±åº¦å®¡è®®çš„èƒ½åŠ›ç”±æ¨ç†ç¼©æ”¾å®šå¾‹ï¼ˆScaling Inference Lawï¼‰é©±åŠ¨ï¼Œæ›´å¤šçš„è®¡ç®—â€œæ€è€ƒæ—¶é—´â€ç›´æ¥è½¬åŒ–ä¸ºæ›´é²æ£’çš„è‡ªä¸»è¡ŒåŠ¨ã€‚</mark>

The next frontier is the multi-agent system, where frameworks like Chain of Debates (CoD) create collaborative agent societies that reason together to achieve a common goal.
<mark>ä¸‹ä¸€ä¸ªå‰æ²¿æ˜¯å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œå…¶ä¸­è¾©è®ºé“¾ï¼ˆCoDï¼‰ç­‰æ¡†æ¶åˆ›å»ºäº†åä½œæ€§çš„æ™ºèƒ½ä½“ç¤¾ä¼šï¼Œå®ƒä»¬èƒ½å¤ŸååŒæ¨ç†ä»¥è¾¾æˆå…±åŒç›®æ ‡ã€‚</mark>

This is not theoretical; agentic applications like Deep Research already demonstrate how autonomous agents can execute complex, multi-step investigations on a user's behalf.
<mark>è¿™å¹¶éçº¸ä¸Šè°ˆå…µï¼›åƒæ·±åº¦ç ”ç©¶ï¼ˆDeep Researchï¼‰è¿™æ ·çš„å…·æ™ºèƒ½ä½“åº”ç”¨å·²ç»å±•ç¤ºäº†è‡ªä¸»æ™ºèƒ½ä½“å¦‚ä½•èƒ½ä»£è¡¨ç”¨æˆ·æ‰§è¡Œå¤æ‚çš„ã€å¤šæ­¥éª¤çš„è°ƒæŸ¥ã€‚</mark>

The overarching goal is to engineer reliable and transparent autonomous agents that can be trusted to independently manage and solve intricate problems.
<mark>æ€»ä½“çš„ç›®æ ‡æ˜¯æ„å»ºå¯é ä¸”é€æ˜çš„è‡ªä¸»æ™ºèƒ½ä½“ï¼Œè¿™äº›æ™ºèƒ½ä½“èƒ½å¤Ÿè¢«ä¿¡ä»»ç‹¬ç«‹åœ°ç®¡ç†å’Œè§£å†³å¤æ‚çš„éš¾é¢˜ã€‚</mark>

Ultimately, by combining explicit reasoning with the power to act, these methodologies are completing the transformation of AI into truly agentic problem-solvers.
<mark>æœ€ç»ˆï¼Œé€šè¿‡å°†æ˜ç¡®çš„æ¨ç†ä¸è¡ŒåŠ¨èƒ½åŠ›ç›¸ç»“åˆï¼Œè¿™äº›æ–¹æ³•è®ºæ­£åœ¨å®Œæˆäººå·¥æ™ºèƒ½å‘çœŸæ­£å…·æ™ºèƒ½ä½“ç‰¹æ€§çš„é—®é¢˜è§£å†³è€…çš„è½¬å˜ã€‚</mark>

---

## References
<mark>## å‚è€ƒæ–‡çŒ®</mark>

Relevant research includes:
<mark>ç›¸å…³ç ”ç©¶åŒ…æ‹¬ï¼š</mark>

1. "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models" by Wei et al. (2022)
<mark>1. Wei ç­‰äººï¼ˆ2022ï¼‰çš„ã€ŠChain-of-Thought Prompting Elicits Reasoning in Large Language Modelsã€‹</mark>

2. "Tree of Thoughts: Deliberate Problem Solving with Large Language Models" by Yao et al. (2023)
<mark>2. Yao ç­‰äººï¼ˆ2023ï¼‰çš„ã€ŠTree of Thoughts: Deliberate Problem Solving with Large Language Modelsã€‹</mark>

3. "Program-Aided Language Models" by Gao et al. (2023)
<mark>3. Gao ç­‰äººï¼ˆ2023ï¼‰çš„ã€ŠProgram-Aided Language Modelsã€‹</mark>

4. "ReAct: Synergizing Reasoning and Acting in Language Models" by Yao et al. (2023)
<mark>4. Yao ç­‰äººï¼ˆ2023ï¼‰çš„ã€ŠReAct: Synergizing Reasoning and Acting in Language Modelsã€‹</mark>

5. Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving, 2024
<mark>5. ã€ŠInference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solvingã€‹, 2024</mark>

6. Multi-Agent Design: Optimizing Agents with Better Prompts and Topologies, https://arxiv.org/abs/2502.02533
<mark>6. ã€ŠMulti-Agent Design: Optimizing Agents with Better Prompts and Topologiesã€‹, https://arxiv.org/abs/2502.02533</mark>

